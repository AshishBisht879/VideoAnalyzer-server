{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'inputs': {'gemini_results': [('Carvana', 1), ('Mustang', 1)], 'vi_results': [('Apple Store', 0.99), ('Google Store', 0.99)], 'ocr_text': [('Carvana', 1)]}, 'output': {'final': ['Carvana']}}, {'inputs': {'gemini_results': [('Breeze', 1), ('Febreze', 1)], 'vi_results': [('Febreze', 0.99), ('Procter & Gamble', 0.99)], 'ocr_text': [('Febreze', 0.9)]}, 'output': {'final': ['Febreze']}}, {'inputs': {'gemini_results': [('Dove', 1)], 'vi_results': [('Unilever', 0.99), ('Nike', 0.99)], 'ocr_text': [('Dove', 1), ('Unilever', 0.8)]}, 'output': {'final': ['Unilever', 'Dove']}}, {'inputs': {'gemini_results': [('Paramount+', 1)], 'vi_results': [('Paramount Network', 0.99)], 'ocr_text': [('Paramount+', 0.9)]}, 'output': {'final': ['Paramount+']}}, {'inputs': {'gemini_results': [('Discover Newport', 1)], 'vi_results': [], 'ocr_text': [('Discover Newport', 0.9)]}, 'output': {'final': ['Discover Newport']}}, {'inputs': {'gemini_results': [('Raymour & Flanigan', 1)], 'vi_results': [('Raymour & Flanigan', 0.99), (\"Macy's\", 0.99)], 'ocr_text': [('Raymour & Flanigan', 1)]}, 'output': {'final': ['Raymour & Flanigan']}}, {'inputs': {'gemini_results': [('Nectar', 1), ('Sealy', 1), ('Casper', 1), ('Serta', 1), ('Stearns & Foster', 1), ('SmartLife', 1), ('King Koil', 1)], 'vi_results': [('Raymour & Flanigan', 0.99), ('Sealy Corporation', 0.99)], 'ocr_text': [('Nectar', 0.9), ('Sealy', 0.8), ('Casper', 0.7), ('Serta', 0.6), ('Stearns & Foster', 0.5), ('SmartLife', 0.4), ('King Koil', 0.3)]}, 'output': {'final': ['SmartLife', 'Casper', 'Serta', 'King Koil', 'Stearns & Foster', 'Nectar', 'Sealy']}}, {'inputs': {'gemini_results': [('BETMGM', 1)], 'vi_results': [('National Hockey League', 0.99), ('Taco Bell', 0.99)], 'ocr_text': [('BETMGM', 1)]}, 'output': {'final': ['BETMGM']}}, {'inputs': {'gemini_results': [], 'vi_results': [], 'ocr_text': [('Missouri', 0.9)]}, 'output': {'final': ['Missouri']}}, {'inputs': {'gemini_results': [('Paramount+', 1)], 'vi_results': [('Paramount Network', 0.99)], 'ocr_text': [('Paramount+', 0.9)]}, 'output': {'final': ['Paramount+']}}, {'inputs': {'gemini_results': [('Raymour & Flanigan', 1)], 'vi_results': [('Raymour & Flanigan', 0.99)], 'ocr_text': [('Raymour & Flanigan', 0.9)]}, 'output': {'final': ['Raymour & Flanigan']}}, {'inputs': {'gemini_results': [('HelloFresh', 1), ('Peacock', 0.8)], 'vi_results': [('HelloFresh', 0.99)], 'ocr_text': [('HelloFresh', 0.95)]}, 'output': {'final': ['HelloFresh']}}, {'inputs': {'gemini_results': [('CREATION MUSEUM', 1)], 'vi_results': [], 'ocr_text': [('CREATION MUSEUM', 0.8)]}, 'output': {'final': ['CREATION MUSEUM']}}, {'inputs': {'gemini_results': [('Ruggable', 1)], 'vi_results': [], 'ocr_text': [('Ruggable', 0.9)]}, 'output': {'final': ['Ruggable']}}, {'inputs': {'gemini_results': [('WELLS FARGO', 1)], 'vi_results': [('Wells Fargo', 0.99), ('Visa Inc.', 0.99)], 'ocr_text': [('WELLS FARGO', 1)]}, 'output': {'final': ['WELLS FARGO']}}, {'inputs': {'gemini_results': [('Jacoby & Meyers', 1)], 'vi_results': [('Los Angeles Dodgers', 0.99)], 'ocr_text': [('Jacoby & Meyers', 0.9)]}, 'output': {'final': ['Jacoby & Meyers']}}, {'inputs': {'gemini_results': [('Ziploc', 1), ('Disney', 1), ('Pixar', 1), ('SC Johnson', 1)], 'vi_results': [('Ziploc', 0.99), ('The Walt Disney Company', 0.99)], 'ocr_text': [('Ziploc', 0.9), ('Disney PIXAR', 0.8)]}, 'output': {'final': ['Ziploc']}}, {'inputs': {'gemini_results': [('Xfinity', 1), ('Xfinity Mobile', 1)], 'vi_results': [('Comcast', 0.99)], 'ocr_text': [('Xfinity', 1)]}, 'output': {'final': ['Xfinity']}}, {'inputs': {'gemini_results': [('Comcast', 1)], 'vi_results': [('Comcast Business', 0.99), ('Mini', 0.99)], 'ocr_text': [('Comcast', 0.9)]}, 'output': {'final': ['Comcast']}}, {'inputs': {'gemini_results': [('Comcast Business', 1)], 'vi_results': [('Comcast Business', 0.99)], 'ocr_text': [('COMCAST BUSINESS', 1)]}, 'output': {'final': ['COMCAST BUSINESS']}}, {'inputs': {'gemini_results': [('Xfinity Mobile', 1)], 'vi_results': [], 'ocr_text': [('Xfinity Mobile', 0.95)]}, 'output': {'final': ['Xfinity Mobile']}}, {'inputs': {'gemini_results': [('Jacoby & Meyers', 1)], 'vi_results': [('Los Angeles Dodgers', 0.99)], 'ocr_text': [('Jacoby & Meyers', 0.95)]}, 'output': {'final': ['Jacoby & Meyers']}}, {'inputs': {'gemini_results': [('Mastercard', 1)], 'vi_results': [('Stand Up to Cancer', 0.99), ('Mastercard', 0.99)], 'ocr_text': [('Mastercard', 1)]}, 'output': {'final': ['Mastercard']}}, {'inputs': {'gemini_results': [('Ginger', 0.8), ('BETMGM', 0.9)], 'vi_results': [('NBA', 0.99)], 'ocr_text': [('BETMGM', 1)]}, 'output': {'final': ['BETMGM']}}, {'inputs': {'gemini_results': [], 'vi_results': [('Oris SA', 0.99)], 'ocr_text': [('ORIS', 0.8)]}, 'output': {'final': ['ORIS']}}, {'inputs': {'gemini_results': [('Safelite', 1)], 'vi_results': [('Safelite', 0.99), ('Nike', 0.99)], 'ocr_text': [('Safelite', 1)]}, 'output': {'final': ['Safelite']}}, {'inputs': {'gemini_results': [(\"Baker's\", 1), ('Fox', 1)], 'vi_results': [('KTVU', 0.99), ('Fox Broadcasting Company', 0.99)], 'ocr_text': [('AT&T', 0.9)]}, 'output': {'final': [\"Baker's\", 'Fox']}}, {'inputs': {'gemini_results': [('Freeform', 1), ('Hulu', 1)], 'vi_results': [('Hulu', 0.99)], 'ocr_text': [('Freeform', 0.9), ('hulu', 0.9)]}, 'output': {'final': ['Freeform', 'hulu']}}, {'inputs': {'gemini_results': [('Booking.com', 1), ('Disney', 1)], 'vi_results': [('The Walt Disney Company', 0.99)], 'ocr_text': [('Booking.com', 0.9)]}, 'output': {'final': ['Booking.com']}}, {'inputs': {'gemini_results': [('Paramount+', 0.8), ('Peridot Plus', 0.8)], 'vi_results': [('Paramount Network', 0.99)], 'ocr_text': [('Paramount+', 0.9)]}, 'output': {'final': ['Paramount+']}}, {'inputs': {'gemini_results': [('Sherwin-Williams', 1), ('Williams', 0.8)], 'vi_results': [('Sherwin-Williams', 0.99)], 'ocr_text': [('SHERWIN-WILLIAMS', 1)]}, 'output': {'final': ['SHERWIN-WILLIAMS']}}, {'inputs': {'gemini_results': [('Disney', 1), ('Hulu', 1), ('Google', 1), ('FX', 1), ('20th Century Studios', 1), ('Searchlight Pictures', 1)], 'vi_results': [('The Walt Disney Company', 0.99), ('Hulu', 0.99)], 'ocr_text': [('Disney', 0.9), ('Hulu', 0.8)]}, 'output': {'final': ['Hulu', 'Disney']}}, {'inputs': {'gemini_results': [('MADE IN COOKWARE', 1)], 'vi_results': [('Ballet National de Marseille', 0.99)], 'ocr_text': [('MADE IN COOKWARE', 0.9)]}, 'output': {'final': ['MADE IN COOKWARE']}}, {'inputs': {'gemini_results': [('Pandora', 1)], 'vi_results': [('Pandora', 0.99)], 'ocr_text': [('Pandora', 1)]}, 'output': {'final': ['Pandora']}}, {'inputs': {'gemini_results': [('Lincoln', 1)], 'vi_results': [], 'ocr_text': [('Lincoln', 1)]}, 'output': {'final': ['Lincoln']}}, {'inputs': {'gemini_results': [('U.S. Bank', 1)], 'vi_results': [('U.S. Bancorp', 0.99), ('CitroÃ«n', 0.99)], 'ocr_text': [('U.S. Bank', 0.9)]}, 'output': {'final': ['U.S. Bank']}}, {'inputs': {'gemini_results': [('Paramount+', 1), (\"RUPAUL'S DRAG RACE ALL STARS\", 1), ('BIG BROTHER', 1), ('THE CHALLENGE', 1), ('SURVIVOR', 1), ('ARE YOU THE ONE?', 1), ('INK MASTER', 1), ('QUEEN OF THE UNIVERSE', 1), ('BAR RESCUE', 1)], 'vi_results': [], 'ocr_text': [('Paramount+', 0.9), (\"RUPAUL'S DRAG RACE ALL STARS\", 0.8), ('BIG BROTHER', 0.8), ('THE CHALLENGE', 0.7), ('SURVIVOR', 0.6), ('ARE YOU THE ONE?', 0.5), ('INK MASTER', 0.4), ('QUEEN OF THE UNIVERSE', 0.4), ('BAR RESCUE', 0.3)]}, 'output': {'final': ['INK MASTER', \"RUPAUL'S DRAG RACE ALL STARS\", 'ARE YOU THE ONE?', 'Paramount+', 'QUEEN OF THE UNIVERSE', 'BIG BROTHER', 'SURVIVOR', 'THE CHALLENGE', 'BAR RESCUE']}}, {'inputs': {'gemini_results': [('MAIN EVENT', 1)], 'vi_results': [], 'ocr_text': [('MAIN EVENT', 0.9)]}, 'output': {'final': ['MAIN EVENT']}}, {'inputs': {'gemini_results': [], 'vi_results': [], 'ocr_text': [('Amazon Prime', 0.9)]}, 'output': {'final': ['Amazon Prime']}}, {'inputs': {'gemini_results': [('ARIAT', 1)], 'vi_results': [], 'ocr_text': [('ARIAT', 0.9)]}, 'output': {'final': ['ARIAT']}}, {'inputs': {'gemini_results': [('Bassett', 1)], 'vi_results': [('Bassett Furniture', 0.99)], 'ocr_text': [('Bassett', 1)]}, 'output': {'final': ['Bassett']}}, {'inputs': {'gemini_results': [('Taco Bell', 1)], 'vi_results': [('Taco Bell', 0.99)], 'ocr_text': [('Taco Bell', 1)]}, 'output': {'final': ['Taco Bell']}}, {'inputs': {'gemini_results': [('Bassett', 1)], 'vi_results': [('Bassett Furniture', 0.99)], 'ocr_text': [('Bassett', 1), ('Stearns & Foster', 0.8), ('Hunter Douglas', 0.8), ('Chanel', 0.2)]}, 'output': {'final': ['Bassett']}}, {'inputs': {'gemini_results': [('Taco Bell', 0.9)], 'vi_results': [('Taco Bell', 0.99)], 'ocr_text': [('Taco Bell', 1)]}, 'output': {'final': ['Taco Bell']}}]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "res = requests.get('http://35.222.204.206:5000/records')\n",
    "data = res.json()\n",
    "\n",
    "data_set = []\n",
    "\n",
    "for record in data:\n",
    "    \n",
    "    inputs={}\n",
    "    output={}\n",
    "    \n",
    "    transcript_result = record[\"brands_audio\"]\n",
    "    \n",
    "    if len(transcript_result[\"gemini_results\"]):\n",
    "        inputs[\"gemini_results\"] =[(k,v) for k,v in  transcript_result[\"gemini_results\"].items()]\n",
    "    else:\n",
    "        inputs[\"gemini_results\"] = []\n",
    "    \n",
    "    vi_results = [(k,v) for k,v in record[\"brands_video_gcp\"].items()]\n",
    "    \n",
    "    if len(vi_results):\n",
    "        inputs[\"vi_results\"] = vi_results\n",
    "    else:\n",
    "        inputs[\"vi_results\"]=[]\n",
    "    \n",
    "    \n",
    "    ocr_result = record[\"ocr_text\"]\n",
    "    \n",
    "    if len(ocr_result):\n",
    "        inputs[\"ocr_text\"] = [(k[\"brand\"],k[\"confidence\"]) for k in ocr_result]\n",
    "    else:\n",
    "        inputs[\"ocr_text\"] = []\n",
    "    \n",
    "    \n",
    "    output[\"final\"]=[k for k in record[\"final_brands\"]]\n",
    "    \n",
    "    data_set.append({\"inputs\":dict(inputs),\"output\":dict(output)})\n",
    "    \n",
    "    \n",
    "print(data_set)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary Loss:\n",
    "\n",
    "Binary loss (0 for correct prediction, 1 for incorrect prediction) is a simpler measure that only considers whether the final prediction matches the actual label, not how confident that prediction was.\n",
    "\n",
    "Gradient Calculation:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "if final_brand != actual_final:\n",
    "    for i, source in enumerate(['gemini_results', 'vi_results', 'ocr_text']):\n",
    "        if any(brand == actual_final for brand, _ in entry['inputs'][source]):\n",
    "            weight_gradients[i] -= 1\n",
    "        if any(brand == final_brand for brand, _ in entry['inputs'][source']):\n",
    "            weight_gradients[i] += 1\n",
    "If the predicted brand is incorrect, we update the weight gradients:\n",
    "We decrease the gradient for sources that had the actual final brand.\n",
    "We increase the gradient for sources that had the incorrectly predicted brand.\n",
    "Weight Update:\n",
    "\n",
    "python\n",
    "Copy code\n",
    "weights -= learning_rate * weight_gradients\n",
    "weights = np.clip(weights, 0, 1)\n",
    "weights /= np.sum(weights)\n",
    "We adjust the weights using the calculated gradients and the learning rate.\n",
    "The weights are clipped to ensure they stay between 0 and 1.\n",
    "The weights are normalized to sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 7, Weights: [0.30952381 0.30952381 0.38095238]\n",
      "Epoch 2/100, Loss: 7, Weights: [0.29543246 0.29543246 0.40913508]\n",
      "Epoch 3/100, Loss: 6, Weights: [0.28105353 0.29125761 0.42768886]\n",
      "Epoch 4/100, Loss: 6, Weights: [0.26638115 0.28699756 0.44662129]\n",
      "Epoch 5/100, Loss: 6, Weights: [0.25140934 0.28265057 0.46594009]\n",
      "Epoch 6/100, Loss: 6, Weights: [0.23613198 0.27821487 0.48565315]\n",
      "Epoch 7/100, Loss: 6, Weights: [0.22054284 0.27368864 0.50576852]\n",
      "Epoch 8/100, Loss: 4, Weights: [0.22054284 0.28368864 0.49576852]\n",
      "Epoch 9/100, Loss: 6, Weights: [0.20463555 0.27927413 0.51609033]\n",
      "Epoch 10/100, Loss: 4, Weights: [0.20463555 0.28927413 0.50609033]\n",
      "Epoch 11/100, Loss: 4, Weights: [0.20463555 0.29927413 0.49609033]\n",
      "Epoch 12/100, Loss: 6, Weights: [0.18840362 0.29517768 0.5164187 ]\n",
      "Epoch 13/100, Loss: 3, Weights: [0.19840362 0.29517768 0.5064187 ]\n",
      "Epoch 14/100, Loss: 4, Weights: [0.19840362 0.30517768 0.4964187 ]\n",
      "Epoch 15/100, Loss: 5, Weights: [0.19224859 0.29099763 0.51675378]\n",
      "Epoch 16/100, Loss: 4, Weights: [0.19224859 0.30099763 0.50675378]\n",
      "Epoch 17/100, Loss: 3, Weights: [0.20224859 0.30099763 0.49675378]\n",
      "Epoch 18/100, Loss: 6, Weights: [0.18596795 0.29693636 0.51709569]\n",
      "Epoch 19/100, Loss: 3, Weights: [0.19596795 0.29693636 0.50709569]\n",
      "Epoch 20/100, Loss: 4, Weights: [0.19596795 0.30693636 0.49709569]\n",
      "Epoch 21/100, Loss: 5, Weights: [0.18976321 0.2927922  0.51744458]\n",
      "Epoch 22/100, Loss: 4, Weights: [0.18976321 0.3027922  0.50744458]\n",
      "Epoch 23/100, Loss: 3, Weights: [0.19976321 0.3027922  0.49744458]\n",
      "Epoch 24/100, Loss: 5, Weights: [0.19363593 0.28856347 0.51780059]\n",
      "Epoch 25/100, Loss: 4, Weights: [0.19363593 0.29856347 0.50780059]\n",
      "Epoch 26/100, Loss: 3, Weights: [0.20363593 0.29856347 0.49780059]\n",
      "Epoch 27/100, Loss: 6, Weights: [0.1873836  0.29445252 0.51816387]\n",
      "Epoch 28/100, Loss: 3, Weights: [0.1973836  0.29445252 0.50816387]\n",
      "Epoch 29/100, Loss: 4, Weights: [0.1973836  0.30445252 0.49816387]\n",
      "Epoch 30/100, Loss: 5, Weights: [0.19120776 0.29025768 0.51853456]\n",
      "Epoch 31/100, Loss: 4, Weights: [0.19120776 0.30025768 0.50853456]\n",
      "Epoch 32/100, Loss: 3, Weights: [0.20120776 0.30025768 0.49853456]\n",
      "Epoch 33/100, Loss: 4, Weights: [0.20120776 0.31025768 0.48853456]\n",
      "Epoch 34/100, Loss: 5, Weights: [0.19510996 0.2961813  0.50870874]\n",
      "Epoch 35/100, Loss: 4, Weights: [0.19510996 0.3061813  0.49870874]\n",
      "Epoch 36/100, Loss: 3, Weights: [0.20510996 0.3061813  0.48870874]\n",
      "Epoch 37/100, Loss: 5, Weights: [0.19909179 0.29202174 0.50888647]\n",
      "Epoch 38/100, Loss: 4, Weights: [0.19909179 0.30202174 0.49888647]\n",
      "Epoch 39/100, Loss: 3, Weights: [0.20909179 0.30202174 0.48888647]\n",
      "Epoch 40/100, Loss: 6, Weights: [0.19295081 0.29798137 0.50906782]\n",
      "Epoch 41/100, Loss: 3, Weights: [0.20295081 0.29798137 0.49906782]\n",
      "Epoch 42/100, Loss: 4, Weights: [0.20295081 0.30798137 0.48906782]\n",
      "Epoch 43/100, Loss: 5, Weights: [0.19688858 0.29385854 0.50925288]\n",
      "Epoch 44/100, Loss: 4, Weights: [0.19688858 0.30385854 0.49925288]\n",
      "Epoch 45/100, Loss: 3, Weights: [0.20688858 0.30385854 0.48925288]\n",
      "Epoch 46/100, Loss: 6, Weights: [0.19070264 0.29985565 0.50944172]\n",
      "Epoch 47/100, Loss: 3, Weights: [0.20070264 0.29985565 0.49944172]\n",
      "Epoch 48/100, Loss: 4, Weights: [0.20070264 0.30985565 0.48944172]\n",
      "Epoch 49/100, Loss: 5, Weights: [0.19459453 0.29577107 0.5096344 ]\n",
      "Epoch 50/100, Loss: 4, Weights: [0.19459453 0.30577107 0.4996344 ]\n",
      "Epoch 51/100, Loss: 3, Weights: [0.20459453 0.30577107 0.4896344 ]\n",
      "Epoch 52/100, Loss: 5, Weights: [0.19856584 0.29160313 0.50983102]\n",
      "Epoch 53/100, Loss: 4, Weights: [0.19856584 0.30160313 0.49983102]\n",
      "Epoch 54/100, Loss: 3, Weights: [0.20856584 0.30160313 0.48983102]\n",
      "Epoch 55/100, Loss: 6, Weights: [0.19241413 0.29755422 0.51003166]\n",
      "Epoch 56/100, Loss: 3, Weights: [0.20241413 0.29755422 0.50003166]\n",
      "Epoch 57/100, Loss: 4, Weights: [0.20241413 0.30755422 0.49003166]\n",
      "Epoch 58/100, Loss: 5, Weights: [0.19634094 0.29342267 0.51023638]\n",
      "Epoch 59/100, Loss: 4, Weights: [0.19634094 0.30342267 0.50023638]\n",
      "Epoch 60/100, Loss: 3, Weights: [0.20634094 0.30342267 0.49023638]\n",
      "Epoch 61/100, Loss: 6, Weights: [0.19014382 0.29941089 0.51044529]\n",
      "Epoch 62/100, Loss: 3, Weights: [0.20014382 0.29941089 0.50044529]\n",
      "Epoch 63/100, Loss: 4, Weights: [0.20014382 0.30941089 0.49044529]\n",
      "Epoch 64/100, Loss: 5, Weights: [0.19402431 0.29531723 0.51065846]\n",
      "Epoch 65/100, Loss: 4, Weights: [0.19402431 0.30531723 0.50065846]\n",
      "Epoch 66/100, Loss: 3, Weights: [0.20402431 0.30531723 0.49065846]\n",
      "Epoch 67/100, Loss: 5, Weights: [0.19798399 0.29114003 0.51087598]\n",
      "Epoch 68/100, Loss: 4, Weights: [0.19798399 0.30114003 0.50087598]\n",
      "Epoch 69/100, Loss: 4, Weights: [0.19798399 0.31114003 0.49087598]\n",
      "Epoch 70/100, Loss: 5, Weights: [0.19182039 0.29708167 0.51109794]\n",
      "Epoch 71/100, Loss: 3, Weights: [0.20182039 0.29708167 0.50109794]\n",
      "Epoch 72/100, Loss: 4, Weights: [0.20182039 0.30708167 0.49109794]\n",
      "Epoch 73/100, Loss: 5, Weights: [0.1957351  0.29294048 0.51132443]\n",
      "Epoch 74/100, Loss: 4, Weights: [0.1957351  0.30294048 0.50132443]\n",
      "Epoch 75/100, Loss: 3, Weights: [0.2057351  0.30294048 0.49132443]\n",
      "Epoch 76/100, Loss: 6, Weights: [0.18952561 0.29891885 0.51155554]\n",
      "Epoch 77/100, Loss: 3, Weights: [0.19952561 0.29891885 0.50155554]\n",
      "Epoch 78/100, Loss: 4, Weights: [0.19952561 0.30891885 0.49155554]\n",
      "Epoch 79/100, Loss: 5, Weights: [0.19339348 0.29481516 0.51179136]\n",
      "Epoch 80/100, Loss: 4, Weights: [0.19339348 0.30481516 0.50179136]\n",
      "Epoch 81/100, Loss: 3, Weights: [0.20339348 0.30481516 0.49179136]\n",
      "Epoch 82/100, Loss: 5, Weights: [0.19734028 0.29062771 0.512032  ]\n",
      "Epoch 83/100, Loss: 4, Weights: [0.19734028 0.30062771 0.502032  ]\n",
      "Epoch 84/100, Loss: 4, Weights: [0.19734028 0.31062771 0.492032  ]\n",
      "Epoch 85/100, Loss: 5, Weights: [0.19116355 0.29655889 0.51227756]\n",
      "Epoch 86/100, Loss: 4, Weights: [0.19116355 0.30655889 0.50227756]\n",
      "Epoch 87/100, Loss: 3, Weights: [0.20116355 0.30655889 0.49227756]\n",
      "Epoch 88/100, Loss: 5, Weights: [0.19506485 0.29240703 0.51252812]\n",
      "Epoch 89/100, Loss: 4, Weights: [0.19506485 0.30240703 0.50252812]\n",
      "Epoch 90/100, Loss: 3, Weights: [0.20506485 0.30240703 0.49252812]\n",
      "Epoch 91/100, Loss: 6, Weights: [0.18884169 0.29837452 0.51278379]\n",
      "Epoch 92/100, Loss: 3, Weights: [0.19884169 0.29837452 0.50278379]\n",
      "Epoch 93/100, Loss: 4, Weights: [0.19884169 0.30837452 0.49278379]\n",
      "Epoch 94/100, Loss: 5, Weights: [0.1926956  0.29425971 0.51304469]\n",
      "Epoch 95/100, Loss: 4, Weights: [0.1926956  0.30425971 0.50304469]\n",
      "Epoch 96/100, Loss: 3, Weights: [0.2026956  0.30425971 0.49304469]\n",
      "Epoch 97/100, Loss: 6, Weights: [0.18642408 0.30026502 0.51331091]\n",
      "Epoch 98/100, Loss: 3, Weights: [0.19642408 0.30026502 0.50331091]\n",
      "Epoch 99/100, Loss: 3, Weights: [0.20642408 0.30026502 0.49331091]\n",
      "Epoch 100/100, Loss: 6, Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Test Data - Final Brand: Ziploc, Confidence Score: 0.3910420801202134\n",
      "Test Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Carvana, Predicted: Carvana, Confidence: 0.4753895438171502, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Febreze, Predicted: Febreze, Confidence: 0.6617146624617165, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Unilever, Predicted: Unilever, Confidence: 0.41389694475647465, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.689930052895147, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Discover Newport, Predicted: Discover Newport, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 0.7727390112826853, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: SmartLife, Predicted: Nectar, Confidence: 0.160107181584048, Loss: 1\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.5454780225653708, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Missouri, Predicted: Missouri, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.689930052895147, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: HelloFresh, Predicted: HelloFresh, Confidence: 0.8645507383337424, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: CREATION MUSEUM, Predicted: CREATION MUSEUM, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Ruggable, Predicted: Ruggable, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: WELLS FARGO, Predicted: WELLS FARGO, Confidence: 0.5454780225653708, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.689930052895147, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Ziploc, Predicted: Ziploc, Confidence: 0.42589390668767585, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Xfinity, Predicted: Xfinity, Confidence: 0.5927995546853119, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Comcast, Predicted: Comcast, Confidence: 0.5266360429226209, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: COMCAST BUSINESS, Predicted: COMCAST BUSINESS, Confidence: 0.5151082499449552, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Xfinity Mobile, Predicted: Xfinity Mobile, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.6981271519533573, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Mastercard, Predicted: Mastercard, Confidence: 0.7727390112826853, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.6059011268438146, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: ORIS, Predicted: ORIS, Confidence: 0.5835394977395004, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Safelite, Predicted: Safelite, Confidence: 0.7727390112826853, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Baker's, Predicted: AT&T, Confidence: 0.32342932492343324, Loss: 1\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Freeform, Predicted: Freeform, Confidence: 0.4082595322292633, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Booking.com, Predicted: Booking.com, Confidence: 0.5743886488427657, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.579729514957496, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: SHERWIN-WILLIAMS, Predicted: SHERWIN-WILLIAMS, Confidence: 0.4468962384022022, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Hulu, Predicted: Hulu, Confidence: 0.3438486972720807, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: MADE IN COOKWARE, Predicted: MADE IN COOKWARE, Confidence: 0.689930052895147, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Pandora, Predicted: Pandora, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Lincoln, Predicted: Lincoln, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: U.S. Bank, Predicted: U.S. Bank, Confidence: 0.5266360429226209, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: INK MASTER, Predicted: Paramount+, Confidence: 0.1454613676862542, Loss: 1\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: MAIN EVENT, Predicted: MAIN EVENT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Amazon Prime, Predicted: Amazon Prime, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: ARIAT, Predicted: ARIAT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.7059020116765176, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.36628471257735523, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.19022865 0.29618879 0.51358256]\n",
      "Total Loss on Training Data: 3\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "# Example dataset\n",
    "data = copy.deepcopy(data_set)\n",
    "\n",
    "# Combining confidence scores with weights\n",
    "def combine_confidences(inputs, weights):\n",
    "    combined_scores = defaultdict(float)\n",
    "    for source, weight in zip(['gemini_results', 'vi_results', 'ocr_text'], weights):\n",
    "        for brand, score in inputs[source]:\n",
    "            combined_scores[brand] += weight * score\n",
    "    return combined_scores\n",
    "\n",
    "# Normalizing the scores\n",
    "def normalize_scores(combined_scores):\n",
    "    total_score = sum(combined_scores.values())\n",
    "    if total_score == 0:\n",
    "        return combined_scores\n",
    "    for brand in combined_scores:\n",
    "        combined_scores[brand] /= total_score\n",
    "    return combined_scores\n",
    "\n",
    "# Computing the final prediction\n",
    "def get_final_brand(inputs, weights):\n",
    "    combined_scores = combine_confidences(inputs, weights)\n",
    "    normalized_scores = normalize_scores(combined_scores)\n",
    "    final_brand = max(normalized_scores, key=normalized_scores.get)\n",
    "    final_confidence = normalized_scores[final_brand]\n",
    "    return final_brand, final_confidence\n",
    "\n",
    "# Calculating the loss function\n",
    "def calculate_loss(predicted, actual):\n",
    "    return 1 if predicted != actual else 0\n",
    "\n",
    "# Optimizing the weights using gradient descent\n",
    "def optimize_weights(data_set, learning_rate=0.01, epochs=100):\n",
    "    weights = np.ones(3) / 3  # Initial weights for gemini_results, vi_results, ocr_text\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        weight_gradients = np.zeros(3)\n",
    "        for entry in data_set:\n",
    "            actual_final = entry['output']['final'][0]\n",
    "            final_brand, _ = get_final_brand(entry['inputs'], weights)\n",
    "            loss = calculate_loss(final_brand, actual_final)\n",
    "            total_loss += loss\n",
    "            if final_brand != actual_final:\n",
    "                for i, source in enumerate(['gemini_results', 'vi_results', 'ocr_text']):\n",
    "                    if any(brand == actual_final for brand, _ in entry['inputs'][source]):\n",
    "                        weight_gradients[i] -= 1\n",
    "                    if any(brand == final_brand for brand, _ in entry['inputs'][source]):\n",
    "                        weight_gradients[i] += 1\n",
    "        weights -= learning_rate * weight_gradients\n",
    "        weights = np.clip(weights, 0, 1)\n",
    "        weights /= np.sum(weights)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}, Weights: {weights}\")\n",
    "    return weights\n",
    "\n",
    "# Optimizing weights and testing\n",
    "final_weights = optimize_weights(data)\n",
    "\n",
    "# Test case\n",
    "test_data = [\n",
    "    {'inputs': {'gemini_results': [('Disney', 1.00), ('Ziploc', 1.00), ('SC Johnson', 1.00)], 'vi_results': [('Ziploc', 0.99), ('The Walt Disney Company', 0.99)], 'ocr_text': [('Disney', 0.90), ('Ziploc', 0.80), ('MIPS', 0.50)]}},\n",
    "]\n",
    "\n",
    "# Get predictions for test data\n",
    "for entry in test_data:\n",
    "    final_brand, final_confidence = get_final_brand(entry['inputs'], final_weights)\n",
    "    print(f\"Test Data - Final Brand: {final_brand}, Confidence Score: {final_confidence}\")\n",
    "    print(f\"Test Data - Weights: {final_weights}\")\n",
    "\n",
    "# Print results for training data and calculate total loss\n",
    "total_loss = 0\n",
    "for entry in data:\n",
    "    final_brand, final_confidence = get_final_brand(entry['inputs'], final_weights)\n",
    "    actual_final = entry['output']['final'][0]\n",
    "    loss = calculate_loss(final_brand, actual_final)\n",
    "    total_loss += loss\n",
    "    print(f\"Expected: {actual_final}, Predicted: {final_brand}, Confidence: {final_confidence}, Loss: {loss}\")\n",
    "    print(f\"Training Data - Weights: {final_weights}\")\n",
    "\n",
    "print(f\"Total Loss on Training Data: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised Code with Cross-Entropy Loss and Adam Optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 25.01246291782739, Weights: [0.33993399 0.32013201 0.33993399]\n",
      "Epoch 2/100, Loss: 24.556503349931294, Weights: [0.34645844 0.30707765 0.34646391]\n",
      "Epoch 3/100, Loss: 24.112574863564728, Weights: [0.35289958 0.29418036 0.35292006]\n",
      "Epoch 4/100, Loss: 23.68045234482656, Weights: [0.35924994 0.28145075 0.35929931]\n",
      "Epoch 5/100, Loss: 23.259945014984943, Weights: [0.36550164 0.2688999  0.36559846]\n",
      "Epoch 6/100, Loss: 22.85089482712831, Weights: [0.37164639 0.25653938 0.37181423]\n",
      "Epoch 7/100, Loss: 22.45317499149614, Weights: [0.37767549 0.24438127 0.37794324]\n",
      "Epoch 8/100, Loss: 22.066688581650215, Weights: [0.38357986 0.23243811 0.38398203]\n",
      "Epoch 9/100, Loss: 21.691367171825096, Weights: [0.38935    0.22072293 0.38992707]\n",
      "Epoch 10/100, Loss: 21.32716945349052, Weights: [0.39497605 0.20924921 0.39577474]\n",
      "Epoch 11/100, Loss: 20.974079776712266, Weights: [0.4004478  0.19803086 0.40152134]\n",
      "Epoch 12/100, Loss: 20.632106559749037, Weights: [0.40575474 0.18708215 0.40716311]\n",
      "Epoch 13/100, Loss: 20.301280509001295, Weights: [0.41088608 0.17641767 0.41269626]\n",
      "Epoch 14/100, Loss: 19.981652591563304, Weights: [0.41583083 0.16605225 0.41811692]\n",
      "Epoch 15/100, Loss: 19.673291704928605, Weights: [0.4205779  0.15600085 0.42342125]\n",
      "Epoch 16/100, Loss: 19.376281993606938, Weights: [0.42511612 0.14627849 0.42860539]\n",
      "Epoch 17/100, Loss: 19.09071977124526, Weights: [0.42943443 0.13690003 0.43366554]\n",
      "Epoch 18/100, Loss: 18.81671001990261, Weights: [0.43352193 0.12788009 0.43859799]\n",
      "Epoch 19/100, Loss: 18.554362455761176, Weights: [0.43736805 0.11923282 0.44339912]\n",
      "Epoch 20/100, Loss: 18.30378717274175, Weights: [0.44096271 0.11097175 0.44806554]\n",
      "Epoch 21/100, Loss: 18.065089901699412, Weights: [0.44429642 0.10310952 0.45259406]\n",
      "Epoch 22/100, Loss: 17.838366951969572, Weights: [0.44736052 0.0956577  0.45698177]\n",
      "Epoch 23/100, Loss: 17.62369993223787, Weights: [0.4501473  0.08862655 0.46122614]\n",
      "Epoch 24/100, Loss: 17.421150376660787, Weights: [0.45265018 0.08202479 0.46532503]\n",
      "Epoch 25/100, Loss: 17.230754427106575, Weights: [0.45486384 0.07585939 0.46927677]\n",
      "Epoch 26/100, Loss: 17.052517740444337, Weights: [0.45678443 0.07013536 0.47308021]\n",
      "Epoch 27/100, Loss: 16.88641079838868, Weights: [0.45840964 0.06485561 0.47673475]\n",
      "Epoch 28/100, Loss: 16.732364794605857, Weights: [0.45973881 0.06002079 0.4802404 ]\n",
      "Epoch 29/100, Loss: 16.590268258774007, Weights: [0.46077303 0.05562918 0.48359779]\n",
      "Epoch 30/100, Loss: 16.45996455055035, Weights: [0.46151517 0.05167665 0.48680818]\n",
      "Epoch 31/100, Loss: 16.341250319779135, Weights: [0.46196989 0.04815667 0.48987345]\n",
      "Epoch 32/100, Loss: 16.233874985810818, Weights: [0.46214362 0.0450603  0.49279608]\n",
      "Epoch 33/100, Loss: 16.137541242325437, Weights: [0.46204451 0.04237633 0.49557916]\n",
      "Epoch 34/100, Loss: 16.0519065486653, Weights: [0.46168238 0.04009133 0.49822629]\n",
      "Epoch 35/100, Loss: 15.976585528199694, Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Epoch 36/100, Loss: 15.911153161701842, Weights: [0.46021571 0.03665477 0.50312952]\n",
      "Epoch 37/100, Loss: 15.855148641041811, Weights: [0.4591379  0.03546707 0.50539503]\n",
      "Epoch 38/100, Loss: 15.808079736373815, Weights: [0.45785017 0.03460653 0.50754329]\n",
      "Epoch 39/100, Loss: 15.769427527944163, Weights: [0.45636853 0.03405175 0.50957971]\n",
      "Epoch 40/100, Loss: 15.738651360255217, Weights: [0.45470972 0.03378041 0.51150987]\n",
      "Epoch 41/100, Loss: 15.715193889572392, Weights: [0.45289104 0.03376951 0.51333944]\n",
      "Epoch 42/100, Loss: 15.698486113375903, Weights: [0.45093021 0.03399565 0.51507413]\n",
      "Epoch 43/100, Loss: 15.68795229010463, Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Epoch 44/100, Loss: 15.683014677465803, Weights: [0.44665384 0.03506458 0.51828158]\n",
      "Epoch 45/100, Loss: 15.683098036165106, Weights: [0.44437415 0.03586037 0.51976548]\n",
      "Epoch 46/100, Loss: 15.687633862100828, Weights: [0.44202369 0.03679961 0.52117669]\n",
      "Epoch 47/100, Loss: 15.696064323285817, Weights: [0.4396197  0.03785989 0.52252041]\n",
      "Epoch 48/100, Loss: 15.70784588783994, Weights: [0.43717888 0.03901951 0.52380161]\n",
      "Epoch 49/100, Loss: 15.722452636482835, Weights: [0.43471729 0.04025768 0.52502503]\n",
      "Epoch 50/100, Loss: 15.739379257427451, Weights: [0.43225024 0.04155457 0.52619519]\n",
      "Epoch 51/100, Loss: 15.758143723939522, Weights: [0.42979221 0.04289146 0.52731633]\n",
      "Epoch 52/100, Loss: 15.778289655658192, Weights: [0.42735677 0.04425082 0.52839242]\n",
      "Epoch 53/100, Loss: 15.799388364635627, Weights: [0.42495647 0.04561636 0.52942717]\n",
      "Epoch 54/100, Loss: 15.821040586467928, Weights: [0.42260283 0.04697315 0.53042402]\n",
      "Epoch 55/100, Loss: 15.842877896296383, Weights: [0.42030628 0.04830759 0.53138613]\n",
      "Epoch 56/100, Loss: 15.86456380919971, Weights: [0.41807614 0.04960748 0.53231637]\n",
      "Epoch 57/100, Loss: 15.885794564810862, Weights: [0.41592057 0.05086204 0.53321738]\n",
      "Epoch 58/100, Loss: 15.906299597010566, Weights: [0.41384661 0.05206189 0.5340915 ]\n",
      "Epoch 59/100, Loss: 15.925841691308944, Weights: [0.41186015 0.05319902 0.53494082]\n",
      "Epoch 60/100, Loss: 15.944216834979706, Weights: [0.40996596 0.05426683 0.53576721]\n",
      "Epoch 61/100, Loss: 15.96125376804216, Weights: [0.40816774 0.05526001 0.53657225]\n",
      "Epoch 62/100, Loss: 15.976813246631288, Weights: [0.40646812 0.05617456 0.53735733]\n",
      "Epoch 63/100, Loss: 15.990787033960071, Weights: [0.40486873 0.05700767 0.5381236 ]\n",
      "Epoch 64/100, Loss: 16.003096637757405, Weights: [0.40337027 0.05775771 0.53887202]\n",
      "Epoch 65/100, Loss: 16.01369181655948, Weights: [0.40197254 0.05842412 0.53960334]\n",
      "Epoch 66/100, Loss: 16.02254888036434, Weights: [0.40067453 0.05900734 0.54031813]\n",
      "Epoch 67/100, Loss: 16.029668813781832, Weights: [0.39947449 0.05950872 0.5410168 ]\n",
      "Epoch 68/100, Loss: 16.035075251814042, Weights: [0.39836998 0.05993043 0.5416996 ]\n",
      "Epoch 69/100, Loss: 16.03881233971874, Weights: [0.39735799 0.06027537 0.54236664]\n",
      "Epoch 70/100, Loss: 16.04094250901141, Weights: [0.396435   0.06054708 0.54301792]\n",
      "Epoch 71/100, Loss: 16.041544201562772, Weights: [0.39559704 0.06074964 0.54365332]\n",
      "Epoch 72/100, Loss: 16.040709572989464, Weights: [0.39483977 0.06088758 0.54427265]\n",
      "Epoch 73/100, Loss: 16.03854220518404, Weights: [0.39415861 0.06096577 0.54487562]\n",
      "Epoch 74/100, Loss: 16.035154855971527, Weights: [0.39354873 0.06098939 0.54546188]\n",
      "Epoch 75/100, Loss: 16.030667271608113, Weights: [0.39300517 0.06096377 0.54603106]\n",
      "Epoch 76/100, Loss: 16.02520408525166, Weights: [0.3925229  0.06089436 0.54658275]\n",
      "Epoch 77/100, Loss: 16.018892821729562, Weights: [0.39209686 0.06078664 0.5471165 ]\n",
      "Epoch 78/100, Loss: 16.01186202599604, Weights: [0.39172206 0.06064605 0.54763189]\n",
      "Epoch 79/100, Loss: 16.004239529688057, Weights: [0.39139356 0.06047794 0.5481285 ]\n",
      "Epoch 80/100, Loss: 15.996150867225253, Weights: [0.39110658 0.0602875  0.54860592]\n",
      "Epoch 81/100, Loss: 15.987717850010982, Weights: [0.39085652 0.0600797  0.54906378]\n",
      "Epoch 82/100, Loss: 15.97905730452325, Weights: [0.39063899 0.05985927 0.54950175]\n",
      "Epoch 83/100, Loss: 15.97027997747161, Weights: [0.39044982 0.05963064 0.54991954]\n",
      "Epoch 84/100, Loss: 15.961489608762578, Weights: [0.39028512 0.05939795 0.55031693]\n",
      "Epoch 85/100, Loss: 15.952782170780235, Weights: [0.39014128 0.05916497 0.55069374]\n",
      "Epoch 86/100, Loss: 15.944245270460772, Weights: [0.390015   0.05893512 0.55104988]\n",
      "Epoch 87/100, Loss: 15.935957708826125, Weights: [0.38990326 0.05871144 0.5513853 ]\n",
      "Epoch 88/100, Loss: 15.927989191044707, Weights: [0.38980336 0.0584966  0.55170004]\n",
      "Epoch 89/100, Loss: 15.920400178706268, Weights: [0.38971292 0.05829289 0.5519942 ]\n",
      "Epoch 90/100, Loss: 15.913241874829893, Weights: [0.38962984 0.05810222 0.55226794]\n",
      "Epoch 91/100, Loss: 15.906556331165373, Weights: [0.38955235 0.05792615 0.55252151]\n",
      "Epoch 92/100, Loss: 15.90037666659264, Weights: [0.38947894 0.05776586 0.5527552 ]\n",
      "Epoch 93/100, Loss: 15.89472738486423, Weights: [0.3894084  0.05762221 0.55296939]\n",
      "Epoch 94/100, Loss: 15.889624779564683, Weights: [0.38933975 0.05749575 0.5531645 ]\n",
      "Epoch 95/100, Loss: 15.88507741396824, Weights: [0.38927228 0.05738672 0.55334099]\n",
      "Epoch 96/100, Loss: 15.88108666345252, Weights: [0.3892055  0.05729509 0.55349941]\n",
      "Epoch 97/100, Loss: 15.877647308259446, Weights: [0.38913909 0.05722059 0.55364032]\n",
      "Epoch 98/100, Loss: 15.874748164673269, Weights: [0.38907295 0.05716273 0.55376432]\n",
      "Epoch 99/100, Loss: 15.872372743096289, Weights: [0.38900712 0.05712082 0.55387205]\n",
      "Epoch 100/100, Loss: 15.870499922031373, Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Test Data - Final Brand: Ziploc, Confidence Score: 0.355654686729023\n",
      "Test Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Carvana, Predicted: Carvana, Confidence: 0.6525779928561535, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Febreze, Predicted: Febreze, Confidence: 0.679405779514865, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Unilever, Predicted: Dove, Confidence: 0.6289715221057501, Loss: 1\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.940125928504702, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Discover Newport, Predicted: Discover Newport, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 0.9464719319864351, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: SmartLife, Predicted: Nectar, Confidence: 0.1719217352770722, Loss: 1\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.8929438639728702, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Missouri, Predicted: Missouri, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.940125928504702, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: HelloFresh, Predicted: HelloFresh, Confidence: 0.7574579104025543, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: CREATION MUSEUM, Predicted: CREATION MUSEUM, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Ruggable, Predicted: Ruggable, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: WELLS FARGO, Predicted: WELLS FARGO, Confidence: 0.8929438639728702, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.940125928504702, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Ziploc, Predicted: Ziploc, Confidence: 0.3616217863736563, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Xfinity, Predicted: Xfinity, Confidence: 0.679145618200218, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Comcast, Predicted: Comcast, Confidence: 0.8870166313044602, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: COMCAST BUSINESS, Predicted: COMCAST BUSINESS, Confidence: 0.5542806488231159, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Xfinity Mobile, Predicted: Xfinity Mobile, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.9418325786212011, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Mastercard, Predicted: Mastercard, Confidence: 0.9464719319864351, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.7108752917872736, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: ORIS, Predicted: ORIS, Confidence: 0.8868847145903926, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Safelite, Predicted: Safelite, Confidence: 0.9464719319864351, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Baker's, Predicted: AT&T, Confidence: 0.35881155902973005, Loss: 1\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Freeform, Predicted: Freeform, Confidence: 0.4845695398902677, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Booking.com, Predicted: Booking.com, Confidence: 0.6658113912047757, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.687721058274085, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: SHERWIN-WILLIAMS, Predicted: SHERWIN-WILLIAMS, Confidence: 0.4226854774509637, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Hulu, Predicted: Hulu, Confidence: 0.26225556914855674, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: MADE IN COOKWARE, Predicted: MADE IN COOKWARE, Confidence: 0.940125928504702, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Pandora, Predicted: Pandora, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Lincoln, Predicted: Lincoln, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: U.S. Bank, Predicted: U.S. Bank, Confidence: 0.8870166313044602, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: INK MASTER, Predicted: Paramount+, Confidence: 0.1367106578701471, Loss: 1\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: MAIN EVENT, Predicted: MAIN EVENT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Amazon Prime, Predicted: Amazon Prime, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: ARIAT, Predicted: ARIAT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.9434446324243115, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.47226419973911965, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Total Loss on Training Data: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "data = copy.deepcopy(data_set)\n",
    "# Combining confidence scores with weights\n",
    "def combine_confidences(inputs, weights):\n",
    "    combined_scores = defaultdict(float)\n",
    "    for source, weight in zip(['gemini_results', 'vi_results', 'ocr_text'], weights):\n",
    "        for brand, score in inputs[source]:\n",
    "            combined_scores[brand] += weight * score\n",
    "    return combined_scores\n",
    "\n",
    "# Normalizing the scores\n",
    "def normalize_scores(combined_scores):\n",
    "    total_score = sum(combined_scores.values())\n",
    "    if total_score == 0:\n",
    "        return combined_scores\n",
    "    for brand in combined_scores:\n",
    "        combined_scores[brand] /= total_score\n",
    "    return combined_scores\n",
    "\n",
    "# Computing the final prediction\n",
    "def get_final_brand(inputs, weights):\n",
    "    combined_scores = combine_confidences(inputs, weights)\n",
    "    normalized_scores = normalize_scores(combined_scores)\n",
    "    final_brand = max(normalized_scores, key=normalized_scores.get)\n",
    "    final_confidence = normalized_scores[final_brand]\n",
    "    return final_brand, final_confidence, normalized_scores\n",
    "\n",
    "# Calculating cross-entropy loss\n",
    "def cross_entropy_loss(predicted_probs, actual_label, all_brands):\n",
    "    epsilon = 1e-15\n",
    "    predicted_probs = np.array([predicted_probs.get(brand, epsilon) for brand in all_brands])\n",
    "    predicted_probs = np.clip(predicted_probs, epsilon, 1 - epsilon)\n",
    "    actual_vector = np.array([1 if brand == actual_label else 0 for brand in all_brands])\n",
    "    return -np.sum(actual_vector * np.log(predicted_probs))\n",
    "\n",
    "# Simple binary loss for evaluation\n",
    "def calculate_loss(predicted, actual):\n",
    "    return 1 if predicted != actual else 0\n",
    "\n",
    "# Optimizing the weights using Adam optimizer\n",
    "def optimize_weights(data_set, learning_rate=0.01, epochs=100):\n",
    "    weights = np.ones(3) / 3  # Initial weights for gemini_results, vi_results, ocr_text\n",
    "    m = np.zeros(3)\n",
    "    v = np.zeros(3)\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    t = 0\n",
    "    \n",
    "    all_brands = set()\n",
    "    for entry in data_set:\n",
    "        for source in ['gemini_results', 'vi_results', 'ocr_text']:\n",
    "            all_brands.update([brand for brand, _ in entry['inputs'][source]])\n",
    "    all_brands = list(all_brands)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        weight_gradients = np.zeros(3)\n",
    "        for entry in data_set:\n",
    "            actual_final = entry['output']['final'][0]\n",
    "            final_brand, _, predicted_probs = get_final_brand(entry['inputs'], weights)\n",
    "            loss = cross_entropy_loss(predicted_probs, actual_final, all_brands)\n",
    "            total_loss += loss\n",
    "            for i, source in enumerate(['gemini_results', 'vi_results', 'ocr_text']):\n",
    "                for brand, score in entry['inputs'][source]:\n",
    "                    gradient = (predicted_probs.get(brand, 0) - (1 if brand == actual_final else 0)) * score\n",
    "                    weight_gradients[i] += gradient\n",
    "        \n",
    "        t += 1\n",
    "        m = beta1 * m + (1 - beta1) * weight_gradients\n",
    "        v = beta2 * v + (1 - beta2) * (weight_gradients ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        weights -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        weights = np.clip(weights, 0, 1)\n",
    "        weights /= np.sum(weights)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}, Weights: {weights}\")\n",
    "    return weights\n",
    "\n",
    "# Optimizing weights and testing\n",
    "final_weights = optimize_weights(data)\n",
    "\n",
    "# Test case\n",
    "test_data = [\n",
    "    {'inputs': {'gemini_results': [('Disney', 1.00), ('Ziploc', 1.00), ('SC Johnson', 1.00)], 'vi_results': [('Ziploc', 0.99), ('The Walt Disney Company', 0.99)], 'ocr_text': [('Disney', 0.90), ('Ziploc', 0.80), ('MIPS', 0.50)]}},\n",
    "]\n",
    "\n",
    "# Get predictions for test data\n",
    "for entry in test_data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    print(f\"Test Data - Final Brand: {final_brand}, Confidence Score: {final_confidence}\")\n",
    "    print(f\"Test Data - Weights: {final_weights}\")\n",
    "\n",
    "# Print results for training data and calculate total loss\n",
    "total_loss = 0\n",
    "for entry in data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    actual_final = entry['output']['final'][0]\n",
    "    loss = calculate_loss(final_brand, actual_final)\n",
    "    total_loss += loss\n",
    "    print(f\"Expected: {actual_final}, Predicted: {final_brand}, Confidence: {final_confidence}, Loss: {loss}\")\n",
    "    print(f\"Training Data - Weights: {final_weights}\")\n",
    "\n",
    "print(f\"Total Loss on Training Data: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated Code with Best Weights Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 25.01246291782739, Weights: [0.33993399 0.32013201 0.33993399]\n",
      "Epoch 2/1000, Loss: 24.556503349931294, Weights: [0.34645844 0.30707765 0.34646391]\n",
      "Epoch 3/1000, Loss: 24.112574863564728, Weights: [0.35289958 0.29418036 0.35292006]\n",
      "Epoch 4/1000, Loss: 23.68045234482656, Weights: [0.35924994 0.28145075 0.35929931]\n",
      "Epoch 5/1000, Loss: 23.259945014984943, Weights: [0.36550164 0.2688999  0.36559846]\n",
      "Epoch 6/1000, Loss: 22.85089482712831, Weights: [0.37164639 0.25653938 0.37181423]\n",
      "Epoch 7/1000, Loss: 22.45317499149614, Weights: [0.37767549 0.24438127 0.37794324]\n",
      "Epoch 8/1000, Loss: 22.066688581650215, Weights: [0.38357986 0.23243811 0.38398203]\n",
      "Epoch 9/1000, Loss: 21.691367171825096, Weights: [0.38935    0.22072293 0.38992707]\n",
      "Epoch 10/1000, Loss: 21.32716945349052, Weights: [0.39497605 0.20924921 0.39577474]\n",
      "Epoch 11/1000, Loss: 20.974079776712266, Weights: [0.4004478  0.19803086 0.40152134]\n",
      "Epoch 12/1000, Loss: 20.632106559749037, Weights: [0.40575474 0.18708215 0.40716311]\n",
      "Epoch 13/1000, Loss: 20.301280509001295, Weights: [0.41088608 0.17641767 0.41269626]\n",
      "Epoch 14/1000, Loss: 19.981652591563304, Weights: [0.41583083 0.16605225 0.41811692]\n",
      "Epoch 15/1000, Loss: 19.673291704928605, Weights: [0.4205779  0.15600085 0.42342125]\n",
      "Epoch 16/1000, Loss: 19.376281993606938, Weights: [0.42511612 0.14627849 0.42860539]\n",
      "Epoch 17/1000, Loss: 19.09071977124526, Weights: [0.42943443 0.13690003 0.43366554]\n",
      "Epoch 18/1000, Loss: 18.81671001990261, Weights: [0.43352193 0.12788009 0.43859799]\n",
      "Epoch 19/1000, Loss: 18.554362455761176, Weights: [0.43736805 0.11923282 0.44339912]\n",
      "Epoch 20/1000, Loss: 18.30378717274175, Weights: [0.44096271 0.11097175 0.44806554]\n",
      "Epoch 21/1000, Loss: 18.065089901699412, Weights: [0.44429642 0.10310952 0.45259406]\n",
      "Epoch 22/1000, Loss: 17.838366951969572, Weights: [0.44736052 0.0956577  0.45698177]\n",
      "Epoch 23/1000, Loss: 17.62369993223787, Weights: [0.4501473  0.08862655 0.46122614]\n",
      "Epoch 24/1000, Loss: 17.421150376660787, Weights: [0.45265018 0.08202479 0.46532503]\n",
      "Epoch 25/1000, Loss: 17.230754427106575, Weights: [0.45486384 0.07585939 0.46927677]\n",
      "Epoch 26/1000, Loss: 17.052517740444337, Weights: [0.45678443 0.07013536 0.47308021]\n",
      "Epoch 27/1000, Loss: 16.88641079838868, Weights: [0.45840964 0.06485561 0.47673475]\n",
      "Epoch 28/1000, Loss: 16.732364794605857, Weights: [0.45973881 0.06002079 0.4802404 ]\n",
      "Epoch 29/1000, Loss: 16.590268258774007, Weights: [0.46077303 0.05562918 0.48359779]\n",
      "Epoch 30/1000, Loss: 16.45996455055035, Weights: [0.46151517 0.05167665 0.48680818]\n",
      "Epoch 31/1000, Loss: 16.341250319779135, Weights: [0.46196989 0.04815667 0.48987345]\n",
      "Epoch 32/1000, Loss: 16.233874985810818, Weights: [0.46214362 0.0450603  0.49279608]\n",
      "Epoch 33/1000, Loss: 16.137541242325437, Weights: [0.46204451 0.04237633 0.49557916]\n",
      "Epoch 34/1000, Loss: 16.0519065486653, Weights: [0.46168238 0.04009133 0.49822629]\n",
      "Epoch 35/1000, Loss: 15.976585528199694, Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Epoch 36/1000, Loss: 15.911153161701842, Weights: [0.46021571 0.03665477 0.50312952]\n",
      "Epoch 37/1000, Loss: 15.855148641041811, Weights: [0.4591379  0.03546707 0.50539503]\n",
      "Epoch 38/1000, Loss: 15.808079736373815, Weights: [0.45785017 0.03460653 0.50754329]\n",
      "Epoch 39/1000, Loss: 15.769427527944163, Weights: [0.45636853 0.03405175 0.50957971]\n",
      "Epoch 40/1000, Loss: 15.738651360255217, Weights: [0.45470972 0.03378041 0.51150987]\n",
      "Epoch 41/1000, Loss: 15.715193889572392, Weights: [0.45289104 0.03376951 0.51333944]\n",
      "Epoch 42/1000, Loss: 15.698486113375903, Weights: [0.45093021 0.03399565 0.51507413]\n",
      "Epoch 43/1000, Loss: 15.68795229010463, Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Epoch 44/1000, Loss: 15.683014677465803, Weights: [0.44665384 0.03506458 0.51828158]\n",
      "Epoch 45/1000, Loss: 15.683098036165106, Weights: [0.44437415 0.03586037 0.51976548]\n",
      "Epoch 46/1000, Loss: 15.687633862100828, Weights: [0.44202369 0.03679961 0.52117669]\n",
      "Epoch 47/1000, Loss: 15.696064323285817, Weights: [0.4396197  0.03785989 0.52252041]\n",
      "Epoch 48/1000, Loss: 15.70784588783994, Weights: [0.43717888 0.03901951 0.52380161]\n",
      "Epoch 49/1000, Loss: 15.722452636482835, Weights: [0.43471729 0.04025768 0.52502503]\n",
      "Epoch 50/1000, Loss: 15.739379257427451, Weights: [0.43225024 0.04155457 0.52619519]\n",
      "Epoch 51/1000, Loss: 15.758143723939522, Weights: [0.42979221 0.04289146 0.52731633]\n",
      "Epoch 52/1000, Loss: 15.778289655658192, Weights: [0.42735677 0.04425082 0.52839242]\n",
      "Epoch 53/1000, Loss: 15.799388364635627, Weights: [0.42495647 0.04561636 0.52942717]\n",
      "Epoch 54/1000, Loss: 15.821040586467928, Weights: [0.42260283 0.04697315 0.53042402]\n",
      "Epoch 55/1000, Loss: 15.842877896296383, Weights: [0.42030628 0.04830759 0.53138613]\n",
      "Epoch 56/1000, Loss: 15.86456380919971, Weights: [0.41807614 0.04960748 0.53231637]\n",
      "Epoch 57/1000, Loss: 15.885794564810862, Weights: [0.41592057 0.05086204 0.53321738]\n",
      "Epoch 58/1000, Loss: 15.906299597010566, Weights: [0.41384661 0.05206189 0.5340915 ]\n",
      "Epoch 59/1000, Loss: 15.925841691308944, Weights: [0.41186015 0.05319902 0.53494082]\n",
      "Epoch 60/1000, Loss: 15.944216834979706, Weights: [0.40996596 0.05426683 0.53576721]\n",
      "Epoch 61/1000, Loss: 15.96125376804216, Weights: [0.40816774 0.05526001 0.53657225]\n",
      "Epoch 62/1000, Loss: 15.976813246631288, Weights: [0.40646812 0.05617456 0.53735733]\n",
      "Epoch 63/1000, Loss: 15.990787033960071, Weights: [0.40486873 0.05700767 0.5381236 ]\n",
      "Epoch 64/1000, Loss: 16.003096637757405, Weights: [0.40337027 0.05775771 0.53887202]\n",
      "Epoch 65/1000, Loss: 16.01369181655948, Weights: [0.40197254 0.05842412 0.53960334]\n",
      "Epoch 66/1000, Loss: 16.02254888036434, Weights: [0.40067453 0.05900734 0.54031813]\n",
      "Epoch 67/1000, Loss: 16.029668813781832, Weights: [0.39947449 0.05950872 0.5410168 ]\n",
      "Epoch 68/1000, Loss: 16.035075251814042, Weights: [0.39836998 0.05993043 0.5416996 ]\n",
      "Epoch 69/1000, Loss: 16.03881233971874, Weights: [0.39735799 0.06027537 0.54236664]\n",
      "Epoch 70/1000, Loss: 16.04094250901141, Weights: [0.396435   0.06054708 0.54301792]\n",
      "Epoch 71/1000, Loss: 16.041544201562772, Weights: [0.39559704 0.06074964 0.54365332]\n",
      "Epoch 72/1000, Loss: 16.040709572989464, Weights: [0.39483977 0.06088758 0.54427265]\n",
      "Epoch 73/1000, Loss: 16.03854220518404, Weights: [0.39415861 0.06096577 0.54487562]\n",
      "Epoch 74/1000, Loss: 16.035154855971527, Weights: [0.39354873 0.06098939 0.54546188]\n",
      "Epoch 75/1000, Loss: 16.030667271608113, Weights: [0.39300517 0.06096377 0.54603106]\n",
      "Epoch 76/1000, Loss: 16.02520408525166, Weights: [0.3925229  0.06089436 0.54658275]\n",
      "Epoch 77/1000, Loss: 16.018892821729562, Weights: [0.39209686 0.06078664 0.5471165 ]\n",
      "Epoch 78/1000, Loss: 16.01186202599604, Weights: [0.39172206 0.06064605 0.54763189]\n",
      "Epoch 79/1000, Loss: 16.004239529688057, Weights: [0.39139356 0.06047794 0.5481285 ]\n",
      "Epoch 80/1000, Loss: 15.996150867225253, Weights: [0.39110658 0.0602875  0.54860592]\n",
      "Epoch 81/1000, Loss: 15.987717850010982, Weights: [0.39085652 0.0600797  0.54906378]\n",
      "Epoch 82/1000, Loss: 15.97905730452325, Weights: [0.39063899 0.05985927 0.54950175]\n",
      "Epoch 83/1000, Loss: 15.97027997747161, Weights: [0.39044982 0.05963064 0.54991954]\n",
      "Epoch 84/1000, Loss: 15.961489608762578, Weights: [0.39028512 0.05939795 0.55031693]\n",
      "Epoch 85/1000, Loss: 15.952782170780235, Weights: [0.39014128 0.05916497 0.55069374]\n",
      "Epoch 86/1000, Loss: 15.944245270460772, Weights: [0.390015   0.05893512 0.55104988]\n",
      "Epoch 87/1000, Loss: 15.935957708826125, Weights: [0.38990326 0.05871144 0.5513853 ]\n",
      "Epoch 88/1000, Loss: 15.927989191044707, Weights: [0.38980336 0.0584966  0.55170004]\n",
      "Epoch 89/1000, Loss: 15.920400178706268, Weights: [0.38971292 0.05829289 0.5519942 ]\n",
      "Epoch 90/1000, Loss: 15.913241874829893, Weights: [0.38962984 0.05810222 0.55226794]\n",
      "Epoch 91/1000, Loss: 15.906556331165373, Weights: [0.38955235 0.05792615 0.55252151]\n",
      "Epoch 92/1000, Loss: 15.90037666659264, Weights: [0.38947894 0.05776586 0.5527552 ]\n",
      "Epoch 93/1000, Loss: 15.89472738486423, Weights: [0.3894084  0.05762221 0.55296939]\n",
      "Epoch 94/1000, Loss: 15.889624779564683, Weights: [0.38933975 0.05749575 0.5531645 ]\n",
      "Epoch 95/1000, Loss: 15.88507741396824, Weights: [0.38927228 0.05738672 0.55334099]\n",
      "Epoch 96/1000, Loss: 15.88108666345252, Weights: [0.3892055  0.05729509 0.55349941]\n",
      "Epoch 97/1000, Loss: 15.877647308259446, Weights: [0.38913909 0.05722059 0.55364032]\n",
      "Epoch 98/1000, Loss: 15.874748164673269, Weights: [0.38907295 0.05716273 0.55376432]\n",
      "Epoch 99/1000, Loss: 15.872372743096289, Weights: [0.38900712 0.05712082 0.55387205]\n",
      "Epoch 100/1000, Loss: 15.870499922031373, Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Epoch 101/1000, Loss: 15.869104627613021, Weights: [0.38887726 0.05708133 0.55404141]\n",
      "Epoch 102/1000, Loss: 15.868158509050021, Weights: [0.38881392 0.05708167 0.55410441]\n",
      "Epoch 103/1000, Loss: 15.867630601138112, Weights: [0.38875224 0.05709386 0.5541539 ]\n",
      "Epoch 104/1000, Loss: 15.867487965855686, Weights: [0.38869275 0.05711665 0.55419059]\n",
      "Epoch 105/1000, Loss: 15.867696305954164, Weights: [0.38863602 0.05714878 0.5542152 ]\n",
      "Epoch 106/1000, Loss: 15.868220544383162, Weights: [0.38858262 0.05718896 0.55422841]\n",
      "Epoch 107/1000, Loss: 15.869025364335199, Weights: [0.38853316 0.05723591 0.55423093]\n",
      "Epoch 108/1000, Loss: 15.870075705641055, Weights: [0.3884882  0.05728838 0.55422342]\n",
      "Epoch 109/1000, Loss: 15.871337214183994, Weights: [0.38844832 0.05734514 0.55420654]\n",
      "Epoch 110/1000, Loss: 15.872776641915399, Weights: [0.38841402 0.05740505 0.55418092]\n",
      "Epoch 111/1000, Loss: 15.874362195936532, Weights: [0.38838581 0.05746702 0.55414717]\n",
      "Epoch 112/1000, Loss: 15.87606383595023, Weights: [0.3883641  0.05753003 0.55410587]\n",
      "Epoch 113/1000, Loss: 15.87785352017431, Weights: [0.38834929 0.05759315 0.55405757]\n",
      "Epoch 114/1000, Loss: 15.879705400537727, Weights: [0.38834168 0.05765555 0.55400278]\n",
      "Epoch 115/1000, Loss: 15.881595968644001, Weights: [0.38834153 0.05771648 0.55394199]\n",
      "Epoch 116/1000, Loss: 15.883504154580185, Weights: [0.38834903 0.05777531 0.55387566]\n",
      "Epoch 117/1000, Loss: 15.885411381168913, Weights: [0.38836431 0.05783148 0.55380421]\n",
      "Epoch 118/1000, Loss: 15.887301576703711, Weights: [0.38838741 0.05788455 0.55372804]\n",
      "Epoch 119/1000, Loss: 15.889161149572512, Weights: [0.38841835 0.05793416 0.55364749]\n",
      "Epoch 120/1000, Loss: 15.8909789284611, Weights: [0.38845706 0.05798004 0.55356291]\n",
      "Epoch 121/1000, Loss: 15.892746072038152, Weights: [0.3885034  0.05802202 0.55347458]\n",
      "Epoch 122/1000, Loss: 15.894455952158838, Weights: [0.38855723 0.05805999 0.55338278]\n",
      "Epoch 123/1000, Loss: 15.896104014687955, Weights: [0.3886183  0.05809394 0.55328775]\n",
      "Epoch 124/1000, Loss: 15.897687622040653, Weights: [0.38868637 0.05812391 0.55318972]\n",
      "Epoch 125/1000, Loss: 15.899205881473113, Weights: [0.38876113 0.05814999 0.55308888]\n",
      "Epoch 126/1000, Loss: 15.900659463033895, Weights: [0.38884225 0.05817234 0.5529854 ]\n",
      "Epoch 127/1000, Loss: 15.90205041091419, Weights: [0.38892939 0.05819117 0.55287945]\n",
      "Epoch 128/1000, Loss: 15.903381951718709, Weights: [0.38902215 0.0582067  0.55277115]\n",
      "Epoch 129/1000, Loss: 15.904658302925533, Weights: [0.38912016 0.05821921 0.55266064]\n",
      "Epoch 130/1000, Loss: 15.905884484519154, Weights: [0.38922301 0.05822897 0.55254802]\n",
      "Epoch 131/1000, Loss: 15.907066136473517, Weights: [0.3893303 0.0582363 0.5524334]\n",
      "Epoch 132/1000, Loss: 15.908209344437674, Weights: [0.38944164 0.05824151 0.55231686]\n",
      "Epoch 133/1000, Loss: 15.909320475642065, Weights: [0.38955662 0.0582449  0.55219848]\n",
      "Epoch 134/1000, Loss: 15.910406026704779, Weights: [0.38967486 0.05824679 0.55207834]\n",
      "Epoch 135/1000, Loss: 15.911472484680013, Weights: [0.38979599 0.05824749 0.55195651]\n",
      "Epoch 136/1000, Loss: 15.912526202360748, Weights: [0.38991965 0.05824729 0.55183306]\n",
      "Epoch 137/1000, Loss: 15.913573288529081, Weights: [0.39004549 0.05824646 0.55170804]\n",
      "Epoch 138/1000, Loss: 15.914619513544768, Weights: [0.39017321 0.05824526 0.55158153]\n",
      "Epoch 139/1000, Loss: 15.915670230379162, Weights: [0.3903025  0.05824393 0.55145358]\n",
      "Epoch 140/1000, Loss: 15.916730310940842, Weights: [0.39043308 0.05824267 0.55132424]\n",
      "Epoch 141/1000, Loss: 15.91780409730278, Weights: [0.39056472 0.05824168 0.5511936 ]\n",
      "Epoch 142/1000, Loss: 15.918895367231475, Weights: [0.39069718 0.05824112 0.5510617 ]\n",
      "Epoch 143/1000, Loss: 15.920007313236587, Weights: [0.39083026 0.05824113 0.55092861]\n",
      "Epoch 144/1000, Loss: 15.921142534206199, Weights: [0.39096379 0.05824182 0.55079439]\n",
      "Epoch 145/1000, Loss: 15.922303038568248, Weights: [0.39109761 0.05824328 0.55065912]\n",
      "Epoch 146/1000, Loss: 15.923490257822149, Weights: [0.39123158 0.05824556 0.55052285]\n",
      "Epoch 147/1000, Loss: 15.92470506921578, Weights: [0.39136561 0.05824872 0.55038567]\n",
      "Epoch 148/1000, Loss: 15.925947826300485, Weights: [0.3914996  0.05825277 0.55024763]\n",
      "Epoch 149/1000, Loss: 15.927218396079224, Weights: [0.39163347 0.05825771 0.55010882]\n",
      "Epoch 150/1000, Loss: 15.928516201468119, Weights: [0.39176717 0.05826353 0.5499693 ]\n",
      "Epoch 151/1000, Loss: 15.92984026781866, Weights: [0.39190066 0.0582702  0.54982914]\n",
      "Epoch 152/1000, Loss: 15.931189272292775, Weights: [0.3920339  0.05827767 0.54968843]\n",
      "Epoch 153/1000, Loss: 15.932561594944987, Weights: [0.39216689 0.05828589 0.54954722]\n",
      "Epoch 154/1000, Loss: 15.93395537044188, Weights: [0.39229962 0.05829479 0.54940559]\n",
      "Epoch 155/1000, Loss: 15.935368539436556, Weights: [0.39243208 0.05830431 0.54926362]\n",
      "Epoch 156/1000, Loss: 15.936798898712333, Weights: [0.39256428 0.05831437 0.54912136]\n",
      "Epoch 157/1000, Loss: 15.938244149313482, Weights: [0.39269623 0.05832488 0.54897889]\n",
      "Epoch 158/1000, Loss: 15.93970194198843, Weights: [0.39282795 0.05833578 0.54883627]\n",
      "Epoch 159/1000, Loss: 15.941169919380643, Weights: [0.39295946 0.05834698 0.54869356]\n",
      "Epoch 160/1000, Loss: 15.942645754512489, Weights: [0.39309077 0.05835841 0.54855082]\n",
      "Epoch 161/1000, Loss: 15.944127185215384, Weights: [0.3932219  0.05836999 0.54840811]\n",
      "Epoch 162/1000, Loss: 15.94561204426405, Weights: [0.39335286 0.05838165 0.54826549]\n",
      "Epoch 163/1000, Loss: 15.947098285072187, Weights: [0.39348368 0.05839333 0.54812299]\n",
      "Epoch 164/1000, Loss: 15.948584002899839, Weights: [0.39361437 0.05840497 0.54798067]\n",
      "Epoch 165/1000, Loss: 15.950067451608392, Weights: [0.39374492 0.05841651 0.54783857]\n",
      "Epoch 166/1000, Loss: 15.95154705607627, Weights: [0.39387536 0.05842791 0.54769672]\n",
      "Epoch 167/1000, Loss: 15.953021420456734, Weights: [0.39400568 0.05843914 0.54755518]\n",
      "Epoch 168/1000, Loss: 15.954489332517857, Weights: [0.39413589 0.05845015 0.54741396]\n",
      "Epoch 169/1000, Loss: 15.955949764354118, Weights: [0.39426598 0.05846092 0.5472731 ]\n",
      "Epoch 170/1000, Loss: 15.957401869798272, Weights: [0.39439594 0.05847143 0.54713263]\n",
      "Epoch 171/1000, Loss: 15.958844978892254, Weights: [0.39452576 0.05848167 0.54699257]\n",
      "Epoch 172/1000, Loss: 15.960278589796477, Weights: [0.39465543 0.05849164 0.54685293]\n",
      "Epoch 173/1000, Loss: 15.96170235852858, Weights: [0.39478493 0.05850132 0.54671375]\n",
      "Epoch 174/1000, Loss: 15.963116086926506, Weights: [0.39491424 0.05851073 0.54657503]\n",
      "Epoch 175/1000, Loss: 15.964519709226462, Weights: [0.39504335 0.05851987 0.54643678]\n",
      "Epoch 176/1000, Loss: 15.965913277635709, Weights: [0.39517223 0.05852875 0.54629902]\n",
      "Epoch 177/1000, Loss: 15.967296947262975, Weights: [0.39530086 0.05853738 0.54616176]\n",
      "Epoch 178/1000, Loss: 15.968670960747344, Weights: [0.39542921 0.05854579 0.546025  ]\n",
      "Epoch 179/1000, Loss: 15.970035632899894, Weights: [0.39555726 0.05855399 0.54588875]\n",
      "Epoch 180/1000, Loss: 15.971391335642574, Weights: [0.39568498 0.058562   0.54575302]\n",
      "Epoch 181/1000, Loss: 15.972738483496261, Weights: [0.39581235 0.05856984 0.54561781]\n",
      "Epoch 182/1000, Loss: 15.97407751983567, Weights: [0.39593935 0.05857753 0.54548312]\n",
      "Epoch 183/1000, Loss: 15.975408904093843, Weights: [0.39606594 0.0585851  0.54534896]\n",
      "Epoch 184/1000, Loss: 15.976733100063237, Weights: [0.39619212 0.05859256 0.54521532]\n",
      "Epoch 185/1000, Loss: 15.978050565405857, Weights: [0.39631785 0.05859994 0.5450822 ]\n",
      "Epoch 186/1000, Loss: 15.979361742450866, Weights: [0.39644312 0.05860726 0.54494962]\n",
      "Epoch 187/1000, Loss: 15.980667050326103, Weights: [0.39656792 0.05861452 0.54481756]\n",
      "Epoch 188/1000, Loss: 15.981966878439755, Weights: [0.39669222 0.05862175 0.54468603]\n",
      "Epoch 189/1000, Loss: 15.983261581301107, Weights: [0.39681601 0.05862896 0.54455503]\n",
      "Epoch 190/1000, Loss: 15.984551474644167, Weights: [0.39693928 0.05863617 0.54442456]\n",
      "Epoch 191/1000, Loss: 15.985836832796622, Weights: [0.39706202 0.05864337 0.54429461]\n",
      "Epoch 192/1000, Loss: 15.987117887217643, Weights: [0.39718422 0.05865059 0.54416519]\n",
      "Epoch 193/1000, Loss: 15.988394826112867, Weights: [0.39730587 0.05865783 0.5440363 ]\n",
      "Epoch 194/1000, Loss: 15.989667795022756, Weights: [0.39742698 0.05866509 0.54390794]\n",
      "Epoch 195/1000, Loss: 15.990936898271377, Weights: [0.39754753 0.05867237 0.5437801 ]\n",
      "Epoch 196/1000, Loss: 15.99220220115705, Weights: [0.39766752 0.05867968 0.5436528 ]\n",
      "Epoch 197/1000, Loss: 15.993463732762937, Weights: [0.39778696 0.05868702 0.54352602]\n",
      "Epoch 198/1000, Loss: 15.99472148926563, Weights: [0.39790584 0.05869438 0.54339978]\n",
      "Epoch 199/1000, Loss: 15.995975437621707, Weights: [0.39802417 0.05870177 0.54327406]\n",
      "Epoch 200/1000, Loss: 15.997225519516537, Weights: [0.39814194 0.05870918 0.54314888]\n",
      "Epoch 201/1000, Loss: 15.99847165546591, Weights: [0.39825918 0.0587166  0.54302422]\n",
      "Epoch 202/1000, Loss: 15.999713748968746, Weights: [0.39837586 0.05872404 0.5429001 ]\n",
      "Epoch 203/1000, Loss: 16.000951690618177, Weights: [0.39849202 0.05873148 0.5427765 ]\n",
      "Epoch 204/1000, Loss: 16.002185362088564, Weights: [0.39860763 0.05873893 0.54265344]\n",
      "Epoch 205/1000, Loss: 16.003414639926397, Weights: [0.39872273 0.05874637 0.54253091]\n",
      "Epoch 206/1000, Loss: 16.004639399084688, Weights: [0.3988373  0.0587538  0.54240891]\n",
      "Epoch 207/1000, Loss: 16.00585951615138, Weights: [0.39895135 0.05876121 0.54228744]\n",
      "Epoch 208/1000, Loss: 16.007074872233794, Weights: [0.3990649  0.05876861 0.54216649]\n",
      "Epoch 209/1000, Loss: 16.00828535547201, Weights: [0.39917794 0.05877598 0.54204608]\n",
      "Epoch 210/1000, Loss: 16.00949086316443, Weights: [0.39929048 0.05878332 0.5419262 ]\n",
      "Epoch 211/1000, Loss: 16.010691303498717, Weights: [0.39940253 0.05879063 0.54180684]\n",
      "Epoch 212/1000, Loss: 16.011886596890193, Weights: [0.39951409 0.05879791 0.541688  ]\n",
      "Epoch 213/1000, Loss: 16.013076676937683, Weights: [0.39962517 0.05880514 0.54156969]\n",
      "Epoch 214/1000, Loss: 16.014261491014192, Weights: [0.39973576 0.05881233 0.5414519 ]\n",
      "Epoch 215/1000, Loss: 16.015441000515512, Weights: [0.39984589 0.05881948 0.54133463]\n",
      "Epoch 216/1000, Loss: 16.016615180795004, Weights: [0.39995554 0.05882658 0.54121788]\n",
      "Epoch 217/1000, Loss: 16.01778402081658, Weights: [0.40006472 0.05883364 0.54110164]\n",
      "Epoch 218/1000, Loss: 16.018947522560815, Weights: [0.40017344 0.05884066 0.54098591]\n",
      "Epoch 219/1000, Loss: 16.02010570022091, Weights: [0.40028169 0.05884762 0.54087069]\n",
      "Epoch 220/1000, Loss: 16.021258579226025, Weights: [0.40038949 0.05885454 0.54075597]\n",
      "Epoch 221/1000, Loss: 16.022406195129715, Weights: [0.40049683 0.05886142 0.54064175]\n",
      "Epoch 222/1000, Loss: 16.02354859240009, Weights: [0.40060371 0.05886825 0.54052803]\n",
      "Epoch 223/1000, Loss: 16.024685823147035, Weights: [0.40071015 0.05887504 0.54041481]\n",
      "Epoch 224/1000, Loss: 16.025817945819668, Weights: [0.40081613 0.05888179 0.54030208]\n",
      "Epoch 225/1000, Loss: 16.02694502390446, Weights: [0.40092166 0.0588885  0.54018983]\n",
      "Epoch 226/1000, Loss: 16.028067124651677, Weights: [0.40102675 0.05889518 0.54007807]\n",
      "Epoch 227/1000, Loss: 16.029184317854185, Weights: [0.4011314  0.05890181 0.53996679]\n",
      "Epoch 228/1000, Loss: 16.030296674699425, Weights: [0.4012356  0.05890842 0.53985599]\n",
      "Epoch 229/1000, Loss: 16.03140426671153, Weights: [0.40133936 0.05891499 0.53974566]\n",
      "Epoch 230/1000, Loss: 16.03250716479709, Weights: [0.40144268 0.05892153 0.53963579]\n",
      "Epoch 231/1000, Loss: 16.03360543840433, Weights: [0.40154556 0.05892804 0.53952639]\n",
      "Epoch 232/1000, Loss: 16.034699154802283, Weights: [0.40164801 0.05893453 0.53941746]\n",
      "Epoch 233/1000, Loss: 16.035788378482927, Weights: [0.40175003 0.05894099 0.53930898]\n",
      "Epoch 234/1000, Loss: 16.036873170686693, Weights: [0.40185162 0.05894742 0.53920096]\n",
      "Epoch 235/1000, Loss: 16.03795358904864, Weights: [0.40195278 0.05895383 0.53909338]\n",
      "Epoch 236/1000, Loss: 16.039029687360593, Weights: [0.40205352 0.05896022 0.53898626]\n",
      "Epoch 237/1000, Loss: 16.04010151544209, Weights: [0.40215384 0.05896659 0.53887958]\n",
      "Epoch 238/1000, Loss: 16.041169119111693, Weights: [0.40225373 0.05897293 0.53877333]\n",
      "Epoch 239/1000, Loss: 16.04223254024855, Weights: [0.40235321 0.05897926 0.53866753]\n",
      "Epoch 240/1000, Loss: 16.043291816933355, Weights: [0.40245228 0.05898556 0.53856216]\n",
      "Epoch 241/1000, Loss: 16.04434698365707, Weights: [0.40255093 0.05899185 0.53845722]\n",
      "Epoch 242/1000, Loss: 16.045398071585428, Weights: [0.40264918 0.05899811 0.53835271]\n",
      "Epoch 243/1000, Loss: 16.046445108867395, Weights: [0.40274702 0.05900436 0.53824862]\n",
      "Epoch 244/1000, Loss: 16.04748812097573, Weights: [0.40284446 0.05901058 0.53814496]\n",
      "Epoch 245/1000, Loss: 16.048527131068564, Weights: [0.4029415  0.05901679 0.53804171]\n",
      "Epoch 246/1000, Loss: 16.049562160361265, Weights: [0.40303815 0.05902297 0.53793888]\n",
      "Epoch 247/1000, Loss: 16.050593228499075, Weights: [0.40313441 0.05902913 0.53783646]\n",
      "Epoch 248/1000, Loss: 16.05162035392169, Weights: [0.40323027 0.05903527 0.53773445]\n",
      "Epoch 249/1000, Loss: 16.05264355421211, Weights: [0.40332576 0.05904139 0.53763285]\n",
      "Epoch 250/1000, Loss: 16.053662846423432, Weights: [0.40342086 0.05904749 0.53753165]\n",
      "Epoch 251/1000, Loss: 16.054678247378064, Weights: [0.40351558 0.05905357 0.53743086]\n",
      "Epoch 252/1000, Loss: 16.05568977393544, Weights: [0.40360992 0.05905962 0.53733046]\n",
      "Epoch 253/1000, Loss: 16.056697443225072, Weights: [0.40370389 0.05906565 0.53723045]\n",
      "Epoch 254/1000, Loss: 16.057701272843232, Weights: [0.40379749 0.05907166 0.53713084]\n",
      "Epoch 255/1000, Loss: 16.058701281012326, Weights: [0.40389073 0.05907765 0.53703162]\n",
      "Epoch 256/1000, Loss: 16.059697486703204, Weights: [0.4039836  0.05908361 0.53693279]\n",
      "Epoch 257/1000, Loss: 16.06068990972123, Weights: [0.40407611 0.05908955 0.53683434]\n",
      "Epoch 258/1000, Loss: 16.061678570758062, Weights: [0.40416826 0.05909547 0.53673627]\n",
      "Epoch 259/1000, Loss: 16.0626634914113, Weights: [0.40426005 0.05910136 0.53663858]\n",
      "Epoch 260/1000, Loss: 16.063644694175, Weights: [0.4043515  0.05910723 0.53654127]\n",
      "Epoch 261/1000, Loss: 16.064622202404315, Weights: [0.40444259 0.05911308 0.53644433]\n",
      "Epoch 262/1000, Loss: 16.06559604025773, Weights: [0.40453333 0.05911891 0.53634776]\n",
      "Epoch 263/1000, Loss: 16.066566232620747, Weights: [0.40462373 0.05912471 0.53625156]\n",
      "Epoch 264/1000, Loss: 16.06753280501465, Weights: [0.40471378 0.05913049 0.53615573]\n",
      "Epoch 265/1000, Loss: 16.068495783494168, Weights: [0.4048035  0.05913624 0.53606026]\n",
      "Epoch 266/1000, Loss: 16.069455194537714, Weights: [0.40489287 0.05914198 0.53596515]\n",
      "Epoch 267/1000, Loss: 16.070411064933506, Weights: [0.40498191 0.05914769 0.53587039]\n",
      "Epoch 268/1000, Loss: 16.071363421664852, Weights: [0.40507062 0.05915338 0.535776  ]\n",
      "Epoch 269/1000, Loss: 16.072312291797463, Weights: [0.405159   0.05915905 0.53568195]\n",
      "Epoch 270/1000, Loss: 16.073257702371315, Weights: [0.40524704 0.0591647  0.53558825]\n",
      "Epoch 271/1000, Loss: 16.07419968029918, Weights: [0.40533476 0.05917033 0.53549491]\n",
      "Epoch 272/1000, Loss: 16.07513825227373, Weights: [0.40542215 0.05917594 0.53540191]\n",
      "Epoch 273/1000, Loss: 16.07607344468451, Weights: [0.40550923 0.05918153 0.53530925]\n",
      "Epoch 274/1000, Loss: 16.07700528354586, Weights: [0.40559598 0.0591871  0.53521693]\n",
      "Epoch 275/1000, Loss: 16.07793379443635, Weights: [0.40568241 0.05919264 0.53512495]\n",
      "Epoch 276/1000, Loss: 16.078859002450226, Weights: [0.40576853 0.05919817 0.5350333 ]\n",
      "Epoch 277/1000, Loss: 16.07978093216058, Weights: [0.40585433 0.05920368 0.53494199]\n",
      "Epoch 278/1000, Loss: 16.080699607594198, Weights: [0.40593982 0.05920917 0.53485101]\n",
      "Epoch 279/1000, Loss: 16.08161505221738, Weights: [0.406025   0.05921464 0.53476035]\n",
      "Epoch 280/1000, Loss: 16.082527288932077, Weights: [0.40610988 0.05922009 0.53467003]\n",
      "Epoch 281/1000, Loss: 16.083436340081292, Weights: [0.40619445 0.05922553 0.53458003]\n",
      "Epoch 282/1000, Loss: 16.084342227462834, Weights: [0.40627871 0.05923094 0.53449035]\n",
      "Epoch 283/1000, Loss: 16.08524497235011, Weights: [0.40636268 0.05923634 0.53440099]\n",
      "Epoch 284/1000, Loss: 16.086144595518917, Weights: [0.40644634 0.05924171 0.53431195]\n",
      "Epoch 285/1000, Loss: 16.0870411172788, Weights: [0.40652971 0.05924707 0.53422322]\n",
      "Epoch 286/1000, Loss: 16.087934557508003, Weights: [0.40661278 0.05925242 0.53413481]\n",
      "Epoch 287/1000, Loss: 16.08882493569056, Weights: [0.40669555 0.05925774 0.53404671]\n",
      "Epoch 288/1000, Loss: 16.089712270954685, Weights: [0.40677804 0.05926304 0.53395891]\n",
      "Epoch 289/1000, Loss: 16.09059658211122, Weights: [0.40686024 0.05926833 0.53387143]\n",
      "Epoch 290/1000, Loss: 16.09147788769137, Weights: [0.40694215 0.0592736  0.53378425]\n",
      "Epoch 291/1000, Loss: 16.092356205982774, Weights: [0.40702377 0.05927885 0.53369738]\n",
      "Epoch 292/1000, Loss: 16.0932315550634, Weights: [0.40710511 0.05928409 0.5336108 ]\n",
      "Epoch 293/1000, Loss: 16.09410395283257, Weights: [0.40718617 0.0592893  0.53352453]\n",
      "Epoch 294/1000, Loss: 16.094973417038684, Weights: [0.40726695 0.0592945  0.53343855]\n",
      "Epoch 295/1000, Loss: 16.095839965303476, Weights: [0.40734745 0.05929969 0.53335287]\n",
      "Epoch 296/1000, Loss: 16.096703615142506, Weights: [0.40742767 0.05930485 0.53326748]\n",
      "Epoch 297/1000, Loss: 16.09756438398175, Weights: [0.40750762 0.05931    0.53318239]\n",
      "Epoch 298/1000, Loss: 16.09842228917056, Weights: [0.40758729 0.05931513 0.53309758]\n",
      "Epoch 299/1000, Loss: 16.09927734799087, Weights: [0.4076667  0.05932024 0.53301306]\n",
      "Epoch 300/1000, Loss: 16.10012957766299, Weights: [0.40774583 0.05932533 0.53292883]\n",
      "Epoch 301/1000, Loss: 16.10097899534824, Weights: [0.4078247  0.05933041 0.53284489]\n",
      "Epoch 302/1000, Loss: 16.101825618148702, Weights: [0.40790331 0.05933547 0.53276122]\n",
      "Epoch 303/1000, Loss: 16.102669463104473, Weights: [0.40798164 0.05934052 0.53267784]\n",
      "Epoch 304/1000, Loss: 16.10351054718884, Weights: [0.40805972 0.05934555 0.53259474]\n",
      "Epoch 305/1000, Loss: 16.104348887301725, Weights: [0.40813753 0.05935056 0.53251191]\n",
      "Epoch 306/1000, Loss: 16.105184500261835, Weights: [0.40821509 0.05935555 0.53242936]\n",
      "Epoch 307/1000, Loss: 16.10601740279785, Weights: [0.40829239 0.05936053 0.53234708]\n",
      "Epoch 308/1000, Loss: 16.10684761153914, Weights: [0.40836943 0.05936549 0.53226508]\n",
      "Epoch 309/1000, Loss: 16.107675143006226, Weights: [0.40844622 0.05937044 0.53218335]\n",
      "Epoch 310/1000, Loss: 16.10850001360138, Weights: [0.40852275 0.05937537 0.53210188]\n",
      "Epoch 311/1000, Loss: 16.10932223959961, Weights: [0.40859903 0.05938028 0.53202068]\n",
      "Epoch 312/1000, Loss: 16.110141837140326, Weights: [0.40867507 0.05938518 0.53193975]\n",
      "Epoch 313/1000, Loss: 16.110958822219782, Weights: [0.40875085 0.05939006 0.53185909]\n",
      "Epoch 314/1000, Loss: 16.111773210684536, Weights: [0.40882639 0.05939493 0.53177868]\n",
      "Epoch 315/1000, Loss: 16.112585018226007, Weights: [0.40890168 0.05939978 0.53169854]\n",
      "Epoch 316/1000, Loss: 16.113394260376186, Weights: [0.40897673 0.05940461 0.53161865]\n",
      "Epoch 317/1000, Loss: 16.114200952504568, Weights: [0.40905154 0.05940943 0.53153903]\n",
      "Epoch 318/1000, Loss: 16.11500510981625, Weights: [0.4091261  0.05941424 0.53145966]\n",
      "Epoch 319/1000, Loss: 16.115806747351204, Weights: [0.40920043 0.05941903 0.53138054]\n",
      "Epoch 320/1000, Loss: 16.116605879984697, Weights: [0.40927452 0.0594238  0.53130168]\n",
      "Epoch 321/1000, Loss: 16.117402522428588, Weights: [0.40934837 0.05942856 0.53122307]\n",
      "Epoch 322/1000, Loss: 16.11819668923368, Weights: [0.40942199 0.05943331 0.53114471]\n",
      "Epoch 323/1000, Loss: 16.11898839479279, Weights: [0.40949537 0.05943804 0.53106659]\n",
      "Epoch 324/1000, Loss: 16.11977765334445, Weights: [0.40956852 0.05944275 0.53098873]\n",
      "Epoch 325/1000, Loss: 16.12056447897722, Weights: [0.40964144 0.05944745 0.53091111]\n",
      "Epoch 326/1000, Loss: 16.121348885634323, Weights: [0.40971413 0.05945214 0.53083373]\n",
      "Epoch 327/1000, Loss: 16.122130887118605, Weights: [0.40978659 0.05945681 0.5307566 ]\n",
      "Epoch 328/1000, Loss: 16.122910497097585, Weights: [0.40985882 0.05946147 0.53067971]\n",
      "Epoch 329/1000, Loss: 16.123687729108575, Weights: [0.40993083 0.05946611 0.53060306]\n",
      "Epoch 330/1000, Loss: 16.124462596563713, Weights: [0.41000262 0.05947074 0.53052665]\n",
      "Epoch 331/1000, Loss: 16.125235112754822, Weights: [0.41007418 0.05947535 0.53045047]\n",
      "Epoch 332/1000, Loss: 16.126005290858004, Weights: [0.41014551 0.05947995 0.53037454]\n",
      "Epoch 333/1000, Loss: 16.126773143937996, Weights: [0.41021663 0.05948453 0.53029883]\n",
      "Epoch 334/1000, Loss: 16.127538684952054, Weights: [0.41028753 0.05948911 0.53022336]\n",
      "Epoch 335/1000, Loss: 16.128301926753565, Weights: [0.41035821 0.05949366 0.53014812]\n",
      "Epoch 336/1000, Loss: 16.129062882095127, Weights: [0.41042868 0.05949821 0.53007311]\n",
      "Epoch 337/1000, Loss: 16.129821563631324, Weights: [0.41049893 0.05950274 0.52999834]\n",
      "Epoch 338/1000, Loss: 16.130577983920972, Weights: [0.41056896 0.05950725 0.52992379]\n",
      "Epoch 339/1000, Loss: 16.131332155429007, Weights: [0.41063878 0.05951176 0.52984946]\n",
      "Epoch 340/1000, Loss: 16.132084090528032, Weights: [0.41070839 0.05951625 0.52977536]\n",
      "Epoch 341/1000, Loss: 16.13283380149944, Weights: [0.41077779 0.05952072 0.52970149]\n",
      "Epoch 342/1000, Loss: 16.133581300534257, Weights: [0.41084698 0.05952518 0.52962784]\n",
      "Epoch 343/1000, Loss: 16.13432659973371, Weights: [0.41091596 0.05952963 0.52955441]\n",
      "Epoch 344/1000, Loss: 16.135069711109587, Weights: [0.41098473 0.05953407 0.5294812 ]\n",
      "Epoch 345/1000, Loss: 16.135810646584353, Weights: [0.4110533  0.05953849 0.52940821]\n",
      "Epoch 346/1000, Loss: 16.13654941799119, Weights: [0.41112167 0.0595429  0.52933544]\n",
      "Epoch 347/1000, Loss: 16.13728603707388, Weights: [0.41118982 0.05954729 0.52926288]\n",
      "Epoch 348/1000, Loss: 16.138020515486712, Weights: [0.41125778 0.05955168 0.52919054]\n",
      "Epoch 349/1000, Loss: 16.138752864794288, Weights: [0.41132554 0.05955605 0.52911842]\n",
      "Epoch 350/1000, Loss: 16.13948309647137, Weights: [0.41139309 0.05956041 0.5290465 ]\n",
      "Epoch 351/1000, Loss: 16.140211221902835, Weights: [0.41146045 0.05956475 0.5289748 ]\n",
      "Epoch 352/1000, Loss: 16.140937252383587, Weights: [0.4115276  0.05956908 0.52890331]\n",
      "Epoch 353/1000, Loss: 16.14166119911868, Weights: [0.41159456 0.0595734  0.52883204]\n",
      "Epoch 354/1000, Loss: 16.14238307322343, Weights: [0.41166133 0.05957771 0.52876097]\n",
      "Epoch 355/1000, Loss: 16.143102885723742, Weights: [0.4117279 0.059582  0.5286901]\n",
      "Epoch 356/1000, Loss: 16.143820647556495, Weights: [0.41179427 0.05958628 0.52861945]\n",
      "Epoch 357/1000, Loss: 16.144536369570062, Weights: [0.41186045 0.05959055 0.528549  ]\n",
      "Epoch 358/1000, Loss: 16.145250062524973, Weights: [0.41192644 0.05959481 0.52847875]\n",
      "Epoch 359/1000, Loss: 16.145961737094638, Weights: [0.41199224 0.05959905 0.5284087 ]\n",
      "Epoch 360/1000, Loss: 16.14667140386623, Weights: [0.41205785 0.05960329 0.52833886]\n",
      "Epoch 361/1000, Loss: 16.14737907334161, Weights: [0.41212327 0.05960751 0.52826922]\n",
      "Epoch 362/1000, Loss: 16.14808475593837, Weights: [0.4121885  0.05961171 0.52819978]\n",
      "Epoch 363/1000, Loss: 16.148788461990936, Weights: [0.41225355 0.05961591 0.52813054]\n",
      "Epoch 364/1000, Loss: 16.14949020175166, Weights: [0.41231841 0.05962009 0.52806149]\n",
      "Epoch 365/1000, Loss: 16.150189985392025, Weights: [0.41238309 0.05962427 0.52799265]\n",
      "Epoch 366/1000, Loss: 16.15088782300384, Weights: [0.41244758 0.05962843 0.527924  ]\n",
      "Epoch 367/1000, Loss: 16.15158372460044, Weights: [0.41251189 0.05963258 0.52785554]\n",
      "Epoch 368/1000, Loss: 16.152277700117835, Weights: [0.41257601 0.05963671 0.52778727]\n",
      "Epoch 369/1000, Loss: 16.152969759415956, Weights: [0.41263996 0.05964084 0.5277192 ]\n",
      "Epoch 370/1000, Loss: 16.15365991227971, Weights: [0.41270372 0.05964495 0.52765133]\n",
      "Epoch 371/1000, Loss: 16.15434816842016, Weights: [0.41276731 0.05964905 0.52758364]\n",
      "Epoch 372/1000, Loss: 16.15503453747554, Weights: [0.41283072 0.05965315 0.52751614]\n",
      "Epoch 373/1000, Loss: 16.155719029012285, Weights: [0.41289395 0.05965722 0.52744883]\n",
      "Epoch 374/1000, Loss: 16.15640165252598, Weights: [0.412957   0.05966129 0.52738171]\n",
      "Epoch 375/1000, Loss: 16.15708241744226, Weights: [0.41301988 0.05966535 0.52731477]\n",
      "Epoch 376/1000, Loss: 16.157761333117683, Weights: [0.41308259 0.05966939 0.52724802]\n",
      "Epoch 377/1000, Loss: 16.15843840884049, Weights: [0.41314512 0.05967343 0.52718146]\n",
      "Epoch 378/1000, Loss: 16.15911365383136, Weights: [0.41320747 0.05967745 0.52711507]\n",
      "Epoch 379/1000, Loss: 16.15978707724412, Weights: [0.41326966 0.05968146 0.52704888]\n",
      "Epoch 380/1000, Loss: 16.160458688166372, Weights: [0.41333168 0.05968546 0.52698286]\n",
      "Epoch 381/1000, Loss: 16.16112849562013, Weights: [0.41339352 0.05968945 0.52691703]\n",
      "Epoch 382/1000, Loss: 16.161796508562368, Weights: [0.4134552  0.05969343 0.52685137]\n",
      "Epoch 383/1000, Loss: 16.162462735885576, Weights: [0.4135167 0.0596974 0.5267859]\n",
      "Epoch 384/1000, Loss: 16.163127186418304, Weights: [0.41357804 0.05970136 0.5267206 ]\n",
      "Epoch 385/1000, Loss: 16.16378986892564, Weights: [0.41363922 0.0597053  0.52665548]\n",
      "Epoch 386/1000, Loss: 16.164450792109708, Weights: [0.41370022 0.05970924 0.52659054]\n",
      "Epoch 387/1000, Loss: 16.165109964610174, Weights: [0.41376107 0.05971317 0.52652577]\n",
      "Epoch 388/1000, Loss: 16.165767395004696, Weights: [0.41382174 0.05971708 0.52646118]\n",
      "Epoch 389/1000, Loss: 16.166423091809403, Weights: [0.41388226 0.05972098 0.52639676]\n",
      "Epoch 390/1000, Loss: 16.167077063479404, Weights: [0.41394261 0.05972488 0.52633251]\n",
      "Epoch 391/1000, Loss: 16.167729318409226, Weights: [0.4140028  0.05972876 0.52626844]\n",
      "Epoch 392/1000, Loss: 16.168379864933343, Weights: [0.41406283 0.05973263 0.52620454]\n",
      "Epoch 393/1000, Loss: 16.16902871132667, Weights: [0.4141227  0.05973649 0.52614081]\n",
      "Epoch 394/1000, Loss: 16.169675865805033, Weights: [0.41418241 0.05974035 0.52607725]\n",
      "Epoch 395/1000, Loss: 16.170321336525735, Weights: [0.41424196 0.05974419 0.52601386]\n",
      "Epoch 396/1000, Loss: 16.170965131588037, Weights: [0.41430135 0.05974802 0.52595063]\n",
      "Epoch 397/1000, Loss: 16.171607259033724, Weights: [0.41436059 0.05975184 0.52588757]\n",
      "Epoch 398/1000, Loss: 16.172247726847615, Weights: [0.41441966 0.05975565 0.52582468]\n",
      "Epoch 399/1000, Loss: 16.17288654295812, Weights: [0.41447859 0.05975945 0.52576196]\n",
      "Epoch 400/1000, Loss: 16.173523715237774, Weights: [0.41453736 0.05976324 0.5256994 ]\n",
      "Epoch 401/1000, Loss: 16.174159251503795, Weights: [0.41459597 0.05976702 0.525637  ]\n",
      "Epoch 402/1000, Loss: 16.174793159518657, Weights: [0.41465443 0.05977079 0.52557477]\n",
      "Epoch 403/1000, Loss: 16.175425446990573, Weights: [0.41471274 0.05977456 0.5255127 ]\n",
      "Epoch 404/1000, Loss: 16.176056121574092, Weights: [0.4147709  0.05977831 0.52545079]\n",
      "Epoch 405/1000, Loss: 16.17668519087065, Weights: [0.41482891 0.05978205 0.52538905]\n",
      "Epoch 406/1000, Loss: 16.177312662429028, Weights: [0.41488676 0.05978578 0.52532746]\n",
      "Epoch 407/1000, Loss: 16.177938543745974, Weights: [0.41494447 0.0597895  0.52526603]\n",
      "Epoch 408/1000, Loss: 16.17856284226665, Weights: [0.41500202 0.05979321 0.52520476]\n",
      "Epoch 409/1000, Loss: 16.179185565385186, Weights: [0.41505943 0.05979692 0.52514365]\n",
      "Epoch 410/1000, Loss: 16.179806720445136, Weights: [0.41511669 0.05980061 0.5250827 ]\n",
      "Epoch 411/1000, Loss: 16.18042631474, Weights: [0.41517381 0.05980429 0.5250219 ]\n",
      "Epoch 412/1000, Loss: 16.181044355513695, Weights: [0.41523078 0.05980797 0.52496126]\n",
      "Epoch 413/1000, Loss: 16.181660849961013, Weights: [0.4152876  0.05981163 0.52490077]\n",
      "Epoch 414/1000, Loss: 16.18227580522809, Weights: [0.41534428 0.05981529 0.52484044]\n",
      "Epoch 415/1000, Loss: 16.18288922841283, Weights: [0.41540081 0.05981893 0.52478026]\n",
      "Epoch 416/1000, Loss: 16.18350112656538, Weights: [0.4154572  0.05982257 0.52472023]\n",
      "Epoch 417/1000, Loss: 16.18411150668851, Weights: [0.41551345 0.0598262  0.52466035]\n",
      "Epoch 418/1000, Loss: 16.184720375738074, Weights: [0.41556955 0.05982982 0.52460063]\n",
      "Epoch 419/1000, Loss: 16.1853277406234, Weights: [0.41562552 0.05983343 0.52454106]\n",
      "Epoch 420/1000, Loss: 16.185933608207705, Weights: [0.41568134 0.05983703 0.52448163]\n",
      "Epoch 421/1000, Loss: 16.18653798530846, Weights: [0.41573702 0.05984062 0.52442236]\n",
      "Epoch 422/1000, Loss: 16.187140878697836, Weights: [0.41579257 0.0598442  0.52436323]\n",
      "Epoch 423/1000, Loss: 16.187742295103032, Weights: [0.41584797 0.05984777 0.52430426]\n",
      "Epoch 424/1000, Loss: 16.188342241206698, Weights: [0.41590324 0.05985133 0.52424543]\n",
      "Epoch 425/1000, Loss: 16.188940723647303, Weights: [0.41595837 0.05985489 0.52418674]\n",
      "Epoch 426/1000, Loss: 16.189537749019493, Weights: [0.41601336 0.05985844 0.52412821]\n",
      "Epoch 427/1000, Loss: 16.190133323874473, Weights: [0.41606821 0.05986197 0.52406981]\n",
      "Epoch 428/1000, Loss: 16.190727454720374, Weights: [0.41612293 0.0598655  0.52401156]\n",
      "Epoch 429/1000, Loss: 16.191320148022623, Weights: [0.41617752 0.05986902 0.52395346]\n",
      "Epoch 430/1000, Loss: 16.1919114102043, Weights: [0.41623197 0.05987253 0.5238955 ]\n",
      "Epoch 431/1000, Loss: 16.19250124764649, Weights: [0.41628629 0.05987603 0.52383768]\n",
      "Epoch 432/1000, Loss: 16.19308966668866, Weights: [0.41634047 0.05987953 0.52378   ]\n",
      "Epoch 433/1000, Loss: 16.193676673629014, Weights: [0.41639452 0.05988301 0.52372247]\n",
      "Epoch 434/1000, Loss: 16.194262274724824, Weights: [0.41644844 0.05988649 0.52366507]\n",
      "Epoch 435/1000, Loss: 16.19484647619279, Weights: [0.41650223 0.05988995 0.52360782]\n",
      "Epoch 436/1000, Loss: 16.19542928420944, Weights: [0.41655588 0.05989341 0.5235507 ]\n",
      "Epoch 437/1000, Loss: 16.19601070491138, Weights: [0.41660941 0.05989686 0.52349373]\n",
      "Epoch 438/1000, Loss: 16.19659074439574, Weights: [0.41666281 0.05990031 0.52343689]\n",
      "Epoch 439/1000, Loss: 16.19716940872042, Weights: [0.41671607 0.05990374 0.52338019]\n",
      "Epoch 440/1000, Loss: 16.197746703904514, Weights: [0.41676921 0.05990717 0.52332362]\n",
      "Epoch 441/1000, Loss: 16.198322635928584, Weights: [0.41682222 0.05991058 0.52326719]\n",
      "Epoch 442/1000, Loss: 16.198897210735023, Weights: [0.41687511 0.05991399 0.5232109 ]\n",
      "Epoch 443/1000, Loss: 16.199470434228363, Weights: [0.41692786 0.05991739 0.52315474]\n",
      "Epoch 444/1000, Loss: 16.200042312275617, Weights: [0.4169805  0.05992078 0.52309872]\n",
      "Epoch 445/1000, Loss: 16.20061285070659, Weights: [0.417033   0.05992417 0.52304283]\n",
      "Epoch 446/1000, Loss: 16.20118205531418, Weights: [0.41708538 0.05992754 0.52298708]\n",
      "Epoch 447/1000, Loss: 16.20174993185472, Weights: [0.41713763 0.05993091 0.52293145]\n",
      "Epoch 448/1000, Loss: 16.202316486048282, Weights: [0.41718977 0.05993427 0.52287596]\n",
      "Epoch 449/1000, Loss: 16.20288172357895, Weights: [0.41724177 0.05993762 0.5228206 ]\n",
      "Epoch 450/1000, Loss: 16.20344565009516, Weights: [0.41729366 0.05994097 0.52276538]\n",
      "Epoch 451/1000, Loss: 16.204008271209954, Weights: [0.41734542 0.0599443  0.52271028]\n",
      "Epoch 452/1000, Loss: 16.204569592501336, Weights: [0.41739706 0.05994763 0.52265531]\n",
      "Epoch 453/1000, Loss: 16.205129619512494, Weights: [0.41744858 0.05995095 0.52260047]\n",
      "Epoch 454/1000, Loss: 16.20568835775214, Weights: [0.41749998 0.05995426 0.52254576]\n",
      "Epoch 455/1000, Loss: 16.206245812694746, Weights: [0.41755125 0.05995757 0.52249118]\n",
      "Epoch 456/1000, Loss: 16.206801989780864, Weights: [0.41760241 0.05996086 0.52243673]\n",
      "Epoch 457/1000, Loss: 16.207356894417394, Weights: [0.41765345 0.05996415 0.5223824 ]\n",
      "Epoch 458/1000, Loss: 16.207910531977813, Weights: [0.41770437 0.05996743 0.5223282 ]\n",
      "Epoch 459/1000, Loss: 16.208462907802527, Weights: [0.41775517 0.0599707  0.52227413]\n",
      "Epoch 460/1000, Loss: 16.209014027199068, Weights: [0.41780585 0.05997397 0.52222018]\n",
      "Epoch 461/1000, Loss: 16.209563895442383, Weights: [0.41785642 0.05997723 0.52216635]\n",
      "Epoch 462/1000, Loss: 16.210112517775105, Weights: [0.41790687 0.05998048 0.52211265]\n",
      "Epoch 463/1000, Loss: 16.210659899407812, Weights: [0.4179572  0.05998372 0.52205908]\n",
      "Epoch 464/1000, Loss: 16.211206045519262, Weights: [0.41800742 0.05998696 0.52200563]\n",
      "Epoch 465/1000, Loss: 16.211750961256683, Weights: [0.41805752 0.05999018 0.5219523 ]\n",
      "Epoch 466/1000, Loss: 16.212294651736, Weights: [0.41810751 0.0599934  0.52189909]\n",
      "Epoch 467/1000, Loss: 16.21283712204208, Weights: [0.41815738 0.05999662 0.521846  ]\n",
      "Epoch 468/1000, Loss: 16.21337837722901, Weights: [0.41820714 0.05999982 0.52179304]\n",
      "Epoch 469/1000, Loss: 16.21391842232031, Weights: [0.41825679 0.06000302 0.5217402 ]\n",
      "Epoch 470/1000, Loss: 16.214457262309203, Weights: [0.41830632 0.06000621 0.52168747]\n",
      "Epoch 471/1000, Loss: 16.214994902158843, Weights: [0.41835574 0.06000939 0.52163487]\n",
      "Epoch 472/1000, Loss: 16.215531346802557, Weights: [0.41840505 0.06001257 0.52158239]\n",
      "Epoch 473/1000, Loss: 16.21606660114407, Weights: [0.41845424 0.06001574 0.52153002]\n",
      "Epoch 474/1000, Loss: 16.21660067005776, Weights: [0.41850333 0.0600189  0.52147777]\n",
      "Epoch 475/1000, Loss: 16.217133558388877, Weights: [0.4185523  0.06002205 0.52142565]\n",
      "Epoch 476/1000, Loss: 16.217665270953773, Weights: [0.41860117 0.0600252  0.52137363]\n",
      "Epoch 477/1000, Loss: 16.21819581254016, Weights: [0.41864993 0.06002834 0.52132174]\n",
      "Epoch 478/1000, Loss: 16.218725187907257, Weights: [0.41869857 0.06003147 0.52126996]\n",
      "Epoch 479/1000, Loss: 16.219253401786112, Weights: [0.41874711 0.06003459 0.5212183 ]\n",
      "Epoch 480/1000, Loss: 16.219780458879754, Weights: [0.41879554 0.06003771 0.52116675]\n",
      "Epoch 481/1000, Loss: 16.22030636386344, Weights: [0.41884386 0.06004082 0.52111532]\n",
      "Epoch 482/1000, Loss: 16.22083112138487, Weights: [0.41889207 0.06004393 0.521064  ]\n",
      "Epoch 483/1000, Loss: 16.22135473606439, Weights: [0.41894018 0.06004703 0.52101279]\n",
      "Epoch 484/1000, Loss: 16.22187721249521, Weights: [0.41898818 0.06005012 0.5209617 ]\n",
      "Epoch 485/1000, Loss: 16.222398555243633, Weights: [0.41903608 0.0600532  0.52091072]\n",
      "Epoch 486/1000, Loss: 16.222918768849215, Weights: [0.41908386 0.06005628 0.52085986]\n",
      "Epoch 487/1000, Loss: 16.223437857825036, Weights: [0.41913155 0.06005935 0.52080911]\n",
      "Epoch 488/1000, Loss: 16.223955826657853, Weights: [0.41917913 0.06006241 0.52075847]\n",
      "Epoch 489/1000, Loss: 16.224472679808315, Weights: [0.4192266  0.06006546 0.52070794]\n",
      "Epoch 490/1000, Loss: 16.22498842171117, Weights: [0.41927397 0.06006851 0.52065752]\n",
      "Epoch 491/1000, Loss: 16.225503056775477, Weights: [0.41932124 0.06007156 0.52060721]\n",
      "Epoch 492/1000, Loss: 16.226016589384752, Weights: [0.4193684  0.06007459 0.52055701]\n",
      "Epoch 493/1000, Loss: 16.226529023897225, Weights: [0.41941546 0.06007762 0.52050692]\n",
      "Epoch 494/1000, Loss: 16.227040364645983, Weights: [0.41946242 0.06008064 0.52045694]\n",
      "Epoch 495/1000, Loss: 16.22755061593919, Weights: [0.41950928 0.06008366 0.52040706]\n",
      "Epoch 496/1000, Loss: 16.228059782060264, Weights: [0.41955603 0.06008667 0.5203573 ]\n",
      "Epoch 497/1000, Loss: 16.22856786726806, Weights: [0.41960269 0.06008967 0.52030764]\n",
      "Epoch 498/1000, Loss: 16.22907487579706, Weights: [0.41964924 0.06009266 0.52025809]\n",
      "Epoch 499/1000, Loss: 16.229580811857566, Weights: [0.41969569 0.06009565 0.52020865]\n",
      "Epoch 500/1000, Loss: 16.230085679635867, Weights: [0.41974205 0.06009864 0.52015932]\n",
      "Epoch 501/1000, Loss: 16.230589483294406, Weights: [0.4197883  0.06010161 0.52011009]\n",
      "Epoch 502/1000, Loss: 16.23109222697202, Weights: [0.41983446 0.06010458 0.52006096]\n",
      "Epoch 503/1000, Loss: 16.231593914784032, Weights: [0.41988051 0.06010755 0.52001194]\n",
      "Epoch 504/1000, Loss: 16.23209455082248, Weights: [0.41992647 0.0601105  0.51996303]\n",
      "Epoch 505/1000, Loss: 16.232594139156287, Weights: [0.41997233 0.06011345 0.51991422]\n",
      "Epoch 506/1000, Loss: 16.23309268383142, Weights: [0.42001809 0.0601164  0.51986551]\n",
      "Epoch 507/1000, Loss: 16.233590188871055, Weights: [0.42006376 0.06011933 0.51981691]\n",
      "Epoch 508/1000, Loss: 16.234086658275764, Weights: [0.42010932 0.06012227 0.51976841]\n",
      "Epoch 509/1000, Loss: 16.23458209602367, Weights: [0.4201548  0.06012519 0.51972001]\n",
      "Epoch 510/1000, Loss: 16.235076506070623, Weights: [0.42020017 0.06012811 0.51967172]\n",
      "Epoch 511/1000, Loss: 16.23556989235034, Weights: [0.42024545 0.06013102 0.51962353]\n",
      "Epoch 512/1000, Loss: 16.2360622587746, Weights: [0.42029064 0.06013393 0.51957544]\n",
      "Epoch 513/1000, Loss: 16.236553609233383, Weights: [0.42033573 0.06013683 0.51952745]\n",
      "Epoch 514/1000, Loss: 16.237043947595044, Weights: [0.42038072 0.06013972 0.51947956]\n",
      "Epoch 515/1000, Loss: 16.23753327770645, Weights: [0.42042562 0.06014261 0.51943177]\n",
      "Epoch 516/1000, Loss: 16.238021603393165, Weights: [0.42047043 0.06014549 0.51938408]\n",
      "Epoch 517/1000, Loss: 16.23850892845958, Weights: [0.42051514 0.06014837 0.51933649]\n",
      "Epoch 518/1000, Loss: 16.23899525668908, Weights: [0.42055977 0.06015123 0.519289  ]\n",
      "Epoch 519/1000, Loss: 16.2394805918442, Weights: [0.42060429 0.0601541  0.51924161]\n",
      "Epoch 520/1000, Loss: 16.239964937666773, Weights: [0.42064873 0.06015696 0.51919432]\n",
      "Epoch 521/1000, Loss: 16.24044829787806, Weights: [0.42069307 0.06015981 0.51914712]\n",
      "Epoch 522/1000, Loss: 16.24093067617893, Weights: [0.42073732 0.06016265 0.51910002]\n",
      "Epoch 523/1000, Loss: 16.24141207624999, Weights: [0.42078148 0.06016549 0.51905303]\n",
      "Epoch 524/1000, Loss: 16.241892501751735, Weights: [0.42082555 0.06016832 0.51900612]\n",
      "Epoch 525/1000, Loss: 16.24237195632468, Weights: [0.42086953 0.06017115 0.51895932]\n",
      "Epoch 526/1000, Loss: 16.242850443589525, Weights: [0.42091342 0.06017397 0.51891261]\n",
      "Epoch 527/1000, Loss: 16.243327967147284, Weights: [0.42095722 0.06017679 0.51886599]\n",
      "Epoch 528/1000, Loss: 16.243804530579418, Weights: [0.42100093 0.0601796  0.51881947]\n",
      "Epoch 529/1000, Loss: 16.244280137447998, Weights: [0.42104455 0.0601824  0.51877305]\n",
      "Epoch 530/1000, Loss: 16.244754791295822, Weights: [0.42108808 0.0601852  0.51872672]\n",
      "Epoch 531/1000, Loss: 16.245228495646547, Weights: [0.42113152 0.06018799 0.51868049]\n",
      "Epoch 532/1000, Loss: 16.24570125400485, Weights: [0.42117487 0.06019078 0.51863435]\n",
      "Epoch 533/1000, Loss: 16.246173069856543, Weights: [0.42121814 0.06019356 0.5185883 ]\n",
      "Epoch 534/1000, Loss: 16.24664394666872, Weights: [0.42126132 0.06019633 0.51854235]\n",
      "Epoch 535/1000, Loss: 16.24711388788987, Weights: [0.42130441 0.0601991  0.51849649]\n",
      "Epoch 536/1000, Loss: 16.24758289695001, Weights: [0.42134741 0.06020186 0.51845072]\n",
      "Epoch 537/1000, Loss: 16.248050977260853, Weights: [0.42139033 0.06020462 0.51840505]\n",
      "Epoch 538/1000, Loss: 16.248518132215878, Weights: [0.42143316 0.06020737 0.51835946]\n",
      "Epoch 539/1000, Loss: 16.248984365190495, Weights: [0.42147591 0.06021012 0.51831397]\n",
      "Epoch 540/1000, Loss: 16.249449679542174, Weights: [0.42151857 0.06021286 0.51826857]\n",
      "Epoch 541/1000, Loss: 16.249914078610555, Weights: [0.42156114 0.0602156  0.51822326]\n",
      "Epoch 542/1000, Loss: 16.25037756571756, Weights: [0.42160363 0.06021833 0.51817805]\n",
      "Epoch 543/1000, Loss: 16.250840144167547, Weights: [0.42164603 0.06022105 0.51813292]\n",
      "Epoch 544/1000, Loss: 16.251301817247416, Weights: [0.42168835 0.06022377 0.51808788]\n",
      "Epoch 545/1000, Loss: 16.251762588226743, Weights: [0.42173059 0.06022648 0.51804293]\n",
      "Epoch 546/1000, Loss: 16.252222460357856, Weights: [0.42177274 0.06022919 0.51799807]\n",
      "Epoch 547/1000, Loss: 16.25268143687603, Weights: [0.42181481 0.06023189 0.5179533 ]\n",
      "Epoch 548/1000, Loss: 16.25313952099952, Weights: [0.42185679 0.06023459 0.51790862]\n",
      "Epoch 549/1000, Loss: 16.253596715929756, Weights: [0.42189869 0.06023728 0.51786403]\n",
      "Epoch 550/1000, Loss: 16.254053024851387, Weights: [0.42194051 0.06023997 0.51781952]\n",
      "Epoch 551/1000, Loss: 16.254508450932462, Weights: [0.42198225 0.06024265 0.51777511]\n",
      "Epoch 552/1000, Loss: 16.2549629973245, Weights: [0.4220239  0.06024532 0.51773078]\n",
      "Epoch 553/1000, Loss: 16.25541666716262, Weights: [0.42206548 0.06024799 0.51768653]\n",
      "Epoch 554/1000, Loss: 16.25586946356564, Weights: [0.42210697 0.06025065 0.51764238]\n",
      "Epoch 555/1000, Loss: 16.256321389636213, Weights: [0.42214838 0.06025331 0.51759831]\n",
      "Epoch 556/1000, Loss: 16.2567724484609, Weights: [0.42218971 0.06025597 0.51755432]\n",
      "Epoch 557/1000, Loss: 16.257222643110328, Weights: [0.42223096 0.06025862 0.51751043]\n",
      "Epoch 558/1000, Loss: 16.257671976639244, Weights: [0.42227213 0.06026126 0.51746661]\n",
      "Epoch 559/1000, Loss: 16.25812045208667, Weights: [0.42231322 0.0602639  0.51742289]\n",
      "Epoch 560/1000, Loss: 16.258568072475978, Weights: [0.42235423 0.06026653 0.51737925]\n",
      "Epoch 561/1000, Loss: 16.259014840815006, Weights: [0.42239516 0.06026916 0.51733569]\n",
      "Epoch 562/1000, Loss: 16.259460760096164, Weights: [0.42243601 0.06027178 0.51729221]\n",
      "Epoch 563/1000, Loss: 16.25990583329655, Weights: [0.42247678 0.0602744  0.51724883]\n",
      "Epoch 564/1000, Loss: 16.260350063378016, Weights: [0.42251747 0.06027701 0.51720552]\n",
      "Epoch 565/1000, Loss: 16.260793453287313, Weights: [0.42255809 0.06027961 0.5171623 ]\n",
      "Epoch 566/1000, Loss: 16.261236005956157, Weights: [0.42259862 0.06028222 0.51711916]\n",
      "Epoch 567/1000, Loss: 16.261677724301357, Weights: [0.42263908 0.06028481 0.5170761 ]\n",
      "Epoch 568/1000, Loss: 16.262118611224906, Weights: [0.42267947 0.0602874  0.51703313]\n",
      "Epoch 569/1000, Loss: 16.262558669614062, Weights: [0.42271977 0.06028999 0.51699024]\n",
      "Epoch 570/1000, Loss: 16.262997902341475, Weights: [0.42276    0.06029257 0.51694743]\n",
      "Epoch 571/1000, Loss: 16.26343631226525, Weights: [0.42280015 0.06029515 0.5169047 ]\n",
      "Epoch 572/1000, Loss: 16.263873902229093, Weights: [0.42284022 0.06029772 0.51686206]\n",
      "Epoch 573/1000, Loss: 16.264310675062333, Weights: [0.42288022 0.06030029 0.51681949]\n",
      "Epoch 574/1000, Loss: 16.264746633580117, Weights: [0.42292015 0.06030285 0.51677701]\n",
      "Epoch 575/1000, Loss: 16.265181780583394, Weights: [0.42295999 0.0603054  0.5167346 ]\n",
      "Epoch 576/1000, Loss: 16.265616118859086, Weights: [0.42299977 0.06030796 0.51669228]\n",
      "Epoch 577/1000, Loss: 16.266049651180154, Weights: [0.42303946 0.0603105  0.51665004]\n",
      "Epoch 578/1000, Loss: 16.266482380305682, Weights: [0.42307908 0.06031304 0.51660787]\n",
      "Epoch 579/1000, Loss: 16.266914308981, Weights: [0.42311863 0.06031558 0.51656579]\n",
      "Epoch 580/1000, Loss: 16.26734543993772, Weights: [0.42315811 0.06031811 0.51652378]\n",
      "Epoch 581/1000, Loss: 16.26777577589387, Weights: [0.4231975  0.06032064 0.51648186]\n",
      "Epoch 582/1000, Loss: 16.26820531955397, Weights: [0.42323683 0.06032316 0.51644001]\n",
      "Epoch 583/1000, Loss: 16.26863407360912, Weights: [0.42327608 0.06032568 0.51639824]\n",
      "Epoch 584/1000, Loss: 16.2690620407371, Weights: [0.42331526 0.06032819 0.51635655]\n",
      "Epoch 585/1000, Loss: 16.269489223602417, Weights: [0.42335437 0.0603307  0.51631493]\n",
      "Epoch 586/1000, Loss: 16.26991562485642, Weights: [0.4233934 0.0603332 0.5162734]\n",
      "Epoch 587/1000, Loss: 16.2703412471374, Weights: [0.42343236 0.0603357  0.51623194]\n",
      "Epoch 588/1000, Loss: 16.27076609307064, Weights: [0.42347125 0.06033819 0.51619056]\n",
      "Epoch 589/1000, Loss: 16.271190165268536, Weights: [0.42351006 0.06034068 0.51614925]\n",
      "Epoch 590/1000, Loss: 16.271613466330628, Weights: [0.42354881 0.06034317 0.51610802]\n",
      "Epoch 591/1000, Loss: 16.272035998843734, Weights: [0.42358748 0.06034565 0.51606687]\n",
      "Epoch 592/1000, Loss: 16.272457765382022, Weights: [0.42362608 0.06034812 0.5160258 ]\n",
      "Epoch 593/1000, Loss: 16.272878768507056, Weights: [0.42366461 0.06035059 0.51598479]\n",
      "Epoch 594/1000, Loss: 16.273299010767914, Weights: [0.42370307 0.06035306 0.51594387]\n",
      "Epoch 595/1000, Loss: 16.273718494701264, Weights: [0.42374146 0.06035552 0.51590302]\n",
      "Epoch 596/1000, Loss: 16.274137222831406, Weights: [0.42377978 0.06035797 0.51586224]\n",
      "Epoch 597/1000, Loss: 16.27455519767041, Weights: [0.42381803 0.06036042 0.51582154]\n",
      "Epoch 598/1000, Loss: 16.274972421718147, Weights: [0.42385621 0.06036287 0.51578092]\n",
      "Epoch 599/1000, Loss: 16.275388897462395, Weights: [0.42389432 0.06036531 0.51574037]\n",
      "Epoch 600/1000, Loss: 16.275804627378882, Weights: [0.42393236 0.06036775 0.51569989]\n",
      "Epoch 601/1000, Loss: 16.276219613931403, Weights: [0.42397033 0.06037018 0.51565948]\n",
      "Epoch 602/1000, Loss: 16.276633859571863, Weights: [0.42400823 0.06037261 0.51561915]\n",
      "Epoch 603/1000, Loss: 16.277047366740376, Weights: [0.42404607 0.06037504 0.5155789 ]\n",
      "Epoch 604/1000, Loss: 16.277460137865326, Weights: [0.42408383 0.06037746 0.51553871]\n",
      "Epoch 605/1000, Loss: 16.277872175363427, Weights: [0.42412153 0.06037987 0.5154986 ]\n",
      "Epoch 606/1000, Loss: 16.278283481639832, Weights: [0.42415916 0.06038228 0.51545856]\n",
      "Epoch 607/1000, Loss: 16.27869405908818, Weights: [0.42419672 0.06038469 0.51541859]\n",
      "Epoch 608/1000, Loss: 16.27910391009068, Weights: [0.42423421 0.06038709 0.5153787 ]\n",
      "Epoch 609/1000, Loss: 16.279513037018155, Weights: [0.42427164 0.06038949 0.51533887]\n",
      "Epoch 610/1000, Loss: 16.279921442230165, Weights: [0.424309   0.06039188 0.51529912]\n",
      "Epoch 611/1000, Loss: 16.280329128075017, Weights: [0.42434629 0.06039427 0.51525944]\n",
      "Epoch 612/1000, Loss: 16.28073609688989, Weights: [0.42438351 0.06039665 0.51521983]\n",
      "Epoch 613/1000, Loss: 16.281142351000863, Weights: [0.42442067 0.06039903 0.51518029]\n",
      "Epoch 614/1000, Loss: 16.281547892723005, Weights: [0.42445777 0.06040141 0.51514082]\n",
      "Epoch 615/1000, Loss: 16.281952724360437, Weights: [0.42449479 0.06040378 0.51510143]\n",
      "Epoch 616/1000, Loss: 16.282356848206412, Weights: [0.42453175 0.06040615 0.5150621 ]\n",
      "Epoch 617/1000, Loss: 16.28276026654336, Weights: [0.42456865 0.06040851 0.51502284]\n",
      "Epoch 618/1000, Loss: 16.283162981642953, Weights: [0.42460548 0.06041087 0.51498365]\n",
      "Epoch 619/1000, Loss: 16.283564995766213, Weights: [0.42464224 0.06041322 0.51494453]\n",
      "Epoch 620/1000, Loss: 16.283966311163528, Weights: [0.42467894 0.06041557 0.51490548]\n",
      "Epoch 621/1000, Loss: 16.284366930074754, Weights: [0.42471558 0.06041792 0.5148665 ]\n",
      "Epoch 622/1000, Loss: 16.284766854729256, Weights: [0.42475215 0.06042026 0.51482759]\n",
      "Epoch 623/1000, Loss: 16.285166087345964, Weights: [0.42478866 0.0604226  0.51478875]\n",
      "Epoch 624/1000, Loss: 16.285564630133482, Weights: [0.4248251  0.06042493 0.51474997]\n",
      "Epoch 625/1000, Loss: 16.2859624852901, Weights: [0.42486148 0.06042726 0.51471126]\n",
      "Epoch 626/1000, Loss: 16.286359655003885, Weights: [0.42489779 0.06042958 0.51467262]\n",
      "Epoch 627/1000, Loss: 16.286756141452745, Weights: [0.42493404 0.0604319  0.51463405]\n",
      "Epoch 628/1000, Loss: 16.287151946804478, Weights: [0.42497023 0.06043422 0.51459555]\n",
      "Epoch 629/1000, Loss: 16.287547073216825, Weights: [0.42500636 0.06043653 0.51455711]\n",
      "Epoch 630/1000, Loss: 16.287941522837563, Weights: [0.42504242 0.06043884 0.51451874]\n",
      "Epoch 631/1000, Loss: 16.28833529780454, Weights: [0.42507842 0.06044114 0.51448044]\n",
      "Epoch 632/1000, Loss: 16.288728400245734, Weights: [0.42511435 0.06044344 0.5144422 ]\n",
      "Epoch 633/1000, Loss: 16.289120832279334, Weights: [0.42515023 0.06044574 0.51440403]\n",
      "Epoch 634/1000, Loss: 16.28951259601378, Weights: [0.42518604 0.06044803 0.51436593]\n",
      "Epoch 635/1000, Loss: 16.28990369354781, Weights: [0.42522179 0.06045032 0.51432789]\n",
      "Epoch 636/1000, Loss: 16.29029412697056, Weights: [0.42525748 0.0604526  0.51428992]\n",
      "Epoch 637/1000, Loss: 16.290683898361582, Weights: [0.42529311 0.06045488 0.51425201]\n",
      "Epoch 638/1000, Loss: 16.291073009790917, Weights: [0.42532867 0.06045716 0.51421417]\n",
      "Epoch 639/1000, Loss: 16.291461463319166, Weights: [0.42536418 0.06045943 0.51417639]\n",
      "Epoch 640/1000, Loss: 16.29184926099751, Weights: [0.42539962 0.06046169 0.51413868]\n",
      "Epoch 641/1000, Loss: 16.292236404867793, Weights: [0.42543501 0.06046396 0.51410104]\n",
      "Epoch 642/1000, Loss: 16.292622896962587, Weights: [0.42547033 0.06046622 0.51406345]\n",
      "Epoch 643/1000, Loss: 16.293008739305215, Weights: [0.42550559 0.06046847 0.51402593]\n",
      "Epoch 644/1000, Loss: 16.293393933909833, Weights: [0.4255408  0.06047072 0.51398848]\n",
      "Epoch 645/1000, Loss: 16.29377848278147, Weights: [0.42557594 0.06047297 0.51395109]\n",
      "Epoch 646/1000, Loss: 16.294162387916085, Weights: [0.42561102 0.06047521 0.51391376]\n",
      "Epoch 647/1000, Loss: 16.294545651300627, Weights: [0.42564604 0.06047745 0.5138765 ]\n",
      "Epoch 648/1000, Loss: 16.29492827491308, Weights: [0.42568101 0.06047969 0.5138393 ]\n",
      "Epoch 649/1000, Loss: 16.295310260722534, Weights: [0.42571591 0.06048192 0.51380217]\n",
      "Epoch 650/1000, Loss: 16.295691610689197, Weights: [0.42575076 0.06048415 0.51376509]\n",
      "Epoch 651/1000, Loss: 16.296072326764495, Weights: [0.42578555 0.06048637 0.51372808]\n",
      "Epoch 652/1000, Loss: 16.29645241089109, Weights: [0.42582027 0.06048859 0.51369113]\n",
      "Epoch 653/1000, Loss: 16.296831865002943, Weights: [0.42585494 0.06049081 0.51365425]\n",
      "Epoch 654/1000, Loss: 16.297210691025384, Weights: [0.42588955 0.06049302 0.51361742]\n",
      "Epoch 655/1000, Loss: 16.297588890875115, Weights: [0.42592411 0.06049523 0.51358066]\n",
      "Epoch 656/1000, Loss: 16.297966466460313, Weights: [0.4259586  0.06049744 0.51354396]\n",
      "Epoch 657/1000, Loss: 16.298343419680627, Weights: [0.42599304 0.06049964 0.51350732]\n",
      "Epoch 658/1000, Loss: 16.298719752427296, Weights: [0.42602742 0.06050183 0.51347074]\n",
      "Epoch 659/1000, Loss: 16.299095466583122, Weights: [0.42606174 0.06050403 0.51343423]\n",
      "Epoch 660/1000, Loss: 16.29947056402258, Weights: [0.42609601 0.06050622 0.51339777]\n",
      "Epoch 661/1000, Loss: 16.29984504661183, Weights: [0.42613022 0.0605084  0.51336138]\n",
      "Epoch 662/1000, Loss: 16.30021891620877, Weights: [0.42616437 0.06051059 0.51332504]\n",
      "Epoch 663/1000, Loss: 16.300592174663112, Weights: [0.42619846 0.06051276 0.51328877]\n",
      "Epoch 664/1000, Loss: 16.300964823816386, Weights: [0.4262325  0.06051494 0.51325256]\n",
      "Epoch 665/1000, Loss: 16.30133686550203, Weights: [0.42626648 0.06051711 0.51321641]\n",
      "Epoch 666/1000, Loss: 16.30170830154539, Weights: [0.42630041 0.06051928 0.51318031]\n",
      "Epoch 667/1000, Loss: 16.302079133763808, Weights: [0.42633428 0.06052144 0.51314428]\n",
      "Epoch 668/1000, Loss: 16.302449363966666, Weights: [0.42636809 0.0605236  0.5131083 ]\n",
      "Epoch 669/1000, Loss: 16.302818993955384, Weights: [0.42640185 0.06052576 0.51307239]\n",
      "Epoch 670/1000, Loss: 16.303188025523525, Weights: [0.42643555 0.06052791 0.51303653]\n",
      "Epoch 671/1000, Loss: 16.303556460456804, Weights: [0.4264692  0.06053006 0.51300074]\n",
      "Epoch 672/1000, Loss: 16.30392430053315, Weights: [0.42650279 0.06053221 0.512965  ]\n",
      "Epoch 673/1000, Loss: 16.30429154752274, Weights: [0.42653633 0.06053435 0.51292932]\n",
      "Epoch 674/1000, Loss: 16.30465820318804, Weights: [0.42656981 0.06053649 0.5128937 ]\n",
      "Epoch 675/1000, Loss: 16.305024269283866, Weights: [0.42660324 0.06053862 0.51285814]\n",
      "Epoch 676/1000, Loss: 16.30538974755742, Weights: [0.42663661 0.06054075 0.51282263]\n",
      "Epoch 677/1000, Loss: 16.30575463974832, Weights: [0.42666993 0.06054288 0.51278719]\n",
      "Epoch 678/1000, Loss: 16.306118947588647, Weights: [0.4267032 0.060545  0.5127518]\n",
      "Epoch 679/1000, Loss: 16.306482672803018, Weights: [0.42673641 0.06054712 0.51271647]\n",
      "Epoch 680/1000, Loss: 16.306845817108588, Weights: [0.42676956 0.06054924 0.51268119]\n",
      "Epoch 681/1000, Loss: 16.307208382215098, Weights: [0.42680267 0.06055136 0.51264598]\n",
      "Epoch 682/1000, Loss: 16.307570369824955, Weights: [0.42683572 0.06055347 0.51261082]\n",
      "Epoch 683/1000, Loss: 16.307931781633226, Weights: [0.42686871 0.06055557 0.51257571]\n",
      "Epoch 684/1000, Loss: 16.3082926193277, Weights: [0.42690166 0.06055767 0.51254067]\n",
      "Epoch 685/1000, Loss: 16.30865288458893, Weights: [0.42693455 0.06055977 0.51250568]\n",
      "Epoch 686/1000, Loss: 16.30901257909027, Weights: [0.42696738 0.06056187 0.51247075]\n",
      "Epoch 687/1000, Loss: 16.309371704497917, Weights: [0.42700017 0.06056396 0.51243587]\n",
      "Epoch 688/1000, Loss: 16.309730262470953, Weights: [0.4270329  0.06056605 0.51240105]\n",
      "Epoch 689/1000, Loss: 16.310088254661377, Weights: [0.42706558 0.06056814 0.51236628]\n",
      "Epoch 690/1000, Loss: 16.310445682714157, Weights: [0.42709821 0.06057022 0.51233158]\n",
      "Epoch 691/1000, Loss: 16.310802548267255, Weights: [0.42713078 0.0605723  0.51229692]\n",
      "Epoch 692/1000, Loss: 16.311158852951664, Weights: [0.4271633  0.06057437 0.51226232]\n",
      "Epoch 693/1000, Loss: 16.31151459839148, Weights: [0.42719577 0.06057644 0.51222778]\n",
      "Epoch 694/1000, Loss: 16.3118697862039, Weights: [0.42722819 0.06057851 0.51219329]\n",
      "Epoch 695/1000, Loss: 16.312224417999268, Weights: [0.42726056 0.06058058 0.51215886]\n",
      "Epoch 696/1000, Loss: 16.312578495381132, Weights: [0.42729288 0.06058264 0.51212448]\n",
      "Epoch 697/1000, Loss: 16.31293201994628, Weights: [0.42732514 0.0605847  0.51209016]\n",
      "Epoch 698/1000, Loss: 16.313284993284743, Weights: [0.42735736 0.06058675 0.51205589]\n",
      "Epoch 699/1000, Loss: 16.31363741697987, Weights: [0.42738952 0.0605888  0.51202167]\n",
      "Epoch 700/1000, Loss: 16.313989292608355, Weights: [0.42742163 0.06059085 0.51198751]\n",
      "Epoch 701/1000, Loss: 16.314340621740264, Weights: [0.4274537  0.0605929  0.51195341]\n",
      "Epoch 702/1000, Loss: 16.314691405939083, Weights: [0.42748571 0.06059494 0.51191935]\n",
      "Epoch 703/1000, Loss: 16.315041646761745, Weights: [0.42751767 0.06059698 0.51188535]\n",
      "Epoch 704/1000, Loss: 16.315391345758663, Weights: [0.42754958 0.06059901 0.51185141]\n",
      "Epoch 705/1000, Loss: 16.31574050447378, Weights: [0.42758144 0.06060104 0.51181752]\n",
      "Epoch 706/1000, Loss: 16.316089124444606, Weights: [0.42761325 0.06060307 0.51178368]\n",
      "Epoch 707/1000, Loss: 16.31643720720222, Weights: [0.42764501 0.0606051  0.51174989]\n",
      "Epoch 708/1000, Loss: 16.316784754271346, Weights: [0.42767672 0.06060712 0.51171616]\n",
      "Epoch 709/1000, Loss: 16.317131767170363, Weights: [0.42770838 0.06060914 0.51168248]\n",
      "Epoch 710/1000, Loss: 16.31747824741135, Weights: [0.42774    0.06061115 0.51164885]\n",
      "Epoch 711/1000, Loss: 16.31782419650012, Weights: [0.42777156 0.06061317 0.51161527]\n",
      "Epoch 712/1000, Loss: 16.318169615936235, Weights: [0.42780308 0.06061518 0.51158175]\n",
      "Epoch 713/1000, Loss: 16.318514507213077, Weights: [0.42783454 0.06061718 0.51154828]\n",
      "Epoch 714/1000, Loss: 16.318858871817845, Weights: [0.42786596 0.06061918 0.51151486]\n",
      "Epoch 715/1000, Loss: 16.3192027112316, Weights: [0.42789733 0.06062118 0.51148149]\n",
      "Epoch 716/1000, Loss: 16.319546026929324, Weights: [0.42792865 0.06062318 0.51144818]\n",
      "Epoch 717/1000, Loss: 16.319888820379905, Weights: [0.42795992 0.06062517 0.51141491]\n",
      "Epoch 718/1000, Loss: 16.3202310930462, Weights: [0.42799114 0.06062716 0.5113817 ]\n",
      "Epoch 719/1000, Loss: 16.320572846385073, Weights: [0.42802232 0.06062915 0.51134854]\n",
      "Epoch 720/1000, Loss: 16.32091408184741, Weights: [0.42805344 0.06063113 0.51131543]\n",
      "Epoch 721/1000, Loss: 16.321254800878158, Weights: [0.42808452 0.06063311 0.51128237]\n",
      "Epoch 722/1000, Loss: 16.321595004916357, Weights: [0.42811555 0.06063509 0.51124936]\n",
      "Epoch 723/1000, Loss: 16.321934695395154, Weights: [0.42814654 0.06063706 0.5112164 ]\n",
      "Epoch 724/1000, Loss: 16.32227387374188, Weights: [0.42817747 0.06063903 0.51118349]\n",
      "Epoch 725/1000, Loss: 16.322612541378042, Weights: [0.42820836 0.060641   0.51115063]\n",
      "Epoch 726/1000, Loss: 16.322950699719346, Weights: [0.42823921 0.06064297 0.51111783]\n",
      "Epoch 727/1000, Loss: 16.323288350175766, Weights: [0.42827    0.06064493 0.51108507]\n",
      "Epoch 728/1000, Loss: 16.32362549415155, Weights: [0.42830075 0.06064689 0.51105236]\n",
      "Epoch 729/1000, Loss: 16.323962133045253, Weights: [0.42833145 0.06064884 0.51101971]\n",
      "Epoch 730/1000, Loss: 16.32429826824976, Weights: [0.42836211 0.0606508  0.5109871 ]\n",
      "Epoch 731/1000, Loss: 16.324633901152346, Weights: [0.42839271 0.06065274 0.51095454]\n",
      "Epoch 732/1000, Loss: 16.32496903313466, Weights: [0.42842328 0.06065469 0.51092203]\n",
      "Epoch 733/1000, Loss: 16.3253036655728, Weights: [0.42845379 0.06065663 0.51088957]\n",
      "Epoch 734/1000, Loss: 16.325637799837306, Weights: [0.42848426 0.06065857 0.51085716]\n",
      "Epoch 735/1000, Loss: 16.32597143729321, Weights: [0.42851469 0.06066051 0.5108248 ]\n",
      "Epoch 736/1000, Loss: 16.32630457930006, Weights: [0.42854507 0.06066245 0.51079249]\n",
      "Epoch 737/1000, Loss: 16.326637227211954, Weights: [0.4285754  0.06066438 0.51076023]\n",
      "Epoch 738/1000, Loss: 16.326969382377555, Weights: [0.42860568 0.06066631 0.51072801]\n",
      "Epoch 739/1000, Loss: 16.327301046140125, Weights: [0.42863593 0.06066823 0.51069584]\n",
      "Epoch 740/1000, Loss: 16.32763221983756, Weights: [0.42866612 0.06067015 0.51066373]\n",
      "Epoch 741/1000, Loss: 16.327962904802426, Weights: [0.42869627 0.06067207 0.51063166]\n",
      "Epoch 742/1000, Loss: 16.328293102361943, Weights: [0.42872638 0.06067399 0.51059963]\n",
      "Epoch 743/1000, Loss: 16.328622813838074, Weights: [0.42875644 0.0606759  0.51056766]\n",
      "Epoch 744/1000, Loss: 16.32895204054751, Weights: [0.42878645 0.06067781 0.51053573]\n",
      "Epoch 745/1000, Loss: 16.32928078380171, Weights: [0.42881643 0.06067972 0.51050386]\n",
      "Epoch 746/1000, Loss: 16.32960904490693, Weights: [0.42884635 0.06068162 0.51047202]\n",
      "Epoch 747/1000, Loss: 16.329936825164253, Weights: [0.42887623 0.06068353 0.51044024]\n",
      "Epoch 748/1000, Loss: 16.330264125869594, Weights: [0.42890607 0.06068542 0.5104085 ]\n",
      "Epoch 749/1000, Loss: 16.330590948313766, Weights: [0.42893587 0.06068732 0.51037681]\n",
      "Epoch 750/1000, Loss: 16.330917293782466, Weights: [0.42896561 0.06068921 0.51034517]\n",
      "Epoch 751/1000, Loss: 16.33124316355633, Weights: [0.42899532 0.0606911  0.51031358]\n",
      "Epoch 752/1000, Loss: 16.33156855891095, Weights: [0.42902498 0.06069299 0.51028203]\n",
      "Epoch 753/1000, Loss: 16.331893481116882, Weights: [0.4290546  0.06069487 0.51025053]\n",
      "Epoch 754/1000, Loss: 16.332217931439704, Weights: [0.42908417 0.06069676 0.51021907]\n",
      "Epoch 755/1000, Loss: 16.332541911140023, Weights: [0.4291137  0.06069863 0.51018766]\n",
      "Epoch 756/1000, Loss: 16.3328654214735, Weights: [0.42914319 0.06070051 0.5101563 ]\n",
      "Epoch 757/1000, Loss: 16.333188463690885, Weights: [0.42917264 0.06070238 0.51012498]\n",
      "Epoch 758/1000, Loss: 16.333511039038015, Weights: [0.42920204 0.06070425 0.51009371]\n",
      "Epoch 759/1000, Loss: 16.333833148755893, Weights: [0.42923139 0.06070612 0.51006249]\n",
      "Epoch 760/1000, Loss: 16.334154794080657, Weights: [0.42926071 0.06070798 0.51003131]\n",
      "Epoch 761/1000, Loss: 16.334475976243628, Weights: [0.42928998 0.06070985 0.51000017]\n",
      "Epoch 762/1000, Loss: 16.334796696471347, Weights: [0.42931921 0.0607117  0.50996909]\n",
      "Epoch 763/1000, Loss: 16.335116955985562, Weights: [0.4293484  0.06071356 0.50993804]\n",
      "Epoch 764/1000, Loss: 16.335436756003308, Weights: [0.42937754 0.06071541 0.50990705]\n",
      "Epoch 765/1000, Loss: 16.335756097736873, Weights: [0.42940664 0.06071726 0.50987609]\n",
      "Epoch 766/1000, Loss: 16.33607498239386, Weights: [0.4294357  0.06071911 0.50984519]\n",
      "Epoch 767/1000, Loss: 16.336393411177202, Weights: [0.42946472 0.06072096 0.50981433]\n",
      "Epoch 768/1000, Loss: 16.336711385285174, Weights: [0.42949369 0.0607228  0.50978351]\n",
      "Epoch 769/1000, Loss: 16.337028905911428, Weights: [0.42952263 0.06072464 0.50975274]\n",
      "Epoch 770/1000, Loss: 16.337345974245018, Weights: [0.42955152 0.06072647 0.50972201]\n",
      "Epoch 771/1000, Loss: 16.337662591470416, Weights: [0.42958037 0.06072831 0.50969132]\n",
      "Epoch 772/1000, Loss: 16.337978758767537, Weights: [0.42960918 0.06073014 0.50966068]\n",
      "Epoch 773/1000, Loss: 16.338294477311756, Weights: [0.42963795 0.06073197 0.50963009]\n",
      "Epoch 774/1000, Loss: 16.33860974827395, Weights: [0.42966667 0.06073379 0.50959954]\n",
      "Epoch 775/1000, Loss: 16.338924572820503, Weights: [0.42969536 0.06073561 0.50956903]\n",
      "Epoch 776/1000, Loss: 16.33923895211333, Weights: [0.429724   0.06073743 0.50953857]\n",
      "Epoch 777/1000, Loss: 16.339552887309907, Weights: [0.4297526  0.06073925 0.50950815]\n",
      "Epoch 778/1000, Loss: 16.339866379563283, Weights: [0.42978116 0.06074107 0.50947777]\n",
      "Epoch 779/1000, Loss: 16.340179430022122, Weights: [0.42980968 0.06074288 0.50944744]\n",
      "Epoch 780/1000, Loss: 16.34049203983069, Weights: [0.42983816 0.06074469 0.50941715]\n",
      "Epoch 781/1000, Loss: 16.34080421012891, Weights: [0.4298666  0.06074649 0.5093869 ]\n",
      "Epoch 782/1000, Loss: 16.341115942052365, Weights: [0.429895  0.0607483 0.5093567]\n",
      "Epoch 783/1000, Loss: 16.34142723673235, Weights: [0.42992336 0.0607501  0.50932654]\n",
      "Epoch 784/1000, Loss: 16.34173809529583, Weights: [0.42995168 0.0607519  0.50929643]\n",
      "Epoch 785/1000, Loss: 16.342048518865536, Weights: [0.42997995 0.06075369 0.50926635]\n",
      "Epoch 786/1000, Loss: 16.34235850855992, Weights: [0.43000819 0.06075549 0.50923632]\n",
      "Epoch 787/1000, Loss: 16.342668065493225, Weights: [0.43003639 0.06075728 0.50920633]\n",
      "Epoch 788/1000, Loss: 16.342977190775493, Weights: [0.43006455 0.06075907 0.50917639]\n",
      "Epoch 789/1000, Loss: 16.343285885512564, Weights: [0.43009267 0.06076085 0.50914648]\n",
      "Epoch 790/1000, Loss: 16.343594150806123, Weights: [0.43012074 0.06076264 0.50911662]\n",
      "Epoch 791/1000, Loss: 16.343901987753707, Weights: [0.43014878 0.06076442 0.5090868 ]\n",
      "Epoch 792/1000, Loss: 16.34420939744872, Weights: [0.43017678 0.06076619 0.50905702]\n",
      "Epoch 793/1000, Loss: 16.344516380980483, Weights: [0.43020474 0.06076797 0.50902729]\n",
      "Epoch 794/1000, Loss: 16.34482293943421, Weights: [0.43023266 0.06076974 0.5089976 ]\n",
      "Epoch 795/1000, Loss: 16.345129073891076, Weights: [0.43026054 0.06077151 0.50896794]\n",
      "Epoch 796/1000, Loss: 16.345434785428182, Weights: [0.43028839 0.06077328 0.50893833]\n",
      "Epoch 797/1000, Loss: 16.345740075118627, Weights: [0.43031619 0.06077504 0.50890877]\n",
      "Epoch 798/1000, Loss: 16.346044944031497, Weights: [0.43034395 0.06077681 0.50887924]\n",
      "Epoch 799/1000, Loss: 16.346349393231893, Weights: [0.43037168 0.06077857 0.50884975]\n",
      "Epoch 800/1000, Loss: 16.346653423780957, Weights: [0.43039937 0.06078032 0.50882031]\n",
      "Epoch 801/1000, Loss: 16.34695703673587, Weights: [0.43042702 0.06078208 0.5087909 ]\n",
      "Epoch 802/1000, Loss: 16.3472602331499, Weights: [0.43045463 0.06078383 0.50876154]\n",
      "Epoch 803/1000, Loss: 16.3475630140724, Weights: [0.4304822  0.06078558 0.50873222]\n",
      "Epoch 804/1000, Loss: 16.34786538054883, Weights: [0.43050973 0.06078733 0.50870294]\n",
      "Epoch 805/1000, Loss: 16.348167333620786, Weights: [0.43053723 0.06078907 0.5086737 ]\n",
      "Epoch 806/1000, Loss: 16.348468874326013, Weights: [0.43056469 0.06079081 0.5086445 ]\n",
      "Epoch 807/1000, Loss: 16.34877000369842, Weights: [0.43059211 0.06079255 0.50861534]\n",
      "Epoch 808/1000, Loss: 16.349070722768094, Weights: [0.43061949 0.06079429 0.50858622]\n",
      "Epoch 809/1000, Loss: 16.34937103256134, Weights: [0.43064683 0.06079603 0.50855714]\n",
      "Epoch 810/1000, Loss: 16.349670934100672, Weights: [0.43067414 0.06079776 0.50852811]\n",
      "Epoch 811/1000, Loss: 16.349970428404855, Weights: [0.43070141 0.06079949 0.50849911]\n",
      "Epoch 812/1000, Loss: 16.3502695164889, Weights: [0.43072864 0.06080121 0.50847015]\n",
      "Epoch 813/1000, Loss: 16.350568199364098, Weights: [0.43075583 0.06080294 0.50844123]\n",
      "Epoch 814/1000, Loss: 16.350866478038046, Weights: [0.43078299 0.06080466 0.50841235]\n",
      "Epoch 815/1000, Loss: 16.35116435351464, Weights: [0.43081011 0.06080638 0.50838351]\n",
      "Epoch 816/1000, Loss: 16.351461826794097, Weights: [0.43083719 0.0608081  0.50835471]\n",
      "Epoch 817/1000, Loss: 16.35175889887301, Weights: [0.43086423 0.06080981 0.50832595]\n",
      "Epoch 818/1000, Loss: 16.352055570744294, Weights: [0.43089124 0.06081153 0.50829723]\n",
      "Epoch 819/1000, Loss: 16.35235184339729, Weights: [0.43091821 0.06081324 0.50826855]\n",
      "Epoch 820/1000, Loss: 16.35264771781771, Weights: [0.43094515 0.06081494 0.50823991]\n",
      "Epoch 821/1000, Loss: 16.352943194987688, Weights: [0.43097204 0.06081665 0.50821131]\n",
      "Epoch 822/1000, Loss: 16.353238275885793, Weights: [0.43099891 0.06081835 0.50818274]\n",
      "Epoch 823/1000, Loss: 16.353532961487034, Weights: [0.43102573 0.06082005 0.50815422]\n",
      "Epoch 824/1000, Loss: 16.35382725276291, Weights: [0.43105252 0.06082175 0.50812573]\n",
      "Epoch 825/1000, Loss: 16.354121150681372, Weights: [0.43107927 0.06082345 0.50809728]\n",
      "Epoch 826/1000, Loss: 16.354414656206902, Weights: [0.43110599 0.06082514 0.50806887]\n",
      "Epoch 827/1000, Loss: 16.354707770300468, Weights: [0.43113267 0.06082683 0.5080405 ]\n",
      "Epoch 828/1000, Loss: 16.3550004939196, Weights: [0.43115931 0.06082852 0.50801217]\n",
      "Epoch 829/1000, Loss: 16.355292828018356, Weights: [0.43118592 0.06083021 0.50798388]\n",
      "Epoch 830/1000, Loss: 16.355584773547367, Weights: [0.43121249 0.06083189 0.50795562]\n",
      "Epoch 831/1000, Loss: 16.355876331453846, Weights: [0.43123903 0.06083357 0.5079274 ]\n",
      "Epoch 832/1000, Loss: 16.35616750268161, Weights: [0.43126553 0.06083525 0.50789922]\n",
      "Epoch 833/1000, Loss: 16.356458288171066, Weights: [0.43129199 0.06083693 0.50787108]\n",
      "Epoch 834/1000, Loss: 16.35674868885928, Weights: [0.43131842 0.0608386  0.50784298]\n",
      "Epoch 835/1000, Loss: 16.357038705679948, Weights: [0.43134481 0.06084027 0.50781491]\n",
      "Epoch 836/1000, Loss: 16.35732833956343, Weights: [0.43137117 0.06084194 0.50778688]\n",
      "Epoch 837/1000, Loss: 16.35761759143676, Weights: [0.4313975  0.06084361 0.50775889]\n",
      "Epoch 838/1000, Loss: 16.35790646222366, Weights: [0.43142379 0.06084528 0.50773094]\n",
      "Epoch 839/1000, Loss: 16.358194952844574, Weights: [0.43145004 0.06084694 0.50770302]\n",
      "Epoch 840/1000, Loss: 16.358483064216653, Weights: [0.43147626 0.0608486  0.50767514]\n",
      "Epoch 841/1000, Loss: 16.358770797253797, Weights: [0.43150244 0.06085026 0.5076473 ]\n",
      "Epoch 842/1000, Loss: 16.35905815286665, Weights: [0.43152859 0.06085192 0.5076195 ]\n",
      "Epoch 843/1000, Loss: 16.359345131962623, Weights: [0.4315547  0.06085357 0.50759173]\n",
      "Epoch 844/1000, Loss: 16.359631735445937, Weights: [0.43158078 0.06085522 0.507564  ]\n",
      "Epoch 845/1000, Loss: 16.359917964217566, Weights: [0.43160683 0.06085687 0.5075363 ]\n",
      "Epoch 846/1000, Loss: 16.360203819175332, Weights: [0.43163284 0.06085852 0.50750865]\n",
      "Epoch 847/1000, Loss: 16.36048930121388, Weights: [0.43165881 0.06086016 0.50748103]\n",
      "Epoch 848/1000, Loss: 16.360774411224668, Weights: [0.43168475 0.0608618  0.50745344]\n",
      "Epoch 849/1000, Loss: 16.361059150096054, Weights: [0.43171066 0.06086344 0.50742589]\n",
      "Epoch 850/1000, Loss: 16.36134351871324, Weights: [0.43173653 0.06086508 0.50739838]\n",
      "Epoch 851/1000, Loss: 16.361627517958325, Weights: [0.43176237 0.06086672 0.50737091]\n",
      "Epoch 852/1000, Loss: 16.361911148710288, Weights: [0.43178818 0.06086835 0.50734347]\n",
      "Epoch 853/1000, Loss: 16.362194411845056, Weights: [0.43181395 0.06086998 0.50731607]\n",
      "Epoch 854/1000, Loss: 16.36247730823544, Weights: [0.43183969 0.06087161 0.5072887 ]\n",
      "Epoch 855/1000, Loss: 16.362759838751245, Weights: [0.43186539 0.06087324 0.50726137]\n",
      "Epoch 856/1000, Loss: 16.36304200425919, Weights: [0.43189106 0.06087486 0.50723408]\n",
      "Epoch 857/1000, Loss: 16.36332380562298, Weights: [0.4319167  0.06087648 0.50720682]\n",
      "Epoch 858/1000, Loss: 16.36360524370331, Weights: [0.4319423  0.06087811 0.50717959]\n",
      "Epoch 859/1000, Loss: 16.363886319357864, Weights: [0.43196787 0.06087972 0.50715241]\n",
      "Epoch 860/1000, Loss: 16.36416703344134, Weights: [0.43199341 0.06088134 0.50712525]\n",
      "Epoch 861/1000, Loss: 16.364447386805466, Weights: [0.43201891 0.06088295 0.50709814]\n",
      "Epoch 862/1000, Loss: 16.364727380298998, Weights: [0.43204438 0.06088456 0.50707106]\n",
      "Epoch 863/1000, Loss: 16.36500701476774, Weights: [0.43206982 0.06088617 0.50704401]\n",
      "Epoch 864/1000, Loss: 16.36528629105459, Weights: [0.43209522 0.06088778 0.507017  ]\n",
      "Epoch 865/1000, Loss: 16.3655652099995, Weights: [0.43212059 0.06088939 0.50699002]\n",
      "Epoch 866/1000, Loss: 16.365843772439515, Weights: [0.43214593 0.06089099 0.50696308]\n",
      "Epoch 867/1000, Loss: 16.366121979208792, Weights: [0.43217123 0.06089259 0.50693618]\n",
      "Epoch 868/1000, Loss: 16.366399831138605, Weights: [0.43219651 0.06089419 0.50690931]\n",
      "Epoch 869/1000, Loss: 16.366677329057353, Weights: [0.43222175 0.06089578 0.50688247]\n",
      "Epoch 870/1000, Loss: 16.366954473790596, Weights: [0.43224695 0.06089738 0.50685567]\n",
      "Epoch 871/1000, Loss: 16.36723126616102, Weights: [0.43227213 0.06089897 0.5068289 ]\n",
      "Epoch 872/1000, Loss: 16.367507706988523, Weights: [0.43229727 0.06090056 0.50680217]\n",
      "Epoch 873/1000, Loss: 16.367783797090144, Weights: [0.43232238 0.06090215 0.50677547]\n",
      "Epoch 874/1000, Loss: 16.36805953728014, Weights: [0.43234746 0.06090373 0.50674881]\n",
      "Epoch 875/1000, Loss: 16.368334928369972, Weights: [0.4323725  0.06090532 0.50672218]\n",
      "Epoch 876/1000, Loss: 16.36860997116832, Weights: [0.43239752 0.0609069  0.50669558]\n",
      "Epoch 877/1000, Loss: 16.368884666481094, Weights: [0.4324225  0.06090848 0.50666902]\n",
      "Epoch 878/1000, Loss: 16.369159015111453, Weights: [0.43244745 0.06091005 0.5066425 ]\n",
      "Epoch 879/1000, Loss: 16.36943301785981, Weights: [0.43247237 0.06091163 0.506616  ]\n",
      "Epoch 880/1000, Loss: 16.36970667552385, Weights: [0.43249725 0.0609132  0.50658954]\n",
      "Epoch 881/1000, Loss: 16.36997998889854, Weights: [0.43252211 0.06091477 0.50656312]\n",
      "Epoch 882/1000, Loss: 16.37025295877613, Weights: [0.43254693 0.06091634 0.50653673]\n",
      "Epoch 883/1000, Loss: 16.370525585946194, Weights: [0.43257172 0.06091791 0.50651037]\n",
      "Epoch 884/1000, Loss: 16.370797871195617, Weights: [0.43259648 0.06091947 0.50648405]\n",
      "Epoch 885/1000, Loss: 16.371069815308605, Weights: [0.43262121 0.06092104 0.50645775]\n",
      "Epoch 886/1000, Loss: 16.371341419066713, Weights: [0.43264591 0.0609226  0.5064315 ]\n",
      "Epoch 887/1000, Loss: 16.37161268324885, Weights: [0.43267057 0.06092416 0.50640527]\n",
      "Epoch 888/1000, Loss: 16.37188360863128, Weights: [0.43269521 0.06092571 0.50637908]\n",
      "Epoch 889/1000, Loss: 16.372154195987655, Weights: [0.43271981 0.06092727 0.50635292]\n",
      "Epoch 890/1000, Loss: 16.372424446089013, Weights: [0.43274438 0.06092882 0.5063268 ]\n",
      "Epoch 891/1000, Loss: 16.37269435970379, Weights: [0.43276892 0.06093037 0.50630071]\n",
      "Epoch 892/1000, Loss: 16.372963937597827, Weights: [0.43279343 0.06093192 0.50627465]\n",
      "Epoch 893/1000, Loss: 16.373233180534402, Weights: [0.43281791 0.06093346 0.50624862]\n",
      "Epoch 894/1000, Loss: 16.373502089274215, Weights: [0.43284236 0.06093501 0.50622263]\n",
      "Epoch 895/1000, Loss: 16.37377066457541, Weights: [0.43286678 0.06093655 0.50619667]\n",
      "Epoch 896/1000, Loss: 16.37403890719359, Weights: [0.43289117 0.06093809 0.50617074]\n",
      "Epoch 897/1000, Loss: 16.374306817881838, Weights: [0.43291552 0.06093963 0.50614485]\n",
      "Epoch 898/1000, Loss: 16.374574397390692, Weights: [0.43293985 0.06094117 0.50611899]\n",
      "Epoch 899/1000, Loss: 16.374841646468195, Weights: [0.43296414 0.0609427  0.50609316]\n",
      "Epoch 900/1000, Loss: 16.375108565859893, Weights: [0.43298841 0.06094423 0.50606736]\n",
      "Epoch 901/1000, Loss: 16.375375156308834, Weights: [0.43301265 0.06094576 0.50604159]\n",
      "Epoch 902/1000, Loss: 16.37564141855558, Weights: [0.43303685 0.06094729 0.50601586]\n",
      "Epoch 903/1000, Loss: 16.375907353338256, Weights: [0.43306103 0.06094882 0.50599016]\n",
      "Epoch 904/1000, Loss: 16.376172961392506, Weights: [0.43308517 0.06095034 0.50596449]\n",
      "Epoch 905/1000, Loss: 16.376438243451528, Weights: [0.43310928 0.06095186 0.50593885]\n",
      "Epoch 906/1000, Loss: 16.3767032002461, Weights: [0.43313337 0.06095338 0.50591325]\n",
      "Epoch 907/1000, Loss: 16.376967832504565, Weights: [0.43315742 0.0609549  0.50588767]\n",
      "Epoch 908/1000, Loss: 16.377232140952852, Weights: [0.43318145 0.06095642 0.50586213]\n",
      "Epoch 909/1000, Loss: 16.377496126314483, Weights: [0.43320544 0.06095793 0.50583662]\n",
      "Epoch 910/1000, Loss: 16.37775978931061, Weights: [0.43322941 0.06095945 0.50581114]\n",
      "Epoch 911/1000, Loss: 16.378023130659976, Weights: [0.43325335 0.06096096 0.5057857 ]\n",
      "Epoch 912/1000, Loss: 16.37828615107896, Weights: [0.43327725 0.06096246 0.50576028]\n",
      "Epoch 913/1000, Loss: 16.378548851281575, Weights: [0.43330113 0.06096397 0.5057349 ]\n",
      "Epoch 914/1000, Loss: 16.37881123197949, Weights: [0.43332498 0.06096548 0.50570955]\n",
      "Epoch 915/1000, Loss: 16.37907329388203, Weights: [0.4333488  0.06096698 0.50568423]\n",
      "Epoch 916/1000, Loss: 16.379335037696187, Weights: [0.43337258 0.06096848 0.50565894]\n",
      "Epoch 917/1000, Loss: 16.37959646412663, Weights: [0.43339634 0.06096998 0.50563368]\n",
      "Epoch 918/1000, Loss: 16.379857573875714, Weights: [0.43342008 0.06097148 0.50560845]\n",
      "Epoch 919/1000, Loss: 16.380118367643494, Weights: [0.43344378 0.06097297 0.50558325]\n",
      "Epoch 920/1000, Loss: 16.380378846127737, Weights: [0.43346745 0.06097446 0.50555809]\n",
      "Epoch 921/1000, Loss: 16.380639010023923, Weights: [0.43349109 0.06097596 0.50553295]\n",
      "Epoch 922/1000, Loss: 16.380898860025255, Weights: [0.43351471 0.06097745 0.50550785]\n",
      "Epoch 923/1000, Loss: 16.381158396822688, Weights: [0.43353829 0.06097893 0.50548277]\n",
      "Epoch 924/1000, Loss: 16.38141762110491, Weights: [0.43356185 0.06098042 0.50545773]\n",
      "Epoch 925/1000, Loss: 16.381676533558366, Weights: [0.43358538 0.0609819  0.50543272]\n",
      "Epoch 926/1000, Loss: 16.381935134867273, Weights: [0.43360888 0.06098338 0.50540774]\n",
      "Epoch 927/1000, Loss: 16.382193425713623, Weights: [0.43363235 0.06098486 0.50538279]\n",
      "Epoch 928/1000, Loss: 16.38245140677719, Weights: [0.43365579 0.06098634 0.50535786]\n",
      "Epoch 929/1000, Loss: 16.382709078735537, Weights: [0.43367921 0.06098782 0.50533297]\n",
      "Epoch 930/1000, Loss: 16.38296644226404, Weights: [0.43370259 0.06098929 0.50530811]\n",
      "Epoch 931/1000, Loss: 16.383223498035882, Weights: [0.43372595 0.06099077 0.50528328]\n",
      "Epoch 932/1000, Loss: 16.383480246722076, Weights: [0.43374928 0.06099224 0.50525848]\n",
      "Epoch 933/1000, Loss: 16.38373668899145, Weights: [0.43377258 0.06099371 0.50523371]\n",
      "Epoch 934/1000, Loss: 16.383992825510695, Weights: [0.43379585 0.06099517 0.50520897]\n",
      "Epoch 935/1000, Loss: 16.384248656944333, Weights: [0.4338191  0.06099664 0.50518426]\n",
      "Epoch 936/1000, Loss: 16.384504183954753, Weights: [0.43384232 0.0609981  0.50515958]\n",
      "Epoch 937/1000, Loss: 16.384759407202203, Weights: [0.43386551 0.06099956 0.50513493]\n",
      "Epoch 938/1000, Loss: 16.385014327344823, Weights: [0.43388867 0.06100102 0.50511031]\n",
      "Epoch 939/1000, Loss: 16.385268945038625, Weights: [0.4339118  0.06100248 0.50508572]\n",
      "Epoch 940/1000, Loss: 16.38552326093752, Weights: [0.43393491 0.06100394 0.50506116]\n",
      "Epoch 941/1000, Loss: 16.385777275693325, Weights: [0.43395798 0.06100539 0.50503663]\n",
      "Epoch 942/1000, Loss: 16.38603098995576, Weights: [0.43398103 0.06100684 0.50501212]\n",
      "Epoch 943/1000, Loss: 16.386284404372482, Weights: [0.43400406 0.06100829 0.50498765]\n",
      "Epoch 944/1000, Loss: 16.386537519589055, Weights: [0.43402705 0.06100974 0.50496321]\n",
      "Epoch 945/1000, Loss: 16.386790336248996, Weights: [0.43405002 0.06101119 0.50493879]\n",
      "Epoch 946/1000, Loss: 16.387042854993776, Weights: [0.43407296 0.06101263 0.50491441]\n",
      "Epoch 947/1000, Loss: 16.387295076462802, Weights: [0.43409587 0.06101408 0.50489005]\n",
      "Epoch 948/1000, Loss: 16.387547001293456, Weights: [0.43411876 0.06101552 0.50486572]\n",
      "Epoch 949/1000, Loss: 16.38779863012109, Weights: [0.43414162 0.06101696 0.50484143]\n",
      "Epoch 950/1000, Loss: 16.38804996357904, Weights: [0.43416445 0.0610184  0.50481716]\n",
      "Epoch 951/1000, Loss: 16.38830100229862, Weights: [0.43418725 0.06101983 0.50479292]\n",
      "Epoch 952/1000, Loss: 16.388551746909148, Weights: [0.43421003 0.06102127 0.5047687 ]\n",
      "Epoch 953/1000, Loss: 16.388802198037958, Weights: [0.43423278 0.0610227  0.50474452]\n",
      "Epoch 954/1000, Loss: 16.38905235631038, Weights: [0.4342555  0.06102413 0.50472037]\n",
      "Epoch 955/1000, Loss: 16.389302222349784, Weights: [0.4342782  0.06102556 0.50469624]\n",
      "Epoch 956/1000, Loss: 16.389551796777557, Weights: [0.43430087 0.06102699 0.50467215]\n",
      "Epoch 957/1000, Loss: 16.389801080213118, Weights: [0.43432351 0.06102841 0.50464808]\n",
      "Epoch 958/1000, Loss: 16.39005007327395, Weights: [0.43434613 0.06102984 0.50462404]\n",
      "Epoch 959/1000, Loss: 16.39029877657559, Weights: [0.43436872 0.06103126 0.50460003]\n",
      "Epoch 960/1000, Loss: 16.390547190731617, Weights: [0.43439128 0.06103268 0.50457604]\n",
      "Epoch 961/1000, Loss: 16.390795316353707, Weights: [0.43441382 0.0610341  0.50455209]\n",
      "Epoch 962/1000, Loss: 16.391043154051594, Weights: [0.43443633 0.06103551 0.50452816]\n",
      "Epoch 963/1000, Loss: 16.39129070443311, Weights: [0.43445881 0.06103693 0.50450426]\n",
      "Epoch 964/1000, Loss: 16.39153796810417, Weights: [0.43448127 0.06103834 0.50448039]\n",
      "Epoch 965/1000, Loss: 16.3917849456688, Weights: [0.4345037  0.06103975 0.50445655]\n",
      "Epoch 966/1000, Loss: 16.39203163772914, Weights: [0.4345261  0.06104116 0.50443273]\n",
      "Epoch 967/1000, Loss: 16.392278044885433, Weights: [0.43454848 0.06104257 0.50440895]\n",
      "Epoch 968/1000, Loss: 16.39252416773606, Weights: [0.43457083 0.06104398 0.50438519]\n",
      "Epoch 969/1000, Loss: 16.39277000687753, Weights: [0.43459316 0.06104538 0.50436146]\n",
      "Epoch 970/1000, Loss: 16.393015562904488, Weights: [0.43461546 0.06104679 0.50433776]\n",
      "Epoch 971/1000, Loss: 16.39326083640973, Weights: [0.43463773 0.06104819 0.50431408]\n",
      "Epoch 972/1000, Loss: 16.39350582798422, Weights: [0.43465998 0.06104959 0.50429043]\n",
      "Epoch 973/1000, Loss: 16.393750538217063, Weights: [0.4346822  0.06105099 0.50426681]\n",
      "Epoch 974/1000, Loss: 16.393994967695544, Weights: [0.4347044  0.06105238 0.50424322]\n",
      "Epoch 975/1000, Loss: 16.394239117005135, Weights: [0.43472657 0.06105378 0.50421965]\n",
      "Epoch 976/1000, Loss: 16.39448298672948, Weights: [0.43474871 0.06105517 0.50419611]\n",
      "Epoch 977/1000, Loss: 16.39472657745042, Weights: [0.43477083 0.06105656 0.5041726 ]\n",
      "Epoch 978/1000, Loss: 16.394969889748, Weights: [0.43479293 0.06105795 0.50414912]\n",
      "Epoch 979/1000, Loss: 16.395212924200468, Weights: [0.434815   0.06105934 0.50412566]\n",
      "Epoch 980/1000, Loss: 16.395455681384288, Weights: [0.43483704 0.06106072 0.50410224]\n",
      "Epoch 981/1000, Loss: 16.39569816187414, Weights: [0.43485906 0.06106211 0.50407883]\n",
      "Epoch 982/1000, Loss: 16.395940366242943, Weights: [0.43488105 0.06106349 0.50405546]\n",
      "Epoch 983/1000, Loss: 16.396182295061845, Weights: [0.43490302 0.06106487 0.50403211]\n",
      "Epoch 984/1000, Loss: 16.39642394890024, Weights: [0.43492496 0.06106625 0.50400879]\n",
      "Epoch 985/1000, Loss: 16.39666532832576, Weights: [0.43494687 0.06106763 0.5039855 ]\n",
      "Epoch 986/1000, Loss: 16.396906433904324, Weights: [0.43496876 0.06106901 0.50396223]\n",
      "Epoch 987/1000, Loss: 16.39714726620008, Weights: [0.43499063 0.06107038 0.50393899]\n",
      "Epoch 988/1000, Loss: 16.39738782577547, Weights: [0.43501247 0.06107175 0.50391577]\n",
      "Epoch 989/1000, Loss: 16.397628113191203, Weights: [0.43503429 0.06107313 0.50389259]\n",
      "Epoch 990/1000, Loss: 16.397868129006284, Weights: [0.43505608 0.0610745  0.50386943]\n",
      "Epoch 991/1000, Loss: 16.398107873777995, Weights: [0.43507784 0.06107586 0.50384629]\n",
      "Epoch 992/1000, Loss: 16.39834734806192, Weights: [0.43509959 0.06107723 0.50382318]\n",
      "Epoch 993/1000, Loss: 16.398586552411963, Weights: [0.4351213  0.06107859 0.5038001 ]\n",
      "Epoch 994/1000, Loss: 16.39882548738032, Weights: [0.43514299 0.06107996 0.50377705]\n",
      "Epoch 995/1000, Loss: 16.399064153517525, Weights: [0.43516466 0.06108132 0.50375402]\n",
      "Epoch 996/1000, Loss: 16.399302551372415, Weights: [0.4351863  0.06108268 0.50373102]\n",
      "Epoch 997/1000, Loss: 16.399540681492184, Weights: [0.43520792 0.06108404 0.50370804]\n",
      "Epoch 998/1000, Loss: 16.399778544422347, Weights: [0.43522951 0.06108539 0.50368509]\n",
      "Epoch 999/1000, Loss: 16.400016140706768, Weights: [0.43525108 0.06108675 0.50366217]\n",
      "Epoch 1000/1000, Loss: 16.400253470887666, Weights: [0.43527263 0.0610881  0.50363927]\n",
      "Test Data - Final Brand: Disney, Confidence Score: 0.3581785979445597\n",
      "Test Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Carvana, Predicted: Carvana, Confidence: 0.6512681938791487, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Febreze, Predicted: Febreze, Confidence: 0.6624995712489699, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Unilever, Predicted: Dove, Confidence: 0.6672309927313562, Loss: 1\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.964038562172586, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Discover Newport, Predicted: Discover Newport, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 0.9670220347252108, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: SmartLife, Predicted: Nectar, Confidence: 0.16985844376874884, Loss: 1\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.9340440694504215, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Missouri, Predicted: Missouri, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.964038562172586, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: HelloFresh, Predicted: HelloFresh, Confidence: 0.7306045032423248, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: CREATION MUSEUM, Predicted: CREATION MUSEUM, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Ruggable, Predicted: Ruggable, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: WELLS FARGO, Predicted: WELLS FARGO, Confidence: 0.9340440694504215, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.964038562172586, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Ziploc, Predicted: Ziploc, Confidence: 0.34572889265404516, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Xfinity, Predicted: Xfinity, Confidence: 0.6665959661467862, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Comcast, Predicted: Comcast, Confidence: 0.9305737906560861, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: COMCAST BUSINESS, Predicted: COMCAST BUSINESS, Confidence: 0.5168976322550637, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Xfinity Mobile, Predicted: Xfinity Mobile, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.964992639285903, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Mastercard, Predicted: Mastercard, Confidence: 0.9670220347252108, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.7007513789149289, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: ORIS, Predicted: ORIS, Confidence: 0.923813623437072, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Safelite, Predicted: Safelite, Confidence: 0.9670220347252108, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Baker's, Predicted: AT&T, Confidence: 0.32499914249793976, Loss: 1\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Freeform, Predicted: Freeform, Confidence: 0.49084502755698595, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Booking.com, Predicted: Booking.com, Confidence: 0.6542625716188107, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.6770147386644149, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: SHERWIN-WILLIAMS, Predicted: SHERWIN-WILLIAMS, Confidence: 0.3802955435551574, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Hulu, Predicted: Disney, Confidence: 0.2510918066773616, Loss: 1\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: MADE IN COOKWARE, Predicted: MADE IN COOKWARE, Confidence: 0.964038562172586, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Pandora, Predicted: Pandora, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Lincoln, Predicted: Lincoln, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: U.S. Bank, Predicted: U.S. Bank, Confidence: 0.9305737906560861, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: INK MASTER, Predicted: Paramount+, Confidence: 0.13380779233190582, Loss: 1\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: MAIN EVENT, Predicted: MAIN EVENT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Amazon Prime, Predicted: Amazon Prime, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: ARIAT, Predicted: ARIAT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.9658974003791337, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.5003571931869296, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Total Loss on Training Data: 5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Example dataset\n",
    "data = data_set\n",
    "\n",
    "# Combining confidence scores with weights\n",
    "def combine_confidences(inputs, weights):\n",
    "    combined_scores = defaultdict(float)\n",
    "    for source, weight in zip(['gemini_results', 'vi_results', 'ocr_text'], weights):\n",
    "        for brand, score in inputs[source]:\n",
    "            combined_scores[brand] += weight * score\n",
    "    return combined_scores\n",
    "\n",
    "# Normalizing the scores\n",
    "def normalize_scores(combined_scores):\n",
    "    total_score = sum(combined_scores.values())\n",
    "    if total_score == 0:\n",
    "        return combined_scores\n",
    "    for brand in combined_scores:\n",
    "        combined_scores[brand] /= total_score\n",
    "    return combined_scores\n",
    "\n",
    "# Computing the final prediction\n",
    "def get_final_brand(inputs, weights):\n",
    "    combined_scores = combine_confidences(inputs, weights)\n",
    "    normalized_scores = normalize_scores(combined_scores)\n",
    "    final_brand = max(normalized_scores, key=normalized_scores.get)\n",
    "    final_confidence = normalized_scores[final_brand]\n",
    "    return final_brand, final_confidence, normalized_scores\n",
    "\n",
    "# Calculating cross-entropy loss\n",
    "def cross_entropy_loss(predicted_probs, actual_label, all_brands):\n",
    "    epsilon = 1e-15\n",
    "    predicted_probs = np.array([predicted_probs.get(brand, epsilon) for brand in all_brands])\n",
    "    predicted_probs = np.clip(predicted_probs, epsilon, 1 - epsilon)\n",
    "    actual_vector = np.array([1 if brand == actual_label else 0 for brand in all_brands])\n",
    "    return -np.sum(actual_vector * np.log(predicted_probs))\n",
    "\n",
    "# Simple binary loss for evaluation\n",
    "def calculate_loss(predicted, actual):\n",
    "    return 1 if predicted != actual else 0\n",
    "\n",
    "# Optimizing the weights using Adam optimizer\n",
    "def optimize_weights(data_set, learning_rate=0.01, epochs=1000):\n",
    "    weights = np.ones(3) / 3  # Initial weights for gemini_results, vi_results, ocr_text\n",
    "    m = np.zeros(3)\n",
    "    v = np.zeros(3)\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    t = 0\n",
    "    \n",
    "    best_weights = weights.copy()\n",
    "    min_loss = float('inf')\n",
    "    \n",
    "    all_brands = set()\n",
    "    for entry in data_set:\n",
    "        for source in ['gemini_results', 'vi_results', 'ocr_text']:\n",
    "            all_brands.update([brand for brand, _ in entry['inputs'][source]])\n",
    "    all_brands = list(all_brands)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        weight_gradients = np.zeros(3)\n",
    "        for entry in data_set:\n",
    "            actual_final = entry['output']['final'][0]\n",
    "            final_brand, _, predicted_probs = get_final_brand(entry['inputs'], weights)\n",
    "            loss = cross_entropy_loss(predicted_probs, actual_final, all_brands)\n",
    "            total_loss += loss\n",
    "            for i, source in enumerate(['gemini_results', 'vi_results', 'ocr_text']):\n",
    "                for brand, score in entry['inputs'][source]:\n",
    "                    gradient = (predicted_probs.get(brand, 0) - (1 if brand == actual_final else 0)) * score\n",
    "                    weight_gradients[i] += gradient\n",
    "        \n",
    "        if total_loss < min_loss:\n",
    "            min_loss = total_loss\n",
    "            best_weights = weights.copy()\n",
    "        \n",
    "        t += 1\n",
    "        m = beta1 * m + (1 - beta1) * weight_gradients\n",
    "        v = beta2 * v + (1 - beta2) * (weight_gradients ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        weights -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        weights = np.clip(weights, 0, 1)\n",
    "        weights /= np.sum(weights)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss}, Weights: {weights}\")\n",
    "    return best_weights\n",
    "\n",
    "# Optimizing weights and testing\n",
    "final_weights = optimize_weights(data)\n",
    "\n",
    "# Test case\n",
    "test_data = [\n",
    "    {'inputs': {'gemini_results': [('Disney', 1.00), ('Ziploc', 1.00), ('SC Johnson', 1.00)], 'vi_results': [('Ziploc', 0.99), ('The Walt Disney Company', 0.99)], 'ocr_text': [('Disney', 0.90), ('Ziploc', 0.80), ('MIPS', 0.50)]}},\n",
    "]\n",
    "\n",
    "# Get predictions for test data\n",
    "for entry in test_data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    print(f\"Test Data - Final Brand: {final_brand}, Confidence Score: {final_confidence}\")\n",
    "    print(f\"Test Data - Weights: {final_weights}\")\n",
    "\n",
    "# Print results for training data and calculate total loss\n",
    "total_loss = 0\n",
    "for entry in data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    actual_final = entry['output']['final'][0]\n",
    "    loss = calculate_loss(final_brand, actual_final)\n",
    "    total_loss += loss\n",
    "    print(f\"Expected: {actual_final}, Predicted: {final_brand}, Confidence: {final_confidence}, Loss: {loss}\")\n",
    "    print(f\"Training Data - Weights: {final_weights}\")\n",
    "\n",
    "print(f\"Total Loss on Training Data: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Updated Code with Hybrid Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Cross-Entropy Loss: 25.01246291782739, Binary Loss: 7, Weights: [0.33993399 0.32013201 0.33993399]\n",
      "Epoch 2/1000, Cross-Entropy Loss: 24.556503349931294, Binary Loss: 7, Weights: [0.34645844 0.30707765 0.34646391]\n",
      "Epoch 3/1000, Cross-Entropy Loss: 24.112574863564728, Binary Loss: 6, Weights: [0.35289958 0.29418036 0.35292006]\n",
      "Epoch 4/1000, Cross-Entropy Loss: 23.68045234482656, Binary Loss: 6, Weights: [0.35924994 0.28145075 0.35929931]\n",
      "Epoch 5/1000, Cross-Entropy Loss: 23.259945014984943, Binary Loss: 5, Weights: [0.36550164 0.2688999  0.36559846]\n",
      "Epoch 6/1000, Cross-Entropy Loss: 22.85089482712831, Binary Loss: 5, Weights: [0.37164639 0.25653938 0.37181423]\n",
      "Epoch 7/1000, Cross-Entropy Loss: 22.45317499149614, Binary Loss: 5, Weights: [0.37767549 0.24438127 0.37794324]\n",
      "Epoch 8/1000, Cross-Entropy Loss: 22.066688581650215, Binary Loss: 5, Weights: [0.38357986 0.23243811 0.38398203]\n",
      "Epoch 9/1000, Cross-Entropy Loss: 21.691367171825096, Binary Loss: 5, Weights: [0.38935    0.22072293 0.38992707]\n",
      "Epoch 10/1000, Cross-Entropy Loss: 21.32716945349052, Binary Loss: 5, Weights: [0.39497605 0.20924921 0.39577474]\n",
      "Epoch 11/1000, Cross-Entropy Loss: 20.974079776712266, Binary Loss: 5, Weights: [0.4004478  0.19803086 0.40152134]\n",
      "Epoch 12/1000, Cross-Entropy Loss: 20.632106559749037, Binary Loss: 5, Weights: [0.40575474 0.18708215 0.40716311]\n",
      "Epoch 13/1000, Cross-Entropy Loss: 20.301280509001295, Binary Loss: 5, Weights: [0.41088608 0.17641767 0.41269626]\n",
      "Epoch 14/1000, Cross-Entropy Loss: 19.981652591563304, Binary Loss: 5, Weights: [0.41583083 0.16605225 0.41811692]\n",
      "Epoch 15/1000, Cross-Entropy Loss: 19.673291704928605, Binary Loss: 5, Weights: [0.4205779  0.15600085 0.42342125]\n",
      "Epoch 16/1000, Cross-Entropy Loss: 19.376281993606938, Binary Loss: 5, Weights: [0.42511612 0.14627849 0.42860539]\n",
      "Epoch 17/1000, Cross-Entropy Loss: 19.09071977124526, Binary Loss: 5, Weights: [0.42943443 0.13690003 0.43366554]\n",
      "Epoch 18/1000, Cross-Entropy Loss: 18.81671001990261, Binary Loss: 5, Weights: [0.43352193 0.12788009 0.43859799]\n",
      "Epoch 19/1000, Cross-Entropy Loss: 18.554362455761176, Binary Loss: 5, Weights: [0.43736805 0.11923282 0.44339912]\n",
      "Epoch 20/1000, Cross-Entropy Loss: 18.30378717274175, Binary Loss: 5, Weights: [0.44096271 0.11097175 0.44806554]\n",
      "Epoch 21/1000, Cross-Entropy Loss: 18.065089901699412, Binary Loss: 5, Weights: [0.44429642 0.10310952 0.45259406]\n",
      "Epoch 22/1000, Cross-Entropy Loss: 17.838366951969572, Binary Loss: 5, Weights: [0.44736052 0.0956577  0.45698177]\n",
      "Epoch 23/1000, Cross-Entropy Loss: 17.62369993223787, Binary Loss: 5, Weights: [0.4501473  0.08862655 0.46122614]\n",
      "Epoch 24/1000, Cross-Entropy Loss: 17.421150376660787, Binary Loss: 5, Weights: [0.45265018 0.08202479 0.46532503]\n",
      "Epoch 25/1000, Cross-Entropy Loss: 17.230754427106575, Binary Loss: 5, Weights: [0.45486384 0.07585939 0.46927677]\n",
      "Epoch 26/1000, Cross-Entropy Loss: 17.052517740444337, Binary Loss: 5, Weights: [0.45678443 0.07013536 0.47308021]\n",
      "Epoch 27/1000, Cross-Entropy Loss: 16.88641079838868, Binary Loss: 5, Weights: [0.45840964 0.06485561 0.47673475]\n",
      "Epoch 28/1000, Cross-Entropy Loss: 16.732364794605857, Binary Loss: 5, Weights: [0.45973881 0.06002079 0.4802404 ]\n",
      "Epoch 29/1000, Cross-Entropy Loss: 16.590268258774007, Binary Loss: 5, Weights: [0.46077303 0.05562918 0.48359779]\n",
      "Epoch 30/1000, Cross-Entropy Loss: 16.45996455055035, Binary Loss: 5, Weights: [0.46151517 0.05167665 0.48680818]\n",
      "Epoch 31/1000, Cross-Entropy Loss: 16.341250319779135, Binary Loss: 5, Weights: [0.46196989 0.04815667 0.48987345]\n",
      "Epoch 32/1000, Cross-Entropy Loss: 16.233874985810818, Binary Loss: 6, Weights: [0.46214362 0.0450603  0.49279608]\n",
      "Epoch 33/1000, Cross-Entropy Loss: 16.137541242325437, Binary Loss: 6, Weights: [0.46204451 0.04237633 0.49557916]\n",
      "Epoch 34/1000, Cross-Entropy Loss: 16.0519065486653, Binary Loss: 6, Weights: [0.46168238 0.04009133 0.49822629]\n",
      "Epoch 35/1000, Cross-Entropy Loss: 15.976585528199694, Binary Loss: 6, Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Epoch 36/1000, Cross-Entropy Loss: 15.911153161701842, Binary Loss: 4, Weights: [0.46021571 0.03665477 0.50312952]\n",
      "Epoch 37/1000, Cross-Entropy Loss: 15.855148641041811, Binary Loss: 4, Weights: [0.4591379  0.03546707 0.50539503]\n",
      "Epoch 38/1000, Cross-Entropy Loss: 15.808079736373815, Binary Loss: 4, Weights: [0.45785017 0.03460653 0.50754329]\n",
      "Epoch 39/1000, Cross-Entropy Loss: 15.769427527944163, Binary Loss: 4, Weights: [0.45636853 0.03405175 0.50957971]\n",
      "Epoch 40/1000, Cross-Entropy Loss: 15.738651360255217, Binary Loss: 5, Weights: [0.45470972 0.03378041 0.51150987]\n",
      "Epoch 41/1000, Cross-Entropy Loss: 15.715193889572392, Binary Loss: 5, Weights: [0.45289104 0.03376951 0.51333944]\n",
      "Epoch 42/1000, Cross-Entropy Loss: 15.698486113375903, Binary Loss: 5, Weights: [0.45093021 0.03399565 0.51507413]\n",
      "Epoch 43/1000, Cross-Entropy Loss: 15.68795229010463, Binary Loss: 5, Weights: [0.44884515 0.03443521 0.51671964]\n",
      "Epoch 44/1000, Cross-Entropy Loss: 15.683014677465803, Binary Loss: 5, Weights: [0.44665384 0.03506458 0.51828158]\n",
      "Epoch 45/1000, Cross-Entropy Loss: 15.683098036165106, Binary Loss: 5, Weights: [0.44437415 0.03586037 0.51976548]\n",
      "Epoch 46/1000, Cross-Entropy Loss: 15.687633862100828, Binary Loss: 5, Weights: [0.44202369 0.03679961 0.52117669]\n",
      "Epoch 47/1000, Cross-Entropy Loss: 15.696064323285817, Binary Loss: 5, Weights: [0.4396197  0.03785989 0.52252041]\n",
      "Epoch 48/1000, Cross-Entropy Loss: 15.70784588783994, Binary Loss: 5, Weights: [0.43717888 0.03901951 0.52380161]\n",
      "Epoch 49/1000, Cross-Entropy Loss: 15.722452636482835, Binary Loss: 5, Weights: [0.43471729 0.04025768 0.52502503]\n",
      "Epoch 50/1000, Cross-Entropy Loss: 15.739379257427451, Binary Loss: 5, Weights: [0.43225024 0.04155457 0.52619519]\n",
      "Epoch 51/1000, Cross-Entropy Loss: 15.758143723939522, Binary Loss: 5, Weights: [0.42979221 0.04289146 0.52731633]\n",
      "Epoch 52/1000, Cross-Entropy Loss: 15.778289655658192, Binary Loss: 5, Weights: [0.42735677 0.04425082 0.52839242]\n",
      "Epoch 53/1000, Cross-Entropy Loss: 15.799388364635627, Binary Loss: 5, Weights: [0.42495647 0.04561636 0.52942717]\n",
      "Epoch 54/1000, Cross-Entropy Loss: 15.821040586467928, Binary Loss: 5, Weights: [0.42260283 0.04697315 0.53042402]\n",
      "Epoch 55/1000, Cross-Entropy Loss: 15.842877896296383, Binary Loss: 5, Weights: [0.42030628 0.04830759 0.53138613]\n",
      "Epoch 56/1000, Cross-Entropy Loss: 15.86456380919971, Binary Loss: 5, Weights: [0.41807614 0.04960748 0.53231637]\n",
      "Epoch 57/1000, Cross-Entropy Loss: 15.885794564810862, Binary Loss: 5, Weights: [0.41592057 0.05086204 0.53321738]\n",
      "Epoch 58/1000, Cross-Entropy Loss: 15.906299597010566, Binary Loss: 5, Weights: [0.41384661 0.05206189 0.5340915 ]\n",
      "Epoch 59/1000, Cross-Entropy Loss: 15.925841691308944, Binary Loss: 5, Weights: [0.41186015 0.05319902 0.53494082]\n",
      "Epoch 60/1000, Cross-Entropy Loss: 15.944216834979706, Binary Loss: 5, Weights: [0.40996596 0.05426683 0.53576721]\n",
      "Epoch 61/1000, Cross-Entropy Loss: 15.96125376804216, Binary Loss: 4, Weights: [0.40816774 0.05526001 0.53657225]\n",
      "Epoch 62/1000, Cross-Entropy Loss: 15.976813246631288, Binary Loss: 4, Weights: [0.40646812 0.05617456 0.53735733]\n",
      "Epoch 63/1000, Cross-Entropy Loss: 15.990787033960071, Binary Loss: 4, Weights: [0.40486873 0.05700767 0.5381236 ]\n",
      "Epoch 64/1000, Cross-Entropy Loss: 16.003096637757405, Binary Loss: 4, Weights: [0.40337027 0.05775771 0.53887202]\n",
      "Epoch 65/1000, Cross-Entropy Loss: 16.01369181655948, Binary Loss: 4, Weights: [0.40197254 0.05842412 0.53960334]\n",
      "Epoch 66/1000, Cross-Entropy Loss: 16.02254888036434, Binary Loss: 4, Weights: [0.40067453 0.05900734 0.54031813]\n",
      "Epoch 67/1000, Cross-Entropy Loss: 16.029668813781832, Binary Loss: 4, Weights: [0.39947449 0.05950872 0.5410168 ]\n",
      "Epoch 68/1000, Cross-Entropy Loss: 16.035075251814042, Binary Loss: 4, Weights: [0.39836998 0.05993043 0.5416996 ]\n",
      "Epoch 69/1000, Cross-Entropy Loss: 16.03881233971874, Binary Loss: 4, Weights: [0.39735799 0.06027537 0.54236664]\n",
      "Epoch 70/1000, Cross-Entropy Loss: 16.04094250901141, Binary Loss: 4, Weights: [0.396435   0.06054708 0.54301792]\n",
      "Epoch 71/1000, Cross-Entropy Loss: 16.041544201562772, Binary Loss: 4, Weights: [0.39559704 0.06074964 0.54365332]\n",
      "Epoch 72/1000, Cross-Entropy Loss: 16.040709572989464, Binary Loss: 4, Weights: [0.39483977 0.06088758 0.54427265]\n",
      "Epoch 73/1000, Cross-Entropy Loss: 16.03854220518404, Binary Loss: 4, Weights: [0.39415861 0.06096577 0.54487562]\n",
      "Epoch 74/1000, Cross-Entropy Loss: 16.035154855971527, Binary Loss: 4, Weights: [0.39354873 0.06098939 0.54546188]\n",
      "Epoch 75/1000, Cross-Entropy Loss: 16.030667271608113, Binary Loss: 4, Weights: [0.39300517 0.06096377 0.54603106]\n",
      "Epoch 76/1000, Cross-Entropy Loss: 16.02520408525166, Binary Loss: 4, Weights: [0.3925229  0.06089436 0.54658275]\n",
      "Epoch 77/1000, Cross-Entropy Loss: 16.018892821729562, Binary Loss: 4, Weights: [0.39209686 0.06078664 0.5471165 ]\n",
      "Epoch 78/1000, Cross-Entropy Loss: 16.01186202599604, Binary Loss: 4, Weights: [0.39172206 0.06064605 0.54763189]\n",
      "Epoch 79/1000, Cross-Entropy Loss: 16.004239529688057, Binary Loss: 4, Weights: [0.39139356 0.06047794 0.5481285 ]\n",
      "Epoch 80/1000, Cross-Entropy Loss: 15.996150867225253, Binary Loss: 4, Weights: [0.39110658 0.0602875  0.54860592]\n",
      "Epoch 81/1000, Cross-Entropy Loss: 15.987717850010982, Binary Loss: 4, Weights: [0.39085652 0.0600797  0.54906378]\n",
      "Epoch 82/1000, Cross-Entropy Loss: 15.97905730452325, Binary Loss: 4, Weights: [0.39063899 0.05985927 0.54950175]\n",
      "Epoch 83/1000, Cross-Entropy Loss: 15.97027997747161, Binary Loss: 4, Weights: [0.39044982 0.05963064 0.54991954]\n",
      "Epoch 84/1000, Cross-Entropy Loss: 15.961489608762578, Binary Loss: 4, Weights: [0.39028512 0.05939795 0.55031693]\n",
      "Epoch 85/1000, Cross-Entropy Loss: 15.952782170780235, Binary Loss: 4, Weights: [0.39014128 0.05916497 0.55069374]\n",
      "Epoch 86/1000, Cross-Entropy Loss: 15.944245270460772, Binary Loss: 4, Weights: [0.390015   0.05893512 0.55104988]\n",
      "Epoch 87/1000, Cross-Entropy Loss: 15.935957708826125, Binary Loss: 4, Weights: [0.38990326 0.05871144 0.5513853 ]\n",
      "Epoch 88/1000, Cross-Entropy Loss: 15.927989191044707, Binary Loss: 4, Weights: [0.38980336 0.0584966  0.55170004]\n",
      "Epoch 89/1000, Cross-Entropy Loss: 15.920400178706268, Binary Loss: 4, Weights: [0.38971292 0.05829289 0.5519942 ]\n",
      "Epoch 90/1000, Cross-Entropy Loss: 15.913241874829893, Binary Loss: 4, Weights: [0.38962984 0.05810222 0.55226794]\n",
      "Epoch 91/1000, Cross-Entropy Loss: 15.906556331165373, Binary Loss: 4, Weights: [0.38955235 0.05792615 0.55252151]\n",
      "Epoch 92/1000, Cross-Entropy Loss: 15.90037666659264, Binary Loss: 4, Weights: [0.38947894 0.05776586 0.5527552 ]\n",
      "Epoch 93/1000, Cross-Entropy Loss: 15.89472738486423, Binary Loss: 4, Weights: [0.3894084  0.05762221 0.55296939]\n",
      "Epoch 94/1000, Cross-Entropy Loss: 15.889624779564683, Binary Loss: 4, Weights: [0.38933975 0.05749575 0.5531645 ]\n",
      "Epoch 95/1000, Cross-Entropy Loss: 15.88507741396824, Binary Loss: 4, Weights: [0.38927228 0.05738672 0.55334099]\n",
      "Epoch 96/1000, Cross-Entropy Loss: 15.88108666345252, Binary Loss: 4, Weights: [0.3892055  0.05729509 0.55349941]\n",
      "Epoch 97/1000, Cross-Entropy Loss: 15.877647308259446, Binary Loss: 4, Weights: [0.38913909 0.05722059 0.55364032]\n",
      "Epoch 98/1000, Cross-Entropy Loss: 15.874748164673269, Binary Loss: 4, Weights: [0.38907295 0.05716273 0.55376432]\n",
      "Epoch 99/1000, Cross-Entropy Loss: 15.872372743096289, Binary Loss: 4, Weights: [0.38900712 0.05712082 0.55387205]\n",
      "Epoch 100/1000, Cross-Entropy Loss: 15.870499922031373, Binary Loss: 4, Weights: [0.38894179 0.05709402 0.55396419]\n",
      "Epoch 101/1000, Cross-Entropy Loss: 15.869104627613021, Binary Loss: 4, Weights: [0.38887726 0.05708133 0.55404141]\n",
      "Epoch 102/1000, Cross-Entropy Loss: 15.868158509050021, Binary Loss: 4, Weights: [0.38881392 0.05708167 0.55410441]\n",
      "Epoch 103/1000, Cross-Entropy Loss: 15.867630601138112, Binary Loss: 4, Weights: [0.38875224 0.05709386 0.5541539 ]\n",
      "Epoch 104/1000, Cross-Entropy Loss: 15.867487965855686, Binary Loss: 4, Weights: [0.38869275 0.05711665 0.55419059]\n",
      "Epoch 105/1000, Cross-Entropy Loss: 15.867696305954164, Binary Loss: 4, Weights: [0.38863602 0.05714878 0.5542152 ]\n",
      "Epoch 106/1000, Cross-Entropy Loss: 15.868220544383162, Binary Loss: 4, Weights: [0.38858262 0.05718896 0.55422841]\n",
      "Epoch 107/1000, Cross-Entropy Loss: 15.869025364335199, Binary Loss: 4, Weights: [0.38853316 0.05723591 0.55423093]\n",
      "Epoch 108/1000, Cross-Entropy Loss: 15.870075705641055, Binary Loss: 4, Weights: [0.3884882  0.05728838 0.55422342]\n",
      "Epoch 109/1000, Cross-Entropy Loss: 15.871337214183994, Binary Loss: 4, Weights: [0.38844832 0.05734514 0.55420654]\n",
      "Epoch 110/1000, Cross-Entropy Loss: 15.872776641915399, Binary Loss: 4, Weights: [0.38841402 0.05740505 0.55418092]\n",
      "Epoch 111/1000, Cross-Entropy Loss: 15.874362195936532, Binary Loss: 4, Weights: [0.38838581 0.05746702 0.55414717]\n",
      "Epoch 112/1000, Cross-Entropy Loss: 15.87606383595023, Binary Loss: 4, Weights: [0.3883641  0.05753003 0.55410587]\n",
      "Epoch 113/1000, Cross-Entropy Loss: 15.87785352017431, Binary Loss: 4, Weights: [0.38834929 0.05759315 0.55405757]\n",
      "Epoch 114/1000, Cross-Entropy Loss: 15.879705400537727, Binary Loss: 4, Weights: [0.38834168 0.05765555 0.55400278]\n",
      "Epoch 115/1000, Cross-Entropy Loss: 15.881595968644001, Binary Loss: 4, Weights: [0.38834153 0.05771648 0.55394199]\n",
      "Epoch 116/1000, Cross-Entropy Loss: 15.883504154580185, Binary Loss: 4, Weights: [0.38834903 0.05777531 0.55387566]\n",
      "Epoch 117/1000, Cross-Entropy Loss: 15.885411381168913, Binary Loss: 4, Weights: [0.38836431 0.05783148 0.55380421]\n",
      "Epoch 118/1000, Cross-Entropy Loss: 15.887301576703711, Binary Loss: 4, Weights: [0.38838741 0.05788455 0.55372804]\n",
      "Epoch 119/1000, Cross-Entropy Loss: 15.889161149572512, Binary Loss: 4, Weights: [0.38841835 0.05793416 0.55364749]\n",
      "Epoch 120/1000, Cross-Entropy Loss: 15.8909789284611, Binary Loss: 4, Weights: [0.38845706 0.05798004 0.55356291]\n",
      "Epoch 121/1000, Cross-Entropy Loss: 15.892746072038152, Binary Loss: 4, Weights: [0.3885034  0.05802202 0.55347458]\n",
      "Epoch 122/1000, Cross-Entropy Loss: 15.894455952158838, Binary Loss: 4, Weights: [0.38855723 0.05805999 0.55338278]\n",
      "Epoch 123/1000, Cross-Entropy Loss: 15.896104014687955, Binary Loss: 4, Weights: [0.3886183  0.05809394 0.55328775]\n",
      "Epoch 124/1000, Cross-Entropy Loss: 15.897687622040653, Binary Loss: 4, Weights: [0.38868637 0.05812391 0.55318972]\n",
      "Epoch 125/1000, Cross-Entropy Loss: 15.899205881473113, Binary Loss: 4, Weights: [0.38876113 0.05814999 0.55308888]\n",
      "Epoch 126/1000, Cross-Entropy Loss: 15.900659463033895, Binary Loss: 4, Weights: [0.38884225 0.05817234 0.5529854 ]\n",
      "Epoch 127/1000, Cross-Entropy Loss: 15.90205041091419, Binary Loss: 4, Weights: [0.38892939 0.05819117 0.55287945]\n",
      "Epoch 128/1000, Cross-Entropy Loss: 15.903381951718709, Binary Loss: 4, Weights: [0.38902215 0.0582067  0.55277115]\n",
      "Epoch 129/1000, Cross-Entropy Loss: 15.904658302925533, Binary Loss: 4, Weights: [0.38912016 0.05821921 0.55266064]\n",
      "Epoch 130/1000, Cross-Entropy Loss: 15.905884484519154, Binary Loss: 4, Weights: [0.38922301 0.05822897 0.55254802]\n",
      "Epoch 131/1000, Cross-Entropy Loss: 15.907066136473517, Binary Loss: 4, Weights: [0.3893303 0.0582363 0.5524334]\n",
      "Epoch 132/1000, Cross-Entropy Loss: 15.908209344437674, Binary Loss: 4, Weights: [0.38944164 0.05824151 0.55231686]\n",
      "Epoch 133/1000, Cross-Entropy Loss: 15.909320475642065, Binary Loss: 4, Weights: [0.38955662 0.0582449  0.55219848]\n",
      "Epoch 134/1000, Cross-Entropy Loss: 15.910406026704779, Binary Loss: 4, Weights: [0.38967486 0.05824679 0.55207834]\n",
      "Epoch 135/1000, Cross-Entropy Loss: 15.911472484680013, Binary Loss: 4, Weights: [0.38979599 0.05824749 0.55195651]\n",
      "Epoch 136/1000, Cross-Entropy Loss: 15.912526202360748, Binary Loss: 4, Weights: [0.38991965 0.05824729 0.55183306]\n",
      "Epoch 137/1000, Cross-Entropy Loss: 15.913573288529081, Binary Loss: 4, Weights: [0.39004549 0.05824646 0.55170804]\n",
      "Epoch 138/1000, Cross-Entropy Loss: 15.914619513544768, Binary Loss: 4, Weights: [0.39017321 0.05824526 0.55158153]\n",
      "Epoch 139/1000, Cross-Entropy Loss: 15.915670230379162, Binary Loss: 4, Weights: [0.3903025  0.05824393 0.55145358]\n",
      "Epoch 140/1000, Cross-Entropy Loss: 15.916730310940842, Binary Loss: 4, Weights: [0.39043308 0.05824267 0.55132424]\n",
      "Epoch 141/1000, Cross-Entropy Loss: 15.91780409730278, Binary Loss: 4, Weights: [0.39056472 0.05824168 0.5511936 ]\n",
      "Epoch 142/1000, Cross-Entropy Loss: 15.918895367231475, Binary Loss: 4, Weights: [0.39069718 0.05824112 0.5510617 ]\n",
      "Epoch 143/1000, Cross-Entropy Loss: 15.920007313236587, Binary Loss: 4, Weights: [0.39083026 0.05824113 0.55092861]\n",
      "Epoch 144/1000, Cross-Entropy Loss: 15.921142534206199, Binary Loss: 4, Weights: [0.39096379 0.05824182 0.55079439]\n",
      "Epoch 145/1000, Cross-Entropy Loss: 15.922303038568248, Binary Loss: 4, Weights: [0.39109761 0.05824328 0.55065912]\n",
      "Epoch 146/1000, Cross-Entropy Loss: 15.923490257822149, Binary Loss: 4, Weights: [0.39123158 0.05824556 0.55052285]\n",
      "Epoch 147/1000, Cross-Entropy Loss: 15.92470506921578, Binary Loss: 4, Weights: [0.39136561 0.05824872 0.55038567]\n",
      "Epoch 148/1000, Cross-Entropy Loss: 15.925947826300485, Binary Loss: 4, Weights: [0.3914996  0.05825277 0.55024763]\n",
      "Epoch 149/1000, Cross-Entropy Loss: 15.927218396079224, Binary Loss: 4, Weights: [0.39163347 0.05825771 0.55010882]\n",
      "Epoch 150/1000, Cross-Entropy Loss: 15.928516201468119, Binary Loss: 4, Weights: [0.39176717 0.05826353 0.5499693 ]\n",
      "Epoch 151/1000, Cross-Entropy Loss: 15.92984026781866, Binary Loss: 4, Weights: [0.39190066 0.0582702  0.54982914]\n",
      "Epoch 152/1000, Cross-Entropy Loss: 15.931189272292775, Binary Loss: 4, Weights: [0.3920339  0.05827767 0.54968843]\n",
      "Epoch 153/1000, Cross-Entropy Loss: 15.932561594944987, Binary Loss: 4, Weights: [0.39216689 0.05828589 0.54954722]\n",
      "Epoch 154/1000, Cross-Entropy Loss: 15.93395537044188, Binary Loss: 4, Weights: [0.39229962 0.05829479 0.54940559]\n",
      "Epoch 155/1000, Cross-Entropy Loss: 15.935368539436556, Binary Loss: 4, Weights: [0.39243208 0.05830431 0.54926362]\n",
      "Epoch 156/1000, Cross-Entropy Loss: 15.936798898712333, Binary Loss: 4, Weights: [0.39256428 0.05831437 0.54912136]\n",
      "Epoch 157/1000, Cross-Entropy Loss: 15.938244149313482, Binary Loss: 4, Weights: [0.39269623 0.05832488 0.54897889]\n",
      "Epoch 158/1000, Cross-Entropy Loss: 15.93970194198843, Binary Loss: 4, Weights: [0.39282795 0.05833578 0.54883627]\n",
      "Epoch 159/1000, Cross-Entropy Loss: 15.941169919380643, Binary Loss: 4, Weights: [0.39295946 0.05834698 0.54869356]\n",
      "Epoch 160/1000, Cross-Entropy Loss: 15.942645754512489, Binary Loss: 4, Weights: [0.39309077 0.05835841 0.54855082]\n",
      "Epoch 161/1000, Cross-Entropy Loss: 15.944127185215384, Binary Loss: 4, Weights: [0.3932219  0.05836999 0.54840811]\n",
      "Epoch 162/1000, Cross-Entropy Loss: 15.94561204426405, Binary Loss: 4, Weights: [0.39335286 0.05838165 0.54826549]\n",
      "Epoch 163/1000, Cross-Entropy Loss: 15.947098285072187, Binary Loss: 4, Weights: [0.39348368 0.05839333 0.54812299]\n",
      "Epoch 164/1000, Cross-Entropy Loss: 15.948584002899839, Binary Loss: 4, Weights: [0.39361437 0.05840497 0.54798067]\n",
      "Epoch 165/1000, Cross-Entropy Loss: 15.950067451608392, Binary Loss: 4, Weights: [0.39374492 0.05841651 0.54783857]\n",
      "Epoch 166/1000, Cross-Entropy Loss: 15.95154705607627, Binary Loss: 4, Weights: [0.39387536 0.05842791 0.54769672]\n",
      "Epoch 167/1000, Cross-Entropy Loss: 15.953021420456734, Binary Loss: 4, Weights: [0.39400568 0.05843914 0.54755518]\n",
      "Epoch 168/1000, Cross-Entropy Loss: 15.954489332517857, Binary Loss: 4, Weights: [0.39413589 0.05845015 0.54741396]\n",
      "Epoch 169/1000, Cross-Entropy Loss: 15.955949764354118, Binary Loss: 4, Weights: [0.39426598 0.05846092 0.5472731 ]\n",
      "Epoch 170/1000, Cross-Entropy Loss: 15.957401869798272, Binary Loss: 4, Weights: [0.39439594 0.05847143 0.54713263]\n",
      "Epoch 171/1000, Cross-Entropy Loss: 15.958844978892254, Binary Loss: 4, Weights: [0.39452576 0.05848167 0.54699257]\n",
      "Epoch 172/1000, Cross-Entropy Loss: 15.960278589796477, Binary Loss: 4, Weights: [0.39465543 0.05849164 0.54685293]\n",
      "Epoch 173/1000, Cross-Entropy Loss: 15.96170235852858, Binary Loss: 4, Weights: [0.39478493 0.05850132 0.54671375]\n",
      "Epoch 174/1000, Cross-Entropy Loss: 15.963116086926506, Binary Loss: 4, Weights: [0.39491424 0.05851073 0.54657503]\n",
      "Epoch 175/1000, Cross-Entropy Loss: 15.964519709226462, Binary Loss: 4, Weights: [0.39504335 0.05851987 0.54643678]\n",
      "Epoch 176/1000, Cross-Entropy Loss: 15.965913277635709, Binary Loss: 4, Weights: [0.39517223 0.05852875 0.54629902]\n",
      "Epoch 177/1000, Cross-Entropy Loss: 15.967296947262975, Binary Loss: 4, Weights: [0.39530086 0.05853738 0.54616176]\n",
      "Epoch 178/1000, Cross-Entropy Loss: 15.968670960747344, Binary Loss: 4, Weights: [0.39542921 0.05854579 0.546025  ]\n",
      "Epoch 179/1000, Cross-Entropy Loss: 15.970035632899894, Binary Loss: 4, Weights: [0.39555726 0.05855399 0.54588875]\n",
      "Epoch 180/1000, Cross-Entropy Loss: 15.971391335642574, Binary Loss: 4, Weights: [0.39568498 0.058562   0.54575302]\n",
      "Epoch 181/1000, Cross-Entropy Loss: 15.972738483496261, Binary Loss: 4, Weights: [0.39581235 0.05856984 0.54561781]\n",
      "Epoch 182/1000, Cross-Entropy Loss: 15.97407751983567, Binary Loss: 4, Weights: [0.39593935 0.05857753 0.54548312]\n",
      "Epoch 183/1000, Cross-Entropy Loss: 15.975408904093843, Binary Loss: 4, Weights: [0.39606594 0.0585851  0.54534896]\n",
      "Epoch 184/1000, Cross-Entropy Loss: 15.976733100063237, Binary Loss: 4, Weights: [0.39619212 0.05859256 0.54521532]\n",
      "Epoch 185/1000, Cross-Entropy Loss: 15.978050565405857, Binary Loss: 4, Weights: [0.39631785 0.05859994 0.5450822 ]\n",
      "Epoch 186/1000, Cross-Entropy Loss: 15.979361742450866, Binary Loss: 4, Weights: [0.39644312 0.05860726 0.54494962]\n",
      "Epoch 187/1000, Cross-Entropy Loss: 15.980667050326103, Binary Loss: 4, Weights: [0.39656792 0.05861452 0.54481756]\n",
      "Epoch 188/1000, Cross-Entropy Loss: 15.981966878439755, Binary Loss: 4, Weights: [0.39669222 0.05862175 0.54468603]\n",
      "Epoch 189/1000, Cross-Entropy Loss: 15.983261581301107, Binary Loss: 4, Weights: [0.39681601 0.05862896 0.54455503]\n",
      "Epoch 190/1000, Cross-Entropy Loss: 15.984551474644167, Binary Loss: 4, Weights: [0.39693928 0.05863617 0.54442456]\n",
      "Epoch 191/1000, Cross-Entropy Loss: 15.985836832796622, Binary Loss: 4, Weights: [0.39706202 0.05864337 0.54429461]\n",
      "Epoch 192/1000, Cross-Entropy Loss: 15.987117887217643, Binary Loss: 4, Weights: [0.39718422 0.05865059 0.54416519]\n",
      "Epoch 193/1000, Cross-Entropy Loss: 15.988394826112867, Binary Loss: 4, Weights: [0.39730587 0.05865783 0.5440363 ]\n",
      "Epoch 194/1000, Cross-Entropy Loss: 15.989667795022756, Binary Loss: 4, Weights: [0.39742698 0.05866509 0.54390794]\n",
      "Epoch 195/1000, Cross-Entropy Loss: 15.990936898271377, Binary Loss: 4, Weights: [0.39754753 0.05867237 0.5437801 ]\n",
      "Epoch 196/1000, Cross-Entropy Loss: 15.99220220115705, Binary Loss: 4, Weights: [0.39766752 0.05867968 0.5436528 ]\n",
      "Epoch 197/1000, Cross-Entropy Loss: 15.993463732762937, Binary Loss: 4, Weights: [0.39778696 0.05868702 0.54352602]\n",
      "Epoch 198/1000, Cross-Entropy Loss: 15.99472148926563, Binary Loss: 4, Weights: [0.39790584 0.05869438 0.54339978]\n",
      "Epoch 199/1000, Cross-Entropy Loss: 15.995975437621707, Binary Loss: 4, Weights: [0.39802417 0.05870177 0.54327406]\n",
      "Epoch 200/1000, Cross-Entropy Loss: 15.997225519516537, Binary Loss: 4, Weights: [0.39814194 0.05870918 0.54314888]\n",
      "Epoch 201/1000, Cross-Entropy Loss: 15.99847165546591, Binary Loss: 4, Weights: [0.39825918 0.0587166  0.54302422]\n",
      "Epoch 202/1000, Cross-Entropy Loss: 15.999713748968746, Binary Loss: 4, Weights: [0.39837586 0.05872404 0.5429001 ]\n",
      "Epoch 203/1000, Cross-Entropy Loss: 16.000951690618177, Binary Loss: 4, Weights: [0.39849202 0.05873148 0.5427765 ]\n",
      "Epoch 204/1000, Cross-Entropy Loss: 16.002185362088564, Binary Loss: 4, Weights: [0.39860763 0.05873893 0.54265344]\n",
      "Epoch 205/1000, Cross-Entropy Loss: 16.003414639926397, Binary Loss: 4, Weights: [0.39872273 0.05874637 0.54253091]\n",
      "Epoch 206/1000, Cross-Entropy Loss: 16.004639399084688, Binary Loss: 4, Weights: [0.3988373  0.0587538  0.54240891]\n",
      "Epoch 207/1000, Cross-Entropy Loss: 16.00585951615138, Binary Loss: 4, Weights: [0.39895135 0.05876121 0.54228744]\n",
      "Epoch 208/1000, Cross-Entropy Loss: 16.007074872233794, Binary Loss: 4, Weights: [0.3990649  0.05876861 0.54216649]\n",
      "Epoch 209/1000, Cross-Entropy Loss: 16.00828535547201, Binary Loss: 4, Weights: [0.39917794 0.05877598 0.54204608]\n",
      "Epoch 210/1000, Cross-Entropy Loss: 16.00949086316443, Binary Loss: 4, Weights: [0.39929048 0.05878332 0.5419262 ]\n",
      "Epoch 211/1000, Cross-Entropy Loss: 16.010691303498717, Binary Loss: 4, Weights: [0.39940253 0.05879063 0.54180684]\n",
      "Epoch 212/1000, Cross-Entropy Loss: 16.011886596890193, Binary Loss: 4, Weights: [0.39951409 0.05879791 0.541688  ]\n",
      "Epoch 213/1000, Cross-Entropy Loss: 16.013076676937683, Binary Loss: 4, Weights: [0.39962517 0.05880514 0.54156969]\n",
      "Epoch 214/1000, Cross-Entropy Loss: 16.014261491014192, Binary Loss: 4, Weights: [0.39973576 0.05881233 0.5414519 ]\n",
      "Epoch 215/1000, Cross-Entropy Loss: 16.015441000515512, Binary Loss: 4, Weights: [0.39984589 0.05881948 0.54133463]\n",
      "Epoch 216/1000, Cross-Entropy Loss: 16.016615180795004, Binary Loss: 4, Weights: [0.39995554 0.05882658 0.54121788]\n",
      "Epoch 217/1000, Cross-Entropy Loss: 16.01778402081658, Binary Loss: 4, Weights: [0.40006472 0.05883364 0.54110164]\n",
      "Epoch 218/1000, Cross-Entropy Loss: 16.018947522560815, Binary Loss: 4, Weights: [0.40017344 0.05884066 0.54098591]\n",
      "Epoch 219/1000, Cross-Entropy Loss: 16.02010570022091, Binary Loss: 4, Weights: [0.40028169 0.05884762 0.54087069]\n",
      "Epoch 220/1000, Cross-Entropy Loss: 16.021258579226025, Binary Loss: 4, Weights: [0.40038949 0.05885454 0.54075597]\n",
      "Epoch 221/1000, Cross-Entropy Loss: 16.022406195129715, Binary Loss: 4, Weights: [0.40049683 0.05886142 0.54064175]\n",
      "Epoch 222/1000, Cross-Entropy Loss: 16.02354859240009, Binary Loss: 4, Weights: [0.40060371 0.05886825 0.54052803]\n",
      "Epoch 223/1000, Cross-Entropy Loss: 16.024685823147035, Binary Loss: 4, Weights: [0.40071015 0.05887504 0.54041481]\n",
      "Epoch 224/1000, Cross-Entropy Loss: 16.025817945819668, Binary Loss: 4, Weights: [0.40081613 0.05888179 0.54030208]\n",
      "Epoch 225/1000, Cross-Entropy Loss: 16.02694502390446, Binary Loss: 4, Weights: [0.40092166 0.0588885  0.54018983]\n",
      "Epoch 226/1000, Cross-Entropy Loss: 16.028067124651677, Binary Loss: 4, Weights: [0.40102675 0.05889518 0.54007807]\n",
      "Epoch 227/1000, Cross-Entropy Loss: 16.029184317854185, Binary Loss: 4, Weights: [0.4011314  0.05890181 0.53996679]\n",
      "Epoch 228/1000, Cross-Entropy Loss: 16.030296674699425, Binary Loss: 4, Weights: [0.4012356  0.05890842 0.53985599]\n",
      "Epoch 229/1000, Cross-Entropy Loss: 16.03140426671153, Binary Loss: 4, Weights: [0.40133936 0.05891499 0.53974566]\n",
      "Epoch 230/1000, Cross-Entropy Loss: 16.03250716479709, Binary Loss: 4, Weights: [0.40144268 0.05892153 0.53963579]\n",
      "Epoch 231/1000, Cross-Entropy Loss: 16.03360543840433, Binary Loss: 4, Weights: [0.40154556 0.05892804 0.53952639]\n",
      "Epoch 232/1000, Cross-Entropy Loss: 16.034699154802283, Binary Loss: 4, Weights: [0.40164801 0.05893453 0.53941746]\n",
      "Epoch 233/1000, Cross-Entropy Loss: 16.035788378482927, Binary Loss: 4, Weights: [0.40175003 0.05894099 0.53930898]\n",
      "Epoch 234/1000, Cross-Entropy Loss: 16.036873170686693, Binary Loss: 4, Weights: [0.40185162 0.05894742 0.53920096]\n",
      "Epoch 235/1000, Cross-Entropy Loss: 16.03795358904864, Binary Loss: 4, Weights: [0.40195278 0.05895383 0.53909338]\n",
      "Epoch 236/1000, Cross-Entropy Loss: 16.039029687360593, Binary Loss: 4, Weights: [0.40205352 0.05896022 0.53898626]\n",
      "Epoch 237/1000, Cross-Entropy Loss: 16.04010151544209, Binary Loss: 4, Weights: [0.40215384 0.05896659 0.53887958]\n",
      "Epoch 238/1000, Cross-Entropy Loss: 16.041169119111693, Binary Loss: 4, Weights: [0.40225373 0.05897293 0.53877333]\n",
      "Epoch 239/1000, Cross-Entropy Loss: 16.04223254024855, Binary Loss: 4, Weights: [0.40235321 0.05897926 0.53866753]\n",
      "Epoch 240/1000, Cross-Entropy Loss: 16.043291816933355, Binary Loss: 4, Weights: [0.40245228 0.05898556 0.53856216]\n",
      "Epoch 241/1000, Cross-Entropy Loss: 16.04434698365707, Binary Loss: 4, Weights: [0.40255093 0.05899185 0.53845722]\n",
      "Epoch 242/1000, Cross-Entropy Loss: 16.045398071585428, Binary Loss: 4, Weights: [0.40264918 0.05899811 0.53835271]\n",
      "Epoch 243/1000, Cross-Entropy Loss: 16.046445108867395, Binary Loss: 4, Weights: [0.40274702 0.05900436 0.53824862]\n",
      "Epoch 244/1000, Cross-Entropy Loss: 16.04748812097573, Binary Loss: 4, Weights: [0.40284446 0.05901058 0.53814496]\n",
      "Epoch 245/1000, Cross-Entropy Loss: 16.048527131068564, Binary Loss: 4, Weights: [0.4029415  0.05901679 0.53804171]\n",
      "Epoch 246/1000, Cross-Entropy Loss: 16.049562160361265, Binary Loss: 4, Weights: [0.40303815 0.05902297 0.53793888]\n",
      "Epoch 247/1000, Cross-Entropy Loss: 16.050593228499075, Binary Loss: 4, Weights: [0.40313441 0.05902913 0.53783646]\n",
      "Epoch 248/1000, Cross-Entropy Loss: 16.05162035392169, Binary Loss: 4, Weights: [0.40323027 0.05903527 0.53773445]\n",
      "Epoch 249/1000, Cross-Entropy Loss: 16.05264355421211, Binary Loss: 4, Weights: [0.40332576 0.05904139 0.53763285]\n",
      "Epoch 250/1000, Cross-Entropy Loss: 16.053662846423432, Binary Loss: 4, Weights: [0.40342086 0.05904749 0.53753165]\n",
      "Epoch 251/1000, Cross-Entropy Loss: 16.054678247378064, Binary Loss: 4, Weights: [0.40351558 0.05905357 0.53743086]\n",
      "Epoch 252/1000, Cross-Entropy Loss: 16.05568977393544, Binary Loss: 4, Weights: [0.40360992 0.05905962 0.53733046]\n",
      "Epoch 253/1000, Cross-Entropy Loss: 16.056697443225072, Binary Loss: 4, Weights: [0.40370389 0.05906565 0.53723045]\n",
      "Epoch 254/1000, Cross-Entropy Loss: 16.057701272843232, Binary Loss: 4, Weights: [0.40379749 0.05907166 0.53713084]\n",
      "Epoch 255/1000, Cross-Entropy Loss: 16.058701281012326, Binary Loss: 4, Weights: [0.40389073 0.05907765 0.53703162]\n",
      "Epoch 256/1000, Cross-Entropy Loss: 16.059697486703204, Binary Loss: 4, Weights: [0.4039836  0.05908361 0.53693279]\n",
      "Epoch 257/1000, Cross-Entropy Loss: 16.06068990972123, Binary Loss: 4, Weights: [0.40407611 0.05908955 0.53683434]\n",
      "Epoch 258/1000, Cross-Entropy Loss: 16.061678570758062, Binary Loss: 4, Weights: [0.40416826 0.05909547 0.53673627]\n",
      "Epoch 259/1000, Cross-Entropy Loss: 16.0626634914113, Binary Loss: 4, Weights: [0.40426005 0.05910136 0.53663858]\n",
      "Epoch 260/1000, Cross-Entropy Loss: 16.063644694175, Binary Loss: 4, Weights: [0.4043515  0.05910723 0.53654127]\n",
      "Epoch 261/1000, Cross-Entropy Loss: 16.064622202404315, Binary Loss: 4, Weights: [0.40444259 0.05911308 0.53644433]\n",
      "Epoch 262/1000, Cross-Entropy Loss: 16.06559604025773, Binary Loss: 4, Weights: [0.40453333 0.05911891 0.53634776]\n",
      "Epoch 263/1000, Cross-Entropy Loss: 16.066566232620747, Binary Loss: 4, Weights: [0.40462373 0.05912471 0.53625156]\n",
      "Epoch 264/1000, Cross-Entropy Loss: 16.06753280501465, Binary Loss: 4, Weights: [0.40471378 0.05913049 0.53615573]\n",
      "Epoch 265/1000, Cross-Entropy Loss: 16.068495783494168, Binary Loss: 4, Weights: [0.4048035  0.05913624 0.53606026]\n",
      "Epoch 266/1000, Cross-Entropy Loss: 16.069455194537714, Binary Loss: 4, Weights: [0.40489287 0.05914198 0.53596515]\n",
      "Epoch 267/1000, Cross-Entropy Loss: 16.070411064933506, Binary Loss: 4, Weights: [0.40498191 0.05914769 0.53587039]\n",
      "Epoch 268/1000, Cross-Entropy Loss: 16.071363421664852, Binary Loss: 4, Weights: [0.40507062 0.05915338 0.535776  ]\n",
      "Epoch 269/1000, Cross-Entropy Loss: 16.072312291797463, Binary Loss: 4, Weights: [0.405159   0.05915905 0.53568195]\n",
      "Epoch 270/1000, Cross-Entropy Loss: 16.073257702371315, Binary Loss: 4, Weights: [0.40524704 0.0591647  0.53558825]\n",
      "Epoch 271/1000, Cross-Entropy Loss: 16.07419968029918, Binary Loss: 4, Weights: [0.40533476 0.05917033 0.53549491]\n",
      "Epoch 272/1000, Cross-Entropy Loss: 16.07513825227373, Binary Loss: 4, Weights: [0.40542215 0.05917594 0.53540191]\n",
      "Epoch 273/1000, Cross-Entropy Loss: 16.07607344468451, Binary Loss: 4, Weights: [0.40550923 0.05918153 0.53530925]\n",
      "Epoch 274/1000, Cross-Entropy Loss: 16.07700528354586, Binary Loss: 4, Weights: [0.40559598 0.0591871  0.53521693]\n",
      "Epoch 275/1000, Cross-Entropy Loss: 16.07793379443635, Binary Loss: 4, Weights: [0.40568241 0.05919264 0.53512495]\n",
      "Epoch 276/1000, Cross-Entropy Loss: 16.078859002450226, Binary Loss: 4, Weights: [0.40576853 0.05919817 0.5350333 ]\n",
      "Epoch 277/1000, Cross-Entropy Loss: 16.07978093216058, Binary Loss: 4, Weights: [0.40585433 0.05920368 0.53494199]\n",
      "Epoch 278/1000, Cross-Entropy Loss: 16.080699607594198, Binary Loss: 4, Weights: [0.40593982 0.05920917 0.53485101]\n",
      "Epoch 279/1000, Cross-Entropy Loss: 16.08161505221738, Binary Loss: 4, Weights: [0.406025   0.05921464 0.53476035]\n",
      "Epoch 280/1000, Cross-Entropy Loss: 16.082527288932077, Binary Loss: 4, Weights: [0.40610988 0.05922009 0.53467003]\n",
      "Epoch 281/1000, Cross-Entropy Loss: 16.083436340081292, Binary Loss: 4, Weights: [0.40619445 0.05922553 0.53458003]\n",
      "Epoch 282/1000, Cross-Entropy Loss: 16.084342227462834, Binary Loss: 4, Weights: [0.40627871 0.05923094 0.53449035]\n",
      "Epoch 283/1000, Cross-Entropy Loss: 16.08524497235011, Binary Loss: 4, Weights: [0.40636268 0.05923634 0.53440099]\n",
      "Epoch 284/1000, Cross-Entropy Loss: 16.086144595518917, Binary Loss: 4, Weights: [0.40644634 0.05924171 0.53431195]\n",
      "Epoch 285/1000, Cross-Entropy Loss: 16.0870411172788, Binary Loss: 4, Weights: [0.40652971 0.05924707 0.53422322]\n",
      "Epoch 286/1000, Cross-Entropy Loss: 16.087934557508003, Binary Loss: 4, Weights: [0.40661278 0.05925242 0.53413481]\n",
      "Epoch 287/1000, Cross-Entropy Loss: 16.08882493569056, Binary Loss: 4, Weights: [0.40669555 0.05925774 0.53404671]\n",
      "Epoch 288/1000, Cross-Entropy Loss: 16.089712270954685, Binary Loss: 4, Weights: [0.40677804 0.05926304 0.53395891]\n",
      "Epoch 289/1000, Cross-Entropy Loss: 16.09059658211122, Binary Loss: 4, Weights: [0.40686024 0.05926833 0.53387143]\n",
      "Epoch 290/1000, Cross-Entropy Loss: 16.09147788769137, Binary Loss: 4, Weights: [0.40694215 0.0592736  0.53378425]\n",
      "Epoch 291/1000, Cross-Entropy Loss: 16.092356205982774, Binary Loss: 4, Weights: [0.40702377 0.05927885 0.53369738]\n",
      "Epoch 292/1000, Cross-Entropy Loss: 16.0932315550634, Binary Loss: 4, Weights: [0.40710511 0.05928409 0.5336108 ]\n",
      "Epoch 293/1000, Cross-Entropy Loss: 16.09410395283257, Binary Loss: 4, Weights: [0.40718617 0.0592893  0.53352453]\n",
      "Epoch 294/1000, Cross-Entropy Loss: 16.094973417038684, Binary Loss: 4, Weights: [0.40726695 0.0592945  0.53343855]\n",
      "Epoch 295/1000, Cross-Entropy Loss: 16.095839965303476, Binary Loss: 4, Weights: [0.40734745 0.05929969 0.53335287]\n",
      "Epoch 296/1000, Cross-Entropy Loss: 16.096703615142506, Binary Loss: 4, Weights: [0.40742767 0.05930485 0.53326748]\n",
      "Epoch 297/1000, Cross-Entropy Loss: 16.09756438398175, Binary Loss: 4, Weights: [0.40750762 0.05931    0.53318239]\n",
      "Epoch 298/1000, Cross-Entropy Loss: 16.09842228917056, Binary Loss: 4, Weights: [0.40758729 0.05931513 0.53309758]\n",
      "Epoch 299/1000, Cross-Entropy Loss: 16.09927734799087, Binary Loss: 4, Weights: [0.4076667  0.05932024 0.53301306]\n",
      "Epoch 300/1000, Cross-Entropy Loss: 16.10012957766299, Binary Loss: 4, Weights: [0.40774583 0.05932533 0.53292883]\n",
      "Epoch 301/1000, Cross-Entropy Loss: 16.10097899534824, Binary Loss: 4, Weights: [0.4078247  0.05933041 0.53284489]\n",
      "Epoch 302/1000, Cross-Entropy Loss: 16.101825618148702, Binary Loss: 4, Weights: [0.40790331 0.05933547 0.53276122]\n",
      "Epoch 303/1000, Cross-Entropy Loss: 16.102669463104473, Binary Loss: 4, Weights: [0.40798164 0.05934052 0.53267784]\n",
      "Epoch 304/1000, Cross-Entropy Loss: 16.10351054718884, Binary Loss: 4, Weights: [0.40805972 0.05934555 0.53259474]\n",
      "Epoch 305/1000, Cross-Entropy Loss: 16.104348887301725, Binary Loss: 4, Weights: [0.40813753 0.05935056 0.53251191]\n",
      "Epoch 306/1000, Cross-Entropy Loss: 16.105184500261835, Binary Loss: 4, Weights: [0.40821509 0.05935555 0.53242936]\n",
      "Epoch 307/1000, Cross-Entropy Loss: 16.10601740279785, Binary Loss: 4, Weights: [0.40829239 0.05936053 0.53234708]\n",
      "Epoch 308/1000, Cross-Entropy Loss: 16.10684761153914, Binary Loss: 4, Weights: [0.40836943 0.05936549 0.53226508]\n",
      "Epoch 309/1000, Cross-Entropy Loss: 16.107675143006226, Binary Loss: 4, Weights: [0.40844622 0.05937044 0.53218335]\n",
      "Epoch 310/1000, Cross-Entropy Loss: 16.10850001360138, Binary Loss: 4, Weights: [0.40852275 0.05937537 0.53210188]\n",
      "Epoch 311/1000, Cross-Entropy Loss: 16.10932223959961, Binary Loss: 4, Weights: [0.40859903 0.05938028 0.53202068]\n",
      "Epoch 312/1000, Cross-Entropy Loss: 16.110141837140326, Binary Loss: 4, Weights: [0.40867507 0.05938518 0.53193975]\n",
      "Epoch 313/1000, Cross-Entropy Loss: 16.110958822219782, Binary Loss: 4, Weights: [0.40875085 0.05939006 0.53185909]\n",
      "Epoch 314/1000, Cross-Entropy Loss: 16.111773210684536, Binary Loss: 4, Weights: [0.40882639 0.05939493 0.53177868]\n",
      "Epoch 315/1000, Cross-Entropy Loss: 16.112585018226007, Binary Loss: 4, Weights: [0.40890168 0.05939978 0.53169854]\n",
      "Epoch 316/1000, Cross-Entropy Loss: 16.113394260376186, Binary Loss: 4, Weights: [0.40897673 0.05940461 0.53161865]\n",
      "Epoch 317/1000, Cross-Entropy Loss: 16.114200952504568, Binary Loss: 4, Weights: [0.40905154 0.05940943 0.53153903]\n",
      "Epoch 318/1000, Cross-Entropy Loss: 16.11500510981625, Binary Loss: 4, Weights: [0.4091261  0.05941424 0.53145966]\n",
      "Epoch 319/1000, Cross-Entropy Loss: 16.115806747351204, Binary Loss: 4, Weights: [0.40920043 0.05941903 0.53138054]\n",
      "Epoch 320/1000, Cross-Entropy Loss: 16.116605879984697, Binary Loss: 4, Weights: [0.40927452 0.0594238  0.53130168]\n",
      "Epoch 321/1000, Cross-Entropy Loss: 16.117402522428588, Binary Loss: 4, Weights: [0.40934837 0.05942856 0.53122307]\n",
      "Epoch 322/1000, Cross-Entropy Loss: 16.11819668923368, Binary Loss: 4, Weights: [0.40942199 0.05943331 0.53114471]\n",
      "Epoch 323/1000, Cross-Entropy Loss: 16.11898839479279, Binary Loss: 4, Weights: [0.40949537 0.05943804 0.53106659]\n",
      "Epoch 324/1000, Cross-Entropy Loss: 16.11977765334445, Binary Loss: 4, Weights: [0.40956852 0.05944275 0.53098873]\n",
      "Epoch 325/1000, Cross-Entropy Loss: 16.12056447897722, Binary Loss: 4, Weights: [0.40964144 0.05944745 0.53091111]\n",
      "Epoch 326/1000, Cross-Entropy Loss: 16.121348885634323, Binary Loss: 4, Weights: [0.40971413 0.05945214 0.53083373]\n",
      "Epoch 327/1000, Cross-Entropy Loss: 16.122130887118605, Binary Loss: 4, Weights: [0.40978659 0.05945681 0.5307566 ]\n",
      "Epoch 328/1000, Cross-Entropy Loss: 16.122910497097585, Binary Loss: 4, Weights: [0.40985882 0.05946147 0.53067971]\n",
      "Epoch 329/1000, Cross-Entropy Loss: 16.123687729108575, Binary Loss: 4, Weights: [0.40993083 0.05946611 0.53060306]\n",
      "Epoch 330/1000, Cross-Entropy Loss: 16.124462596563713, Binary Loss: 4, Weights: [0.41000262 0.05947074 0.53052665]\n",
      "Epoch 331/1000, Cross-Entropy Loss: 16.125235112754822, Binary Loss: 4, Weights: [0.41007418 0.05947535 0.53045047]\n",
      "Epoch 332/1000, Cross-Entropy Loss: 16.126005290858004, Binary Loss: 4, Weights: [0.41014551 0.05947995 0.53037454]\n",
      "Epoch 333/1000, Cross-Entropy Loss: 16.126773143937996, Binary Loss: 4, Weights: [0.41021663 0.05948453 0.53029883]\n",
      "Epoch 334/1000, Cross-Entropy Loss: 16.127538684952054, Binary Loss: 4, Weights: [0.41028753 0.05948911 0.53022336]\n",
      "Epoch 335/1000, Cross-Entropy Loss: 16.128301926753565, Binary Loss: 4, Weights: [0.41035821 0.05949366 0.53014812]\n",
      "Epoch 336/1000, Cross-Entropy Loss: 16.129062882095127, Binary Loss: 4, Weights: [0.41042868 0.05949821 0.53007311]\n",
      "Epoch 337/1000, Cross-Entropy Loss: 16.129821563631324, Binary Loss: 4, Weights: [0.41049893 0.05950274 0.52999834]\n",
      "Epoch 338/1000, Cross-Entropy Loss: 16.130577983920972, Binary Loss: 4, Weights: [0.41056896 0.05950725 0.52992379]\n",
      "Epoch 339/1000, Cross-Entropy Loss: 16.131332155429007, Binary Loss: 4, Weights: [0.41063878 0.05951176 0.52984946]\n",
      "Epoch 340/1000, Cross-Entropy Loss: 16.132084090528032, Binary Loss: 4, Weights: [0.41070839 0.05951625 0.52977536]\n",
      "Epoch 341/1000, Cross-Entropy Loss: 16.13283380149944, Binary Loss: 4, Weights: [0.41077779 0.05952072 0.52970149]\n",
      "Epoch 342/1000, Cross-Entropy Loss: 16.133581300534257, Binary Loss: 4, Weights: [0.41084698 0.05952518 0.52962784]\n",
      "Epoch 343/1000, Cross-Entropy Loss: 16.13432659973371, Binary Loss: 4, Weights: [0.41091596 0.05952963 0.52955441]\n",
      "Epoch 344/1000, Cross-Entropy Loss: 16.135069711109587, Binary Loss: 4, Weights: [0.41098473 0.05953407 0.5294812 ]\n",
      "Epoch 345/1000, Cross-Entropy Loss: 16.135810646584353, Binary Loss: 4, Weights: [0.4110533  0.05953849 0.52940821]\n",
      "Epoch 346/1000, Cross-Entropy Loss: 16.13654941799119, Binary Loss: 4, Weights: [0.41112167 0.0595429  0.52933544]\n",
      "Epoch 347/1000, Cross-Entropy Loss: 16.13728603707388, Binary Loss: 4, Weights: [0.41118982 0.05954729 0.52926288]\n",
      "Epoch 348/1000, Cross-Entropy Loss: 16.138020515486712, Binary Loss: 4, Weights: [0.41125778 0.05955168 0.52919054]\n",
      "Epoch 349/1000, Cross-Entropy Loss: 16.138752864794288, Binary Loss: 4, Weights: [0.41132554 0.05955605 0.52911842]\n",
      "Epoch 350/1000, Cross-Entropy Loss: 16.13948309647137, Binary Loss: 4, Weights: [0.41139309 0.05956041 0.5290465 ]\n",
      "Epoch 351/1000, Cross-Entropy Loss: 16.140211221902835, Binary Loss: 4, Weights: [0.41146045 0.05956475 0.5289748 ]\n",
      "Epoch 352/1000, Cross-Entropy Loss: 16.140937252383587, Binary Loss: 4, Weights: [0.4115276  0.05956908 0.52890331]\n",
      "Epoch 353/1000, Cross-Entropy Loss: 16.14166119911868, Binary Loss: 4, Weights: [0.41159456 0.0595734  0.52883204]\n",
      "Epoch 354/1000, Cross-Entropy Loss: 16.14238307322343, Binary Loss: 4, Weights: [0.41166133 0.05957771 0.52876097]\n",
      "Epoch 355/1000, Cross-Entropy Loss: 16.143102885723742, Binary Loss: 4, Weights: [0.4117279 0.059582  0.5286901]\n",
      "Epoch 356/1000, Cross-Entropy Loss: 16.143820647556495, Binary Loss: 4, Weights: [0.41179427 0.05958628 0.52861945]\n",
      "Epoch 357/1000, Cross-Entropy Loss: 16.144536369570062, Binary Loss: 4, Weights: [0.41186045 0.05959055 0.528549  ]\n",
      "Epoch 358/1000, Cross-Entropy Loss: 16.145250062524973, Binary Loss: 4, Weights: [0.41192644 0.05959481 0.52847875]\n",
      "Epoch 359/1000, Cross-Entropy Loss: 16.145961737094638, Binary Loss: 4, Weights: [0.41199224 0.05959905 0.5284087 ]\n",
      "Epoch 360/1000, Cross-Entropy Loss: 16.14667140386623, Binary Loss: 4, Weights: [0.41205785 0.05960329 0.52833886]\n",
      "Epoch 361/1000, Cross-Entropy Loss: 16.14737907334161, Binary Loss: 4, Weights: [0.41212327 0.05960751 0.52826922]\n",
      "Epoch 362/1000, Cross-Entropy Loss: 16.14808475593837, Binary Loss: 4, Weights: [0.4121885  0.05961171 0.52819978]\n",
      "Epoch 363/1000, Cross-Entropy Loss: 16.148788461990936, Binary Loss: 4, Weights: [0.41225355 0.05961591 0.52813054]\n",
      "Epoch 364/1000, Cross-Entropy Loss: 16.14949020175166, Binary Loss: 4, Weights: [0.41231841 0.05962009 0.52806149]\n",
      "Epoch 365/1000, Cross-Entropy Loss: 16.150189985392025, Binary Loss: 4, Weights: [0.41238309 0.05962427 0.52799265]\n",
      "Epoch 366/1000, Cross-Entropy Loss: 16.15088782300384, Binary Loss: 4, Weights: [0.41244758 0.05962843 0.527924  ]\n",
      "Epoch 367/1000, Cross-Entropy Loss: 16.15158372460044, Binary Loss: 4, Weights: [0.41251189 0.05963258 0.52785554]\n",
      "Epoch 368/1000, Cross-Entropy Loss: 16.152277700117835, Binary Loss: 4, Weights: [0.41257601 0.05963671 0.52778727]\n",
      "Epoch 369/1000, Cross-Entropy Loss: 16.152969759415956, Binary Loss: 4, Weights: [0.41263996 0.05964084 0.5277192 ]\n",
      "Epoch 370/1000, Cross-Entropy Loss: 16.15365991227971, Binary Loss: 4, Weights: [0.41270372 0.05964495 0.52765133]\n",
      "Epoch 371/1000, Cross-Entropy Loss: 16.15434816842016, Binary Loss: 4, Weights: [0.41276731 0.05964905 0.52758364]\n",
      "Epoch 372/1000, Cross-Entropy Loss: 16.15503453747554, Binary Loss: 4, Weights: [0.41283072 0.05965315 0.52751614]\n",
      "Epoch 373/1000, Cross-Entropy Loss: 16.155719029012285, Binary Loss: 4, Weights: [0.41289395 0.05965722 0.52744883]\n",
      "Epoch 374/1000, Cross-Entropy Loss: 16.15640165252598, Binary Loss: 4, Weights: [0.412957   0.05966129 0.52738171]\n",
      "Epoch 375/1000, Cross-Entropy Loss: 16.15708241744226, Binary Loss: 4, Weights: [0.41301988 0.05966535 0.52731477]\n",
      "Epoch 376/1000, Cross-Entropy Loss: 16.157761333117683, Binary Loss: 4, Weights: [0.41308259 0.05966939 0.52724802]\n",
      "Epoch 377/1000, Cross-Entropy Loss: 16.15843840884049, Binary Loss: 4, Weights: [0.41314512 0.05967343 0.52718146]\n",
      "Epoch 378/1000, Cross-Entropy Loss: 16.15911365383136, Binary Loss: 4, Weights: [0.41320747 0.05967745 0.52711507]\n",
      "Epoch 379/1000, Cross-Entropy Loss: 16.15978707724412, Binary Loss: 4, Weights: [0.41326966 0.05968146 0.52704888]\n",
      "Epoch 380/1000, Cross-Entropy Loss: 16.160458688166372, Binary Loss: 4, Weights: [0.41333168 0.05968546 0.52698286]\n",
      "Epoch 381/1000, Cross-Entropy Loss: 16.16112849562013, Binary Loss: 4, Weights: [0.41339352 0.05968945 0.52691703]\n",
      "Epoch 382/1000, Cross-Entropy Loss: 16.161796508562368, Binary Loss: 4, Weights: [0.4134552  0.05969343 0.52685137]\n",
      "Epoch 383/1000, Cross-Entropy Loss: 16.162462735885576, Binary Loss: 4, Weights: [0.4135167 0.0596974 0.5267859]\n",
      "Epoch 384/1000, Cross-Entropy Loss: 16.163127186418304, Binary Loss: 4, Weights: [0.41357804 0.05970136 0.5267206 ]\n",
      "Epoch 385/1000, Cross-Entropy Loss: 16.16378986892564, Binary Loss: 4, Weights: [0.41363922 0.0597053  0.52665548]\n",
      "Epoch 386/1000, Cross-Entropy Loss: 16.164450792109708, Binary Loss: 4, Weights: [0.41370022 0.05970924 0.52659054]\n",
      "Epoch 387/1000, Cross-Entropy Loss: 16.165109964610174, Binary Loss: 4, Weights: [0.41376107 0.05971317 0.52652577]\n",
      "Epoch 388/1000, Cross-Entropy Loss: 16.165767395004696, Binary Loss: 4, Weights: [0.41382174 0.05971708 0.52646118]\n",
      "Epoch 389/1000, Cross-Entropy Loss: 16.166423091809403, Binary Loss: 4, Weights: [0.41388226 0.05972098 0.52639676]\n",
      "Epoch 390/1000, Cross-Entropy Loss: 16.167077063479404, Binary Loss: 4, Weights: [0.41394261 0.05972488 0.52633251]\n",
      "Epoch 391/1000, Cross-Entropy Loss: 16.167729318409226, Binary Loss: 4, Weights: [0.4140028  0.05972876 0.52626844]\n",
      "Epoch 392/1000, Cross-Entropy Loss: 16.168379864933343, Binary Loss: 4, Weights: [0.41406283 0.05973263 0.52620454]\n",
      "Epoch 393/1000, Cross-Entropy Loss: 16.16902871132667, Binary Loss: 4, Weights: [0.4141227  0.05973649 0.52614081]\n",
      "Epoch 394/1000, Cross-Entropy Loss: 16.169675865805033, Binary Loss: 4, Weights: [0.41418241 0.05974035 0.52607725]\n",
      "Epoch 395/1000, Cross-Entropy Loss: 16.170321336525735, Binary Loss: 4, Weights: [0.41424196 0.05974419 0.52601386]\n",
      "Epoch 396/1000, Cross-Entropy Loss: 16.170965131588037, Binary Loss: 4, Weights: [0.41430135 0.05974802 0.52595063]\n",
      "Epoch 397/1000, Cross-Entropy Loss: 16.171607259033724, Binary Loss: 4, Weights: [0.41436059 0.05975184 0.52588757]\n",
      "Epoch 398/1000, Cross-Entropy Loss: 16.172247726847615, Binary Loss: 4, Weights: [0.41441966 0.05975565 0.52582468]\n",
      "Epoch 399/1000, Cross-Entropy Loss: 16.17288654295812, Binary Loss: 4, Weights: [0.41447859 0.05975945 0.52576196]\n",
      "Epoch 400/1000, Cross-Entropy Loss: 16.173523715237774, Binary Loss: 4, Weights: [0.41453736 0.05976324 0.5256994 ]\n",
      "Epoch 401/1000, Cross-Entropy Loss: 16.174159251503795, Binary Loss: 4, Weights: [0.41459597 0.05976702 0.525637  ]\n",
      "Epoch 402/1000, Cross-Entropy Loss: 16.174793159518657, Binary Loss: 4, Weights: [0.41465443 0.05977079 0.52557477]\n",
      "Epoch 403/1000, Cross-Entropy Loss: 16.175425446990573, Binary Loss: 4, Weights: [0.41471274 0.05977456 0.5255127 ]\n",
      "Epoch 404/1000, Cross-Entropy Loss: 16.176056121574092, Binary Loss: 4, Weights: [0.4147709  0.05977831 0.52545079]\n",
      "Epoch 405/1000, Cross-Entropy Loss: 16.17668519087065, Binary Loss: 4, Weights: [0.41482891 0.05978205 0.52538905]\n",
      "Epoch 406/1000, Cross-Entropy Loss: 16.177312662429028, Binary Loss: 4, Weights: [0.41488676 0.05978578 0.52532746]\n",
      "Epoch 407/1000, Cross-Entropy Loss: 16.177938543745974, Binary Loss: 4, Weights: [0.41494447 0.0597895  0.52526603]\n",
      "Epoch 408/1000, Cross-Entropy Loss: 16.17856284226665, Binary Loss: 4, Weights: [0.41500202 0.05979321 0.52520476]\n",
      "Epoch 409/1000, Cross-Entropy Loss: 16.179185565385186, Binary Loss: 4, Weights: [0.41505943 0.05979692 0.52514365]\n",
      "Epoch 410/1000, Cross-Entropy Loss: 16.179806720445136, Binary Loss: 4, Weights: [0.41511669 0.05980061 0.5250827 ]\n",
      "Epoch 411/1000, Cross-Entropy Loss: 16.18042631474, Binary Loss: 4, Weights: [0.41517381 0.05980429 0.5250219 ]\n",
      "Epoch 412/1000, Cross-Entropy Loss: 16.181044355513695, Binary Loss: 4, Weights: [0.41523078 0.05980797 0.52496126]\n",
      "Epoch 413/1000, Cross-Entropy Loss: 16.181660849961013, Binary Loss: 4, Weights: [0.4152876  0.05981163 0.52490077]\n",
      "Epoch 414/1000, Cross-Entropy Loss: 16.18227580522809, Binary Loss: 4, Weights: [0.41534428 0.05981529 0.52484044]\n",
      "Epoch 415/1000, Cross-Entropy Loss: 16.18288922841283, Binary Loss: 4, Weights: [0.41540081 0.05981893 0.52478026]\n",
      "Epoch 416/1000, Cross-Entropy Loss: 16.18350112656538, Binary Loss: 4, Weights: [0.4154572  0.05982257 0.52472023]\n",
      "Epoch 417/1000, Cross-Entropy Loss: 16.18411150668851, Binary Loss: 4, Weights: [0.41551345 0.0598262  0.52466035]\n",
      "Epoch 418/1000, Cross-Entropy Loss: 16.184720375738074, Binary Loss: 4, Weights: [0.41556955 0.05982982 0.52460063]\n",
      "Epoch 419/1000, Cross-Entropy Loss: 16.1853277406234, Binary Loss: 4, Weights: [0.41562552 0.05983343 0.52454106]\n",
      "Epoch 420/1000, Cross-Entropy Loss: 16.185933608207705, Binary Loss: 4, Weights: [0.41568134 0.05983703 0.52448163]\n",
      "Epoch 421/1000, Cross-Entropy Loss: 16.18653798530846, Binary Loss: 4, Weights: [0.41573702 0.05984062 0.52442236]\n",
      "Epoch 422/1000, Cross-Entropy Loss: 16.187140878697836, Binary Loss: 4, Weights: [0.41579257 0.0598442  0.52436323]\n",
      "Epoch 423/1000, Cross-Entropy Loss: 16.187742295103032, Binary Loss: 4, Weights: [0.41584797 0.05984777 0.52430426]\n",
      "Epoch 424/1000, Cross-Entropy Loss: 16.188342241206698, Binary Loss: 4, Weights: [0.41590324 0.05985133 0.52424543]\n",
      "Epoch 425/1000, Cross-Entropy Loss: 16.188940723647303, Binary Loss: 4, Weights: [0.41595837 0.05985489 0.52418674]\n",
      "Epoch 426/1000, Cross-Entropy Loss: 16.189537749019493, Binary Loss: 4, Weights: [0.41601336 0.05985844 0.52412821]\n",
      "Epoch 427/1000, Cross-Entropy Loss: 16.190133323874473, Binary Loss: 4, Weights: [0.41606821 0.05986197 0.52406981]\n",
      "Epoch 428/1000, Cross-Entropy Loss: 16.190727454720374, Binary Loss: 4, Weights: [0.41612293 0.0598655  0.52401156]\n",
      "Epoch 429/1000, Cross-Entropy Loss: 16.191320148022623, Binary Loss: 4, Weights: [0.41617752 0.05986902 0.52395346]\n",
      "Epoch 430/1000, Cross-Entropy Loss: 16.1919114102043, Binary Loss: 4, Weights: [0.41623197 0.05987253 0.5238955 ]\n",
      "Epoch 431/1000, Cross-Entropy Loss: 16.19250124764649, Binary Loss: 4, Weights: [0.41628629 0.05987603 0.52383768]\n",
      "Epoch 432/1000, Cross-Entropy Loss: 16.19308966668866, Binary Loss: 4, Weights: [0.41634047 0.05987953 0.52378   ]\n",
      "Epoch 433/1000, Cross-Entropy Loss: 16.193676673629014, Binary Loss: 4, Weights: [0.41639452 0.05988301 0.52372247]\n",
      "Epoch 434/1000, Cross-Entropy Loss: 16.194262274724824, Binary Loss: 4, Weights: [0.41644844 0.05988649 0.52366507]\n",
      "Epoch 435/1000, Cross-Entropy Loss: 16.19484647619279, Binary Loss: 4, Weights: [0.41650223 0.05988995 0.52360782]\n",
      "Epoch 436/1000, Cross-Entropy Loss: 16.19542928420944, Binary Loss: 4, Weights: [0.41655588 0.05989341 0.5235507 ]\n",
      "Epoch 437/1000, Cross-Entropy Loss: 16.19601070491138, Binary Loss: 4, Weights: [0.41660941 0.05989686 0.52349373]\n",
      "Epoch 438/1000, Cross-Entropy Loss: 16.19659074439574, Binary Loss: 4, Weights: [0.41666281 0.05990031 0.52343689]\n",
      "Epoch 439/1000, Cross-Entropy Loss: 16.19716940872042, Binary Loss: 4, Weights: [0.41671607 0.05990374 0.52338019]\n",
      "Epoch 440/1000, Cross-Entropy Loss: 16.197746703904514, Binary Loss: 4, Weights: [0.41676921 0.05990717 0.52332362]\n",
      "Epoch 441/1000, Cross-Entropy Loss: 16.198322635928584, Binary Loss: 4, Weights: [0.41682222 0.05991058 0.52326719]\n",
      "Epoch 442/1000, Cross-Entropy Loss: 16.198897210735023, Binary Loss: 4, Weights: [0.41687511 0.05991399 0.5232109 ]\n",
      "Epoch 443/1000, Cross-Entropy Loss: 16.199470434228363, Binary Loss: 4, Weights: [0.41692786 0.05991739 0.52315474]\n",
      "Epoch 444/1000, Cross-Entropy Loss: 16.200042312275617, Binary Loss: 4, Weights: [0.4169805  0.05992078 0.52309872]\n",
      "Epoch 445/1000, Cross-Entropy Loss: 16.20061285070659, Binary Loss: 4, Weights: [0.417033   0.05992417 0.52304283]\n",
      "Epoch 446/1000, Cross-Entropy Loss: 16.20118205531418, Binary Loss: 4, Weights: [0.41708538 0.05992754 0.52298708]\n",
      "Epoch 447/1000, Cross-Entropy Loss: 16.20174993185472, Binary Loss: 4, Weights: [0.41713763 0.05993091 0.52293145]\n",
      "Epoch 448/1000, Cross-Entropy Loss: 16.202316486048282, Binary Loss: 4, Weights: [0.41718977 0.05993427 0.52287596]\n",
      "Epoch 449/1000, Cross-Entropy Loss: 16.20288172357895, Binary Loss: 4, Weights: [0.41724177 0.05993762 0.5228206 ]\n",
      "Epoch 450/1000, Cross-Entropy Loss: 16.20344565009516, Binary Loss: 4, Weights: [0.41729366 0.05994097 0.52276538]\n",
      "Epoch 451/1000, Cross-Entropy Loss: 16.204008271209954, Binary Loss: 4, Weights: [0.41734542 0.0599443  0.52271028]\n",
      "Epoch 452/1000, Cross-Entropy Loss: 16.204569592501336, Binary Loss: 4, Weights: [0.41739706 0.05994763 0.52265531]\n",
      "Epoch 453/1000, Cross-Entropy Loss: 16.205129619512494, Binary Loss: 4, Weights: [0.41744858 0.05995095 0.52260047]\n",
      "Epoch 454/1000, Cross-Entropy Loss: 16.20568835775214, Binary Loss: 4, Weights: [0.41749998 0.05995426 0.52254576]\n",
      "Epoch 455/1000, Cross-Entropy Loss: 16.206245812694746, Binary Loss: 4, Weights: [0.41755125 0.05995757 0.52249118]\n",
      "Epoch 456/1000, Cross-Entropy Loss: 16.206801989780864, Binary Loss: 4, Weights: [0.41760241 0.05996086 0.52243673]\n",
      "Epoch 457/1000, Cross-Entropy Loss: 16.207356894417394, Binary Loss: 4, Weights: [0.41765345 0.05996415 0.5223824 ]\n",
      "Epoch 458/1000, Cross-Entropy Loss: 16.207910531977813, Binary Loss: 4, Weights: [0.41770437 0.05996743 0.5223282 ]\n",
      "Epoch 459/1000, Cross-Entropy Loss: 16.208462907802527, Binary Loss: 4, Weights: [0.41775517 0.0599707  0.52227413]\n",
      "Epoch 460/1000, Cross-Entropy Loss: 16.209014027199068, Binary Loss: 4, Weights: [0.41780585 0.05997397 0.52222018]\n",
      "Epoch 461/1000, Cross-Entropy Loss: 16.209563895442383, Binary Loss: 4, Weights: [0.41785642 0.05997723 0.52216635]\n",
      "Epoch 462/1000, Cross-Entropy Loss: 16.210112517775105, Binary Loss: 4, Weights: [0.41790687 0.05998048 0.52211265]\n",
      "Epoch 463/1000, Cross-Entropy Loss: 16.210659899407812, Binary Loss: 4, Weights: [0.4179572  0.05998372 0.52205908]\n",
      "Epoch 464/1000, Cross-Entropy Loss: 16.211206045519262, Binary Loss: 4, Weights: [0.41800742 0.05998696 0.52200563]\n",
      "Epoch 465/1000, Cross-Entropy Loss: 16.211750961256683, Binary Loss: 4, Weights: [0.41805752 0.05999018 0.5219523 ]\n",
      "Epoch 466/1000, Cross-Entropy Loss: 16.212294651736, Binary Loss: 4, Weights: [0.41810751 0.0599934  0.52189909]\n",
      "Epoch 467/1000, Cross-Entropy Loss: 16.21283712204208, Binary Loss: 4, Weights: [0.41815738 0.05999662 0.521846  ]\n",
      "Epoch 468/1000, Cross-Entropy Loss: 16.21337837722901, Binary Loss: 4, Weights: [0.41820714 0.05999982 0.52179304]\n",
      "Epoch 469/1000, Cross-Entropy Loss: 16.21391842232031, Binary Loss: 4, Weights: [0.41825679 0.06000302 0.5217402 ]\n",
      "Epoch 470/1000, Cross-Entropy Loss: 16.214457262309203, Binary Loss: 4, Weights: [0.41830632 0.06000621 0.52168747]\n",
      "Epoch 471/1000, Cross-Entropy Loss: 16.214994902158843, Binary Loss: 4, Weights: [0.41835574 0.06000939 0.52163487]\n",
      "Epoch 472/1000, Cross-Entropy Loss: 16.215531346802557, Binary Loss: 4, Weights: [0.41840505 0.06001257 0.52158239]\n",
      "Epoch 473/1000, Cross-Entropy Loss: 16.21606660114407, Binary Loss: 4, Weights: [0.41845424 0.06001574 0.52153002]\n",
      "Epoch 474/1000, Cross-Entropy Loss: 16.21660067005776, Binary Loss: 4, Weights: [0.41850333 0.0600189  0.52147777]\n",
      "Epoch 475/1000, Cross-Entropy Loss: 16.217133558388877, Binary Loss: 4, Weights: [0.4185523  0.06002205 0.52142565]\n",
      "Epoch 476/1000, Cross-Entropy Loss: 16.217665270953773, Binary Loss: 4, Weights: [0.41860117 0.0600252  0.52137363]\n",
      "Epoch 477/1000, Cross-Entropy Loss: 16.21819581254016, Binary Loss: 4, Weights: [0.41864993 0.06002834 0.52132174]\n",
      "Epoch 478/1000, Cross-Entropy Loss: 16.218725187907257, Binary Loss: 4, Weights: [0.41869857 0.06003147 0.52126996]\n",
      "Epoch 479/1000, Cross-Entropy Loss: 16.219253401786112, Binary Loss: 4, Weights: [0.41874711 0.06003459 0.5212183 ]\n",
      "Epoch 480/1000, Cross-Entropy Loss: 16.219780458879754, Binary Loss: 4, Weights: [0.41879554 0.06003771 0.52116675]\n",
      "Epoch 481/1000, Cross-Entropy Loss: 16.22030636386344, Binary Loss: 4, Weights: [0.41884386 0.06004082 0.52111532]\n",
      "Epoch 482/1000, Cross-Entropy Loss: 16.22083112138487, Binary Loss: 4, Weights: [0.41889207 0.06004393 0.521064  ]\n",
      "Epoch 483/1000, Cross-Entropy Loss: 16.22135473606439, Binary Loss: 4, Weights: [0.41894018 0.06004703 0.52101279]\n",
      "Epoch 484/1000, Cross-Entropy Loss: 16.22187721249521, Binary Loss: 4, Weights: [0.41898818 0.06005012 0.5209617 ]\n",
      "Epoch 485/1000, Cross-Entropy Loss: 16.222398555243633, Binary Loss: 4, Weights: [0.41903608 0.0600532  0.52091072]\n",
      "Epoch 486/1000, Cross-Entropy Loss: 16.222918768849215, Binary Loss: 4, Weights: [0.41908386 0.06005628 0.52085986]\n",
      "Epoch 487/1000, Cross-Entropy Loss: 16.223437857825036, Binary Loss: 4, Weights: [0.41913155 0.06005935 0.52080911]\n",
      "Epoch 488/1000, Cross-Entropy Loss: 16.223955826657853, Binary Loss: 4, Weights: [0.41917913 0.06006241 0.52075847]\n",
      "Epoch 489/1000, Cross-Entropy Loss: 16.224472679808315, Binary Loss: 4, Weights: [0.4192266  0.06006546 0.52070794]\n",
      "Epoch 490/1000, Cross-Entropy Loss: 16.22498842171117, Binary Loss: 4, Weights: [0.41927397 0.06006851 0.52065752]\n",
      "Epoch 491/1000, Cross-Entropy Loss: 16.225503056775477, Binary Loss: 4, Weights: [0.41932124 0.06007156 0.52060721]\n",
      "Epoch 492/1000, Cross-Entropy Loss: 16.226016589384752, Binary Loss: 4, Weights: [0.4193684  0.06007459 0.52055701]\n",
      "Epoch 493/1000, Cross-Entropy Loss: 16.226529023897225, Binary Loss: 4, Weights: [0.41941546 0.06007762 0.52050692]\n",
      "Epoch 494/1000, Cross-Entropy Loss: 16.227040364645983, Binary Loss: 4, Weights: [0.41946242 0.06008064 0.52045694]\n",
      "Epoch 495/1000, Cross-Entropy Loss: 16.22755061593919, Binary Loss: 4, Weights: [0.41950928 0.06008366 0.52040706]\n",
      "Epoch 496/1000, Cross-Entropy Loss: 16.228059782060264, Binary Loss: 4, Weights: [0.41955603 0.06008667 0.5203573 ]\n",
      "Epoch 497/1000, Cross-Entropy Loss: 16.22856786726806, Binary Loss: 4, Weights: [0.41960269 0.06008967 0.52030764]\n",
      "Epoch 498/1000, Cross-Entropy Loss: 16.22907487579706, Binary Loss: 4, Weights: [0.41964924 0.06009266 0.52025809]\n",
      "Epoch 499/1000, Cross-Entropy Loss: 16.229580811857566, Binary Loss: 4, Weights: [0.41969569 0.06009565 0.52020865]\n",
      "Epoch 500/1000, Cross-Entropy Loss: 16.230085679635867, Binary Loss: 4, Weights: [0.41974205 0.06009864 0.52015932]\n",
      "Epoch 501/1000, Cross-Entropy Loss: 16.230589483294406, Binary Loss: 4, Weights: [0.4197883  0.06010161 0.52011009]\n",
      "Epoch 502/1000, Cross-Entropy Loss: 16.23109222697202, Binary Loss: 4, Weights: [0.41983446 0.06010458 0.52006096]\n",
      "Epoch 503/1000, Cross-Entropy Loss: 16.231593914784032, Binary Loss: 4, Weights: [0.41988051 0.06010755 0.52001194]\n",
      "Epoch 504/1000, Cross-Entropy Loss: 16.23209455082248, Binary Loss: 4, Weights: [0.41992647 0.0601105  0.51996303]\n",
      "Epoch 505/1000, Cross-Entropy Loss: 16.232594139156287, Binary Loss: 4, Weights: [0.41997233 0.06011345 0.51991422]\n",
      "Epoch 506/1000, Cross-Entropy Loss: 16.23309268383142, Binary Loss: 4, Weights: [0.42001809 0.0601164  0.51986551]\n",
      "Epoch 507/1000, Cross-Entropy Loss: 16.233590188871055, Binary Loss: 4, Weights: [0.42006376 0.06011933 0.51981691]\n",
      "Epoch 508/1000, Cross-Entropy Loss: 16.234086658275764, Binary Loss: 4, Weights: [0.42010932 0.06012227 0.51976841]\n",
      "Epoch 509/1000, Cross-Entropy Loss: 16.23458209602367, Binary Loss: 4, Weights: [0.4201548  0.06012519 0.51972001]\n",
      "Epoch 510/1000, Cross-Entropy Loss: 16.235076506070623, Binary Loss: 4, Weights: [0.42020017 0.06012811 0.51967172]\n",
      "Epoch 511/1000, Cross-Entropy Loss: 16.23556989235034, Binary Loss: 4, Weights: [0.42024545 0.06013102 0.51962353]\n",
      "Epoch 512/1000, Cross-Entropy Loss: 16.2360622587746, Binary Loss: 4, Weights: [0.42029064 0.06013393 0.51957544]\n",
      "Epoch 513/1000, Cross-Entropy Loss: 16.236553609233383, Binary Loss: 4, Weights: [0.42033573 0.06013683 0.51952745]\n",
      "Epoch 514/1000, Cross-Entropy Loss: 16.237043947595044, Binary Loss: 4, Weights: [0.42038072 0.06013972 0.51947956]\n",
      "Epoch 515/1000, Cross-Entropy Loss: 16.23753327770645, Binary Loss: 4, Weights: [0.42042562 0.06014261 0.51943177]\n",
      "Epoch 516/1000, Cross-Entropy Loss: 16.238021603393165, Binary Loss: 4, Weights: [0.42047043 0.06014549 0.51938408]\n",
      "Epoch 517/1000, Cross-Entropy Loss: 16.23850892845958, Binary Loss: 4, Weights: [0.42051514 0.06014837 0.51933649]\n",
      "Epoch 518/1000, Cross-Entropy Loss: 16.23899525668908, Binary Loss: 4, Weights: [0.42055977 0.06015123 0.519289  ]\n",
      "Epoch 519/1000, Cross-Entropy Loss: 16.2394805918442, Binary Loss: 4, Weights: [0.42060429 0.0601541  0.51924161]\n",
      "Epoch 520/1000, Cross-Entropy Loss: 16.239964937666773, Binary Loss: 4, Weights: [0.42064873 0.06015696 0.51919432]\n",
      "Epoch 521/1000, Cross-Entropy Loss: 16.24044829787806, Binary Loss: 4, Weights: [0.42069307 0.06015981 0.51914712]\n",
      "Epoch 522/1000, Cross-Entropy Loss: 16.24093067617893, Binary Loss: 4, Weights: [0.42073732 0.06016265 0.51910002]\n",
      "Epoch 523/1000, Cross-Entropy Loss: 16.24141207624999, Binary Loss: 4, Weights: [0.42078148 0.06016549 0.51905303]\n",
      "Epoch 524/1000, Cross-Entropy Loss: 16.241892501751735, Binary Loss: 4, Weights: [0.42082555 0.06016832 0.51900612]\n",
      "Epoch 525/1000, Cross-Entropy Loss: 16.24237195632468, Binary Loss: 4, Weights: [0.42086953 0.06017115 0.51895932]\n",
      "Epoch 526/1000, Cross-Entropy Loss: 16.242850443589525, Binary Loss: 4, Weights: [0.42091342 0.06017397 0.51891261]\n",
      "Epoch 527/1000, Cross-Entropy Loss: 16.243327967147284, Binary Loss: 4, Weights: [0.42095722 0.06017679 0.51886599]\n",
      "Epoch 528/1000, Cross-Entropy Loss: 16.243804530579418, Binary Loss: 4, Weights: [0.42100093 0.0601796  0.51881947]\n",
      "Epoch 529/1000, Cross-Entropy Loss: 16.244280137447998, Binary Loss: 4, Weights: [0.42104455 0.0601824  0.51877305]\n",
      "Epoch 530/1000, Cross-Entropy Loss: 16.244754791295822, Binary Loss: 4, Weights: [0.42108808 0.0601852  0.51872672]\n",
      "Epoch 531/1000, Cross-Entropy Loss: 16.245228495646547, Binary Loss: 4, Weights: [0.42113152 0.06018799 0.51868049]\n",
      "Epoch 532/1000, Cross-Entropy Loss: 16.24570125400485, Binary Loss: 4, Weights: [0.42117487 0.06019078 0.51863435]\n",
      "Epoch 533/1000, Cross-Entropy Loss: 16.246173069856543, Binary Loss: 4, Weights: [0.42121814 0.06019356 0.5185883 ]\n",
      "Epoch 534/1000, Cross-Entropy Loss: 16.24664394666872, Binary Loss: 4, Weights: [0.42126132 0.06019633 0.51854235]\n",
      "Epoch 535/1000, Cross-Entropy Loss: 16.24711388788987, Binary Loss: 4, Weights: [0.42130441 0.0601991  0.51849649]\n",
      "Epoch 536/1000, Cross-Entropy Loss: 16.24758289695001, Binary Loss: 4, Weights: [0.42134741 0.06020186 0.51845072]\n",
      "Epoch 537/1000, Cross-Entropy Loss: 16.248050977260853, Binary Loss: 4, Weights: [0.42139033 0.06020462 0.51840505]\n",
      "Epoch 538/1000, Cross-Entropy Loss: 16.248518132215878, Binary Loss: 4, Weights: [0.42143316 0.06020737 0.51835946]\n",
      "Epoch 539/1000, Cross-Entropy Loss: 16.248984365190495, Binary Loss: 4, Weights: [0.42147591 0.06021012 0.51831397]\n",
      "Epoch 540/1000, Cross-Entropy Loss: 16.249449679542174, Binary Loss: 4, Weights: [0.42151857 0.06021286 0.51826857]\n",
      "Epoch 541/1000, Cross-Entropy Loss: 16.249914078610555, Binary Loss: 4, Weights: [0.42156114 0.0602156  0.51822326]\n",
      "Epoch 542/1000, Cross-Entropy Loss: 16.25037756571756, Binary Loss: 4, Weights: [0.42160363 0.06021833 0.51817805]\n",
      "Epoch 543/1000, Cross-Entropy Loss: 16.250840144167547, Binary Loss: 4, Weights: [0.42164603 0.06022105 0.51813292]\n",
      "Epoch 544/1000, Cross-Entropy Loss: 16.251301817247416, Binary Loss: 4, Weights: [0.42168835 0.06022377 0.51808788]\n",
      "Epoch 545/1000, Cross-Entropy Loss: 16.251762588226743, Binary Loss: 4, Weights: [0.42173059 0.06022648 0.51804293]\n",
      "Epoch 546/1000, Cross-Entropy Loss: 16.252222460357856, Binary Loss: 4, Weights: [0.42177274 0.06022919 0.51799807]\n",
      "Epoch 547/1000, Cross-Entropy Loss: 16.25268143687603, Binary Loss: 4, Weights: [0.42181481 0.06023189 0.5179533 ]\n",
      "Epoch 548/1000, Cross-Entropy Loss: 16.25313952099952, Binary Loss: 4, Weights: [0.42185679 0.06023459 0.51790862]\n",
      "Epoch 549/1000, Cross-Entropy Loss: 16.253596715929756, Binary Loss: 4, Weights: [0.42189869 0.06023728 0.51786403]\n",
      "Epoch 550/1000, Cross-Entropy Loss: 16.254053024851387, Binary Loss: 4, Weights: [0.42194051 0.06023997 0.51781952]\n",
      "Epoch 551/1000, Cross-Entropy Loss: 16.254508450932462, Binary Loss: 4, Weights: [0.42198225 0.06024265 0.51777511]\n",
      "Epoch 552/1000, Cross-Entropy Loss: 16.2549629973245, Binary Loss: 4, Weights: [0.4220239  0.06024532 0.51773078]\n",
      "Epoch 553/1000, Cross-Entropy Loss: 16.25541666716262, Binary Loss: 4, Weights: [0.42206548 0.06024799 0.51768653]\n",
      "Epoch 554/1000, Cross-Entropy Loss: 16.25586946356564, Binary Loss: 4, Weights: [0.42210697 0.06025065 0.51764238]\n",
      "Epoch 555/1000, Cross-Entropy Loss: 16.256321389636213, Binary Loss: 4, Weights: [0.42214838 0.06025331 0.51759831]\n",
      "Epoch 556/1000, Cross-Entropy Loss: 16.2567724484609, Binary Loss: 4, Weights: [0.42218971 0.06025597 0.51755432]\n",
      "Epoch 557/1000, Cross-Entropy Loss: 16.257222643110328, Binary Loss: 4, Weights: [0.42223096 0.06025862 0.51751043]\n",
      "Epoch 558/1000, Cross-Entropy Loss: 16.257671976639244, Binary Loss: 4, Weights: [0.42227213 0.06026126 0.51746661]\n",
      "Epoch 559/1000, Cross-Entropy Loss: 16.25812045208667, Binary Loss: 4, Weights: [0.42231322 0.0602639  0.51742289]\n",
      "Epoch 560/1000, Cross-Entropy Loss: 16.258568072475978, Binary Loss: 4, Weights: [0.42235423 0.06026653 0.51737925]\n",
      "Epoch 561/1000, Cross-Entropy Loss: 16.259014840815006, Binary Loss: 4, Weights: [0.42239516 0.06026916 0.51733569]\n",
      "Epoch 562/1000, Cross-Entropy Loss: 16.259460760096164, Binary Loss: 4, Weights: [0.42243601 0.06027178 0.51729221]\n",
      "Epoch 563/1000, Cross-Entropy Loss: 16.25990583329655, Binary Loss: 4, Weights: [0.42247678 0.0602744  0.51724883]\n",
      "Epoch 564/1000, Cross-Entropy Loss: 16.260350063378016, Binary Loss: 4, Weights: [0.42251747 0.06027701 0.51720552]\n",
      "Epoch 565/1000, Cross-Entropy Loss: 16.260793453287313, Binary Loss: 4, Weights: [0.42255809 0.06027961 0.5171623 ]\n",
      "Epoch 566/1000, Cross-Entropy Loss: 16.261236005956157, Binary Loss: 4, Weights: [0.42259862 0.06028222 0.51711916]\n",
      "Epoch 567/1000, Cross-Entropy Loss: 16.261677724301357, Binary Loss: 4, Weights: [0.42263908 0.06028481 0.5170761 ]\n",
      "Epoch 568/1000, Cross-Entropy Loss: 16.262118611224906, Binary Loss: 4, Weights: [0.42267947 0.0602874  0.51703313]\n",
      "Epoch 569/1000, Cross-Entropy Loss: 16.262558669614062, Binary Loss: 4, Weights: [0.42271977 0.06028999 0.51699024]\n",
      "Epoch 570/1000, Cross-Entropy Loss: 16.262997902341475, Binary Loss: 4, Weights: [0.42276    0.06029257 0.51694743]\n",
      "Epoch 571/1000, Cross-Entropy Loss: 16.26343631226525, Binary Loss: 4, Weights: [0.42280015 0.06029515 0.5169047 ]\n",
      "Epoch 572/1000, Cross-Entropy Loss: 16.263873902229093, Binary Loss: 4, Weights: [0.42284022 0.06029772 0.51686206]\n",
      "Epoch 573/1000, Cross-Entropy Loss: 16.264310675062333, Binary Loss: 4, Weights: [0.42288022 0.06030029 0.51681949]\n",
      "Epoch 574/1000, Cross-Entropy Loss: 16.264746633580117, Binary Loss: 4, Weights: [0.42292015 0.06030285 0.51677701]\n",
      "Epoch 575/1000, Cross-Entropy Loss: 16.265181780583394, Binary Loss: 4, Weights: [0.42295999 0.0603054  0.5167346 ]\n",
      "Epoch 576/1000, Cross-Entropy Loss: 16.265616118859086, Binary Loss: 4, Weights: [0.42299977 0.06030796 0.51669228]\n",
      "Epoch 577/1000, Cross-Entropy Loss: 16.266049651180154, Binary Loss: 4, Weights: [0.42303946 0.0603105  0.51665004]\n",
      "Epoch 578/1000, Cross-Entropy Loss: 16.266482380305682, Binary Loss: 4, Weights: [0.42307908 0.06031304 0.51660787]\n",
      "Epoch 579/1000, Cross-Entropy Loss: 16.266914308981, Binary Loss: 4, Weights: [0.42311863 0.06031558 0.51656579]\n",
      "Epoch 580/1000, Cross-Entropy Loss: 16.26734543993772, Binary Loss: 4, Weights: [0.42315811 0.06031811 0.51652378]\n",
      "Epoch 581/1000, Cross-Entropy Loss: 16.26777577589387, Binary Loss: 4, Weights: [0.4231975  0.06032064 0.51648186]\n",
      "Epoch 582/1000, Cross-Entropy Loss: 16.26820531955397, Binary Loss: 4, Weights: [0.42323683 0.06032316 0.51644001]\n",
      "Epoch 583/1000, Cross-Entropy Loss: 16.26863407360912, Binary Loss: 4, Weights: [0.42327608 0.06032568 0.51639824]\n",
      "Epoch 584/1000, Cross-Entropy Loss: 16.2690620407371, Binary Loss: 4, Weights: [0.42331526 0.06032819 0.51635655]\n",
      "Epoch 585/1000, Cross-Entropy Loss: 16.269489223602417, Binary Loss: 4, Weights: [0.42335437 0.0603307  0.51631493]\n",
      "Epoch 586/1000, Cross-Entropy Loss: 16.26991562485642, Binary Loss: 4, Weights: [0.4233934 0.0603332 0.5162734]\n",
      "Epoch 587/1000, Cross-Entropy Loss: 16.2703412471374, Binary Loss: 4, Weights: [0.42343236 0.0603357  0.51623194]\n",
      "Epoch 588/1000, Cross-Entropy Loss: 16.27076609307064, Binary Loss: 4, Weights: [0.42347125 0.06033819 0.51619056]\n",
      "Epoch 589/1000, Cross-Entropy Loss: 16.271190165268536, Binary Loss: 4, Weights: [0.42351006 0.06034068 0.51614925]\n",
      "Epoch 590/1000, Cross-Entropy Loss: 16.271613466330628, Binary Loss: 4, Weights: [0.42354881 0.06034317 0.51610802]\n",
      "Epoch 591/1000, Cross-Entropy Loss: 16.272035998843734, Binary Loss: 4, Weights: [0.42358748 0.06034565 0.51606687]\n",
      "Epoch 592/1000, Cross-Entropy Loss: 16.272457765382022, Binary Loss: 4, Weights: [0.42362608 0.06034812 0.5160258 ]\n",
      "Epoch 593/1000, Cross-Entropy Loss: 16.272878768507056, Binary Loss: 4, Weights: [0.42366461 0.06035059 0.51598479]\n",
      "Epoch 594/1000, Cross-Entropy Loss: 16.273299010767914, Binary Loss: 4, Weights: [0.42370307 0.06035306 0.51594387]\n",
      "Epoch 595/1000, Cross-Entropy Loss: 16.273718494701264, Binary Loss: 4, Weights: [0.42374146 0.06035552 0.51590302]\n",
      "Epoch 596/1000, Cross-Entropy Loss: 16.274137222831406, Binary Loss: 4, Weights: [0.42377978 0.06035797 0.51586224]\n",
      "Epoch 597/1000, Cross-Entropy Loss: 16.27455519767041, Binary Loss: 4, Weights: [0.42381803 0.06036042 0.51582154]\n",
      "Epoch 598/1000, Cross-Entropy Loss: 16.274972421718147, Binary Loss: 4, Weights: [0.42385621 0.06036287 0.51578092]\n",
      "Epoch 599/1000, Cross-Entropy Loss: 16.275388897462395, Binary Loss: 4, Weights: [0.42389432 0.06036531 0.51574037]\n",
      "Epoch 600/1000, Cross-Entropy Loss: 16.275804627378882, Binary Loss: 4, Weights: [0.42393236 0.06036775 0.51569989]\n",
      "Epoch 601/1000, Cross-Entropy Loss: 16.276219613931403, Binary Loss: 4, Weights: [0.42397033 0.06037018 0.51565948]\n",
      "Epoch 602/1000, Cross-Entropy Loss: 16.276633859571863, Binary Loss: 4, Weights: [0.42400823 0.06037261 0.51561915]\n",
      "Epoch 603/1000, Cross-Entropy Loss: 16.277047366740376, Binary Loss: 4, Weights: [0.42404607 0.06037504 0.5155789 ]\n",
      "Epoch 604/1000, Cross-Entropy Loss: 16.277460137865326, Binary Loss: 4, Weights: [0.42408383 0.06037746 0.51553871]\n",
      "Epoch 605/1000, Cross-Entropy Loss: 16.277872175363427, Binary Loss: 4, Weights: [0.42412153 0.06037987 0.5154986 ]\n",
      "Epoch 606/1000, Cross-Entropy Loss: 16.278283481639832, Binary Loss: 4, Weights: [0.42415916 0.06038228 0.51545856]\n",
      "Epoch 607/1000, Cross-Entropy Loss: 16.27869405908818, Binary Loss: 4, Weights: [0.42419672 0.06038469 0.51541859]\n",
      "Epoch 608/1000, Cross-Entropy Loss: 16.27910391009068, Binary Loss: 4, Weights: [0.42423421 0.06038709 0.5153787 ]\n",
      "Epoch 609/1000, Cross-Entropy Loss: 16.279513037018155, Binary Loss: 4, Weights: [0.42427164 0.06038949 0.51533887]\n",
      "Epoch 610/1000, Cross-Entropy Loss: 16.279921442230165, Binary Loss: 4, Weights: [0.424309   0.06039188 0.51529912]\n",
      "Epoch 611/1000, Cross-Entropy Loss: 16.280329128075017, Binary Loss: 4, Weights: [0.42434629 0.06039427 0.51525944]\n",
      "Epoch 612/1000, Cross-Entropy Loss: 16.28073609688989, Binary Loss: 4, Weights: [0.42438351 0.06039665 0.51521983]\n",
      "Epoch 613/1000, Cross-Entropy Loss: 16.281142351000863, Binary Loss: 4, Weights: [0.42442067 0.06039903 0.51518029]\n",
      "Epoch 614/1000, Cross-Entropy Loss: 16.281547892723005, Binary Loss: 4, Weights: [0.42445777 0.06040141 0.51514082]\n",
      "Epoch 615/1000, Cross-Entropy Loss: 16.281952724360437, Binary Loss: 4, Weights: [0.42449479 0.06040378 0.51510143]\n",
      "Epoch 616/1000, Cross-Entropy Loss: 16.282356848206412, Binary Loss: 4, Weights: [0.42453175 0.06040615 0.5150621 ]\n",
      "Epoch 617/1000, Cross-Entropy Loss: 16.28276026654336, Binary Loss: 4, Weights: [0.42456865 0.06040851 0.51502284]\n",
      "Epoch 618/1000, Cross-Entropy Loss: 16.283162981642953, Binary Loss: 4, Weights: [0.42460548 0.06041087 0.51498365]\n",
      "Epoch 619/1000, Cross-Entropy Loss: 16.283564995766213, Binary Loss: 4, Weights: [0.42464224 0.06041322 0.51494453]\n",
      "Epoch 620/1000, Cross-Entropy Loss: 16.283966311163528, Binary Loss: 4, Weights: [0.42467894 0.06041557 0.51490548]\n",
      "Epoch 621/1000, Cross-Entropy Loss: 16.284366930074754, Binary Loss: 4, Weights: [0.42471558 0.06041792 0.5148665 ]\n",
      "Epoch 622/1000, Cross-Entropy Loss: 16.284766854729256, Binary Loss: 4, Weights: [0.42475215 0.06042026 0.51482759]\n",
      "Epoch 623/1000, Cross-Entropy Loss: 16.285166087345964, Binary Loss: 4, Weights: [0.42478866 0.0604226  0.51478875]\n",
      "Epoch 624/1000, Cross-Entropy Loss: 16.285564630133482, Binary Loss: 4, Weights: [0.4248251  0.06042493 0.51474997]\n",
      "Epoch 625/1000, Cross-Entropy Loss: 16.2859624852901, Binary Loss: 4, Weights: [0.42486148 0.06042726 0.51471126]\n",
      "Epoch 626/1000, Cross-Entropy Loss: 16.286359655003885, Binary Loss: 4, Weights: [0.42489779 0.06042958 0.51467262]\n",
      "Epoch 627/1000, Cross-Entropy Loss: 16.286756141452745, Binary Loss: 4, Weights: [0.42493404 0.0604319  0.51463405]\n",
      "Epoch 628/1000, Cross-Entropy Loss: 16.287151946804478, Binary Loss: 4, Weights: [0.42497023 0.06043422 0.51459555]\n",
      "Epoch 629/1000, Cross-Entropy Loss: 16.287547073216825, Binary Loss: 4, Weights: [0.42500636 0.06043653 0.51455711]\n",
      "Epoch 630/1000, Cross-Entropy Loss: 16.287941522837563, Binary Loss: 4, Weights: [0.42504242 0.06043884 0.51451874]\n",
      "Epoch 631/1000, Cross-Entropy Loss: 16.28833529780454, Binary Loss: 4, Weights: [0.42507842 0.06044114 0.51448044]\n",
      "Epoch 632/1000, Cross-Entropy Loss: 16.288728400245734, Binary Loss: 4, Weights: [0.42511435 0.06044344 0.5144422 ]\n",
      "Epoch 633/1000, Cross-Entropy Loss: 16.289120832279334, Binary Loss: 4, Weights: [0.42515023 0.06044574 0.51440403]\n",
      "Epoch 634/1000, Cross-Entropy Loss: 16.28951259601378, Binary Loss: 4, Weights: [0.42518604 0.06044803 0.51436593]\n",
      "Epoch 635/1000, Cross-Entropy Loss: 16.28990369354781, Binary Loss: 4, Weights: [0.42522179 0.06045032 0.51432789]\n",
      "Epoch 636/1000, Cross-Entropy Loss: 16.29029412697056, Binary Loss: 4, Weights: [0.42525748 0.0604526  0.51428992]\n",
      "Epoch 637/1000, Cross-Entropy Loss: 16.290683898361582, Binary Loss: 4, Weights: [0.42529311 0.06045488 0.51425201]\n",
      "Epoch 638/1000, Cross-Entropy Loss: 16.291073009790917, Binary Loss: 4, Weights: [0.42532867 0.06045716 0.51421417]\n",
      "Epoch 639/1000, Cross-Entropy Loss: 16.291461463319166, Binary Loss: 4, Weights: [0.42536418 0.06045943 0.51417639]\n",
      "Epoch 640/1000, Cross-Entropy Loss: 16.29184926099751, Binary Loss: 4, Weights: [0.42539962 0.06046169 0.51413868]\n",
      "Epoch 641/1000, Cross-Entropy Loss: 16.292236404867793, Binary Loss: 4, Weights: [0.42543501 0.06046396 0.51410104]\n",
      "Epoch 642/1000, Cross-Entropy Loss: 16.292622896962587, Binary Loss: 4, Weights: [0.42547033 0.06046622 0.51406345]\n",
      "Epoch 643/1000, Cross-Entropy Loss: 16.293008739305215, Binary Loss: 4, Weights: [0.42550559 0.06046847 0.51402593]\n",
      "Epoch 644/1000, Cross-Entropy Loss: 16.293393933909833, Binary Loss: 4, Weights: [0.4255408  0.06047072 0.51398848]\n",
      "Epoch 645/1000, Cross-Entropy Loss: 16.29377848278147, Binary Loss: 4, Weights: [0.42557594 0.06047297 0.51395109]\n",
      "Epoch 646/1000, Cross-Entropy Loss: 16.294162387916085, Binary Loss: 4, Weights: [0.42561102 0.06047521 0.51391376]\n",
      "Epoch 647/1000, Cross-Entropy Loss: 16.294545651300627, Binary Loss: 4, Weights: [0.42564604 0.06047745 0.5138765 ]\n",
      "Epoch 648/1000, Cross-Entropy Loss: 16.29492827491308, Binary Loss: 4, Weights: [0.42568101 0.06047969 0.5138393 ]\n",
      "Epoch 649/1000, Cross-Entropy Loss: 16.295310260722534, Binary Loss: 4, Weights: [0.42571591 0.06048192 0.51380217]\n",
      "Epoch 650/1000, Cross-Entropy Loss: 16.295691610689197, Binary Loss: 4, Weights: [0.42575076 0.06048415 0.51376509]\n",
      "Epoch 651/1000, Cross-Entropy Loss: 16.296072326764495, Binary Loss: 4, Weights: [0.42578555 0.06048637 0.51372808]\n",
      "Epoch 652/1000, Cross-Entropy Loss: 16.29645241089109, Binary Loss: 4, Weights: [0.42582027 0.06048859 0.51369113]\n",
      "Epoch 653/1000, Cross-Entropy Loss: 16.296831865002943, Binary Loss: 4, Weights: [0.42585494 0.06049081 0.51365425]\n",
      "Epoch 654/1000, Cross-Entropy Loss: 16.297210691025384, Binary Loss: 4, Weights: [0.42588955 0.06049302 0.51361742]\n",
      "Epoch 655/1000, Cross-Entropy Loss: 16.297588890875115, Binary Loss: 4, Weights: [0.42592411 0.06049523 0.51358066]\n",
      "Epoch 656/1000, Cross-Entropy Loss: 16.297966466460313, Binary Loss: 4, Weights: [0.4259586  0.06049744 0.51354396]\n",
      "Epoch 657/1000, Cross-Entropy Loss: 16.298343419680627, Binary Loss: 4, Weights: [0.42599304 0.06049964 0.51350732]\n",
      "Epoch 658/1000, Cross-Entropy Loss: 16.298719752427296, Binary Loss: 4, Weights: [0.42602742 0.06050183 0.51347074]\n",
      "Epoch 659/1000, Cross-Entropy Loss: 16.299095466583122, Binary Loss: 4, Weights: [0.42606174 0.06050403 0.51343423]\n",
      "Epoch 660/1000, Cross-Entropy Loss: 16.29947056402258, Binary Loss: 4, Weights: [0.42609601 0.06050622 0.51339777]\n",
      "Epoch 661/1000, Cross-Entropy Loss: 16.29984504661183, Binary Loss: 4, Weights: [0.42613022 0.0605084  0.51336138]\n",
      "Epoch 662/1000, Cross-Entropy Loss: 16.30021891620877, Binary Loss: 4, Weights: [0.42616437 0.06051059 0.51332504]\n",
      "Epoch 663/1000, Cross-Entropy Loss: 16.300592174663112, Binary Loss: 4, Weights: [0.42619846 0.06051276 0.51328877]\n",
      "Epoch 664/1000, Cross-Entropy Loss: 16.300964823816386, Binary Loss: 4, Weights: [0.4262325  0.06051494 0.51325256]\n",
      "Epoch 665/1000, Cross-Entropy Loss: 16.30133686550203, Binary Loss: 4, Weights: [0.42626648 0.06051711 0.51321641]\n",
      "Epoch 666/1000, Cross-Entropy Loss: 16.30170830154539, Binary Loss: 4, Weights: [0.42630041 0.06051928 0.51318031]\n",
      "Epoch 667/1000, Cross-Entropy Loss: 16.302079133763808, Binary Loss: 4, Weights: [0.42633428 0.06052144 0.51314428]\n",
      "Epoch 668/1000, Cross-Entropy Loss: 16.302449363966666, Binary Loss: 4, Weights: [0.42636809 0.0605236  0.5131083 ]\n",
      "Epoch 669/1000, Cross-Entropy Loss: 16.302818993955384, Binary Loss: 4, Weights: [0.42640185 0.06052576 0.51307239]\n",
      "Epoch 670/1000, Cross-Entropy Loss: 16.303188025523525, Binary Loss: 4, Weights: [0.42643555 0.06052791 0.51303653]\n",
      "Epoch 671/1000, Cross-Entropy Loss: 16.303556460456804, Binary Loss: 4, Weights: [0.4264692  0.06053006 0.51300074]\n",
      "Epoch 672/1000, Cross-Entropy Loss: 16.30392430053315, Binary Loss: 4, Weights: [0.42650279 0.06053221 0.512965  ]\n",
      "Epoch 673/1000, Cross-Entropy Loss: 16.30429154752274, Binary Loss: 4, Weights: [0.42653633 0.06053435 0.51292932]\n",
      "Epoch 674/1000, Cross-Entropy Loss: 16.30465820318804, Binary Loss: 4, Weights: [0.42656981 0.06053649 0.5128937 ]\n",
      "Epoch 675/1000, Cross-Entropy Loss: 16.305024269283866, Binary Loss: 4, Weights: [0.42660324 0.06053862 0.51285814]\n",
      "Epoch 676/1000, Cross-Entropy Loss: 16.30538974755742, Binary Loss: 4, Weights: [0.42663661 0.06054075 0.51282263]\n",
      "Epoch 677/1000, Cross-Entropy Loss: 16.30575463974832, Binary Loss: 4, Weights: [0.42666993 0.06054288 0.51278719]\n",
      "Epoch 678/1000, Cross-Entropy Loss: 16.306118947588647, Binary Loss: 4, Weights: [0.4267032 0.060545  0.5127518]\n",
      "Epoch 679/1000, Cross-Entropy Loss: 16.306482672803018, Binary Loss: 4, Weights: [0.42673641 0.06054712 0.51271647]\n",
      "Epoch 680/1000, Cross-Entropy Loss: 16.306845817108588, Binary Loss: 4, Weights: [0.42676956 0.06054924 0.51268119]\n",
      "Epoch 681/1000, Cross-Entropy Loss: 16.307208382215098, Binary Loss: 4, Weights: [0.42680267 0.06055136 0.51264598]\n",
      "Epoch 682/1000, Cross-Entropy Loss: 16.307570369824955, Binary Loss: 4, Weights: [0.42683572 0.06055347 0.51261082]\n",
      "Epoch 683/1000, Cross-Entropy Loss: 16.307931781633226, Binary Loss: 4, Weights: [0.42686871 0.06055557 0.51257571]\n",
      "Epoch 684/1000, Cross-Entropy Loss: 16.3082926193277, Binary Loss: 4, Weights: [0.42690166 0.06055767 0.51254067]\n",
      "Epoch 685/1000, Cross-Entropy Loss: 16.30865288458893, Binary Loss: 4, Weights: [0.42693455 0.06055977 0.51250568]\n",
      "Epoch 686/1000, Cross-Entropy Loss: 16.30901257909027, Binary Loss: 4, Weights: [0.42696738 0.06056187 0.51247075]\n",
      "Epoch 687/1000, Cross-Entropy Loss: 16.309371704497917, Binary Loss: 4, Weights: [0.42700017 0.06056396 0.51243587]\n",
      "Epoch 688/1000, Cross-Entropy Loss: 16.309730262470953, Binary Loss: 4, Weights: [0.4270329  0.06056605 0.51240105]\n",
      "Epoch 689/1000, Cross-Entropy Loss: 16.310088254661377, Binary Loss: 4, Weights: [0.42706558 0.06056814 0.51236628]\n",
      "Epoch 690/1000, Cross-Entropy Loss: 16.310445682714157, Binary Loss: 4, Weights: [0.42709821 0.06057022 0.51233158]\n",
      "Epoch 691/1000, Cross-Entropy Loss: 16.310802548267255, Binary Loss: 4, Weights: [0.42713078 0.0605723  0.51229692]\n",
      "Epoch 692/1000, Cross-Entropy Loss: 16.311158852951664, Binary Loss: 4, Weights: [0.4271633  0.06057437 0.51226232]\n",
      "Epoch 693/1000, Cross-Entropy Loss: 16.31151459839148, Binary Loss: 4, Weights: [0.42719577 0.06057644 0.51222778]\n",
      "Epoch 694/1000, Cross-Entropy Loss: 16.3118697862039, Binary Loss: 4, Weights: [0.42722819 0.06057851 0.51219329]\n",
      "Epoch 695/1000, Cross-Entropy Loss: 16.312224417999268, Binary Loss: 4, Weights: [0.42726056 0.06058058 0.51215886]\n",
      "Epoch 696/1000, Cross-Entropy Loss: 16.312578495381132, Binary Loss: 4, Weights: [0.42729288 0.06058264 0.51212448]\n",
      "Epoch 697/1000, Cross-Entropy Loss: 16.31293201994628, Binary Loss: 4, Weights: [0.42732514 0.0605847  0.51209016]\n",
      "Epoch 698/1000, Cross-Entropy Loss: 16.313284993284743, Binary Loss: 4, Weights: [0.42735736 0.06058675 0.51205589]\n",
      "Epoch 699/1000, Cross-Entropy Loss: 16.31363741697987, Binary Loss: 4, Weights: [0.42738952 0.0605888  0.51202167]\n",
      "Epoch 700/1000, Cross-Entropy Loss: 16.313989292608355, Binary Loss: 4, Weights: [0.42742163 0.06059085 0.51198751]\n",
      "Epoch 701/1000, Cross-Entropy Loss: 16.314340621740264, Binary Loss: 4, Weights: [0.4274537  0.0605929  0.51195341]\n",
      "Epoch 702/1000, Cross-Entropy Loss: 16.314691405939083, Binary Loss: 4, Weights: [0.42748571 0.06059494 0.51191935]\n",
      "Epoch 703/1000, Cross-Entropy Loss: 16.315041646761745, Binary Loss: 4, Weights: [0.42751767 0.06059698 0.51188535]\n",
      "Epoch 704/1000, Cross-Entropy Loss: 16.315391345758663, Binary Loss: 4, Weights: [0.42754958 0.06059901 0.51185141]\n",
      "Epoch 705/1000, Cross-Entropy Loss: 16.31574050447378, Binary Loss: 4, Weights: [0.42758144 0.06060104 0.51181752]\n",
      "Epoch 706/1000, Cross-Entropy Loss: 16.316089124444606, Binary Loss: 4, Weights: [0.42761325 0.06060307 0.51178368]\n",
      "Epoch 707/1000, Cross-Entropy Loss: 16.31643720720222, Binary Loss: 4, Weights: [0.42764501 0.0606051  0.51174989]\n",
      "Epoch 708/1000, Cross-Entropy Loss: 16.316784754271346, Binary Loss: 4, Weights: [0.42767672 0.06060712 0.51171616]\n",
      "Epoch 709/1000, Cross-Entropy Loss: 16.317131767170363, Binary Loss: 4, Weights: [0.42770838 0.06060914 0.51168248]\n",
      "Epoch 710/1000, Cross-Entropy Loss: 16.31747824741135, Binary Loss: 4, Weights: [0.42774    0.06061115 0.51164885]\n",
      "Epoch 711/1000, Cross-Entropy Loss: 16.31782419650012, Binary Loss: 4, Weights: [0.42777156 0.06061317 0.51161527]\n",
      "Epoch 712/1000, Cross-Entropy Loss: 16.318169615936235, Binary Loss: 4, Weights: [0.42780308 0.06061518 0.51158175]\n",
      "Epoch 713/1000, Cross-Entropy Loss: 16.318514507213077, Binary Loss: 4, Weights: [0.42783454 0.06061718 0.51154828]\n",
      "Epoch 714/1000, Cross-Entropy Loss: 16.318858871817845, Binary Loss: 4, Weights: [0.42786596 0.06061918 0.51151486]\n",
      "Epoch 715/1000, Cross-Entropy Loss: 16.3192027112316, Binary Loss: 4, Weights: [0.42789733 0.06062118 0.51148149]\n",
      "Epoch 716/1000, Cross-Entropy Loss: 16.319546026929324, Binary Loss: 4, Weights: [0.42792865 0.06062318 0.51144818]\n",
      "Epoch 717/1000, Cross-Entropy Loss: 16.319888820379905, Binary Loss: 4, Weights: [0.42795992 0.06062517 0.51141491]\n",
      "Epoch 718/1000, Cross-Entropy Loss: 16.3202310930462, Binary Loss: 4, Weights: [0.42799114 0.06062716 0.5113817 ]\n",
      "Epoch 719/1000, Cross-Entropy Loss: 16.320572846385073, Binary Loss: 4, Weights: [0.42802232 0.06062915 0.51134854]\n",
      "Epoch 720/1000, Cross-Entropy Loss: 16.32091408184741, Binary Loss: 4, Weights: [0.42805344 0.06063113 0.51131543]\n",
      "Epoch 721/1000, Cross-Entropy Loss: 16.321254800878158, Binary Loss: 4, Weights: [0.42808452 0.06063311 0.51128237]\n",
      "Epoch 722/1000, Cross-Entropy Loss: 16.321595004916357, Binary Loss: 4, Weights: [0.42811555 0.06063509 0.51124936]\n",
      "Epoch 723/1000, Cross-Entropy Loss: 16.321934695395154, Binary Loss: 4, Weights: [0.42814654 0.06063706 0.5112164 ]\n",
      "Epoch 724/1000, Cross-Entropy Loss: 16.32227387374188, Binary Loss: 4, Weights: [0.42817747 0.06063903 0.51118349]\n",
      "Epoch 725/1000, Cross-Entropy Loss: 16.322612541378042, Binary Loss: 4, Weights: [0.42820836 0.060641   0.51115063]\n",
      "Epoch 726/1000, Cross-Entropy Loss: 16.322950699719346, Binary Loss: 4, Weights: [0.42823921 0.06064297 0.51111783]\n",
      "Epoch 727/1000, Cross-Entropy Loss: 16.323288350175766, Binary Loss: 4, Weights: [0.42827    0.06064493 0.51108507]\n",
      "Epoch 728/1000, Cross-Entropy Loss: 16.32362549415155, Binary Loss: 4, Weights: [0.42830075 0.06064689 0.51105236]\n",
      "Epoch 729/1000, Cross-Entropy Loss: 16.323962133045253, Binary Loss: 4, Weights: [0.42833145 0.06064884 0.51101971]\n",
      "Epoch 730/1000, Cross-Entropy Loss: 16.32429826824976, Binary Loss: 4, Weights: [0.42836211 0.0606508  0.5109871 ]\n",
      "Epoch 731/1000, Cross-Entropy Loss: 16.324633901152346, Binary Loss: 4, Weights: [0.42839271 0.06065274 0.51095454]\n",
      "Epoch 732/1000, Cross-Entropy Loss: 16.32496903313466, Binary Loss: 4, Weights: [0.42842328 0.06065469 0.51092203]\n",
      "Epoch 733/1000, Cross-Entropy Loss: 16.3253036655728, Binary Loss: 4, Weights: [0.42845379 0.06065663 0.51088957]\n",
      "Epoch 734/1000, Cross-Entropy Loss: 16.325637799837306, Binary Loss: 4, Weights: [0.42848426 0.06065857 0.51085716]\n",
      "Epoch 735/1000, Cross-Entropy Loss: 16.32597143729321, Binary Loss: 4, Weights: [0.42851469 0.06066051 0.5108248 ]\n",
      "Epoch 736/1000, Cross-Entropy Loss: 16.32630457930006, Binary Loss: 4, Weights: [0.42854507 0.06066245 0.51079249]\n",
      "Epoch 737/1000, Cross-Entropy Loss: 16.326637227211954, Binary Loss: 4, Weights: [0.4285754  0.06066438 0.51076023]\n",
      "Epoch 738/1000, Cross-Entropy Loss: 16.326969382377555, Binary Loss: 4, Weights: [0.42860568 0.06066631 0.51072801]\n",
      "Epoch 739/1000, Cross-Entropy Loss: 16.327301046140125, Binary Loss: 4, Weights: [0.42863593 0.06066823 0.51069584]\n",
      "Epoch 740/1000, Cross-Entropy Loss: 16.32763221983756, Binary Loss: 4, Weights: [0.42866612 0.06067015 0.51066373]\n",
      "Epoch 741/1000, Cross-Entropy Loss: 16.327962904802426, Binary Loss: 4, Weights: [0.42869627 0.06067207 0.51063166]\n",
      "Epoch 742/1000, Cross-Entropy Loss: 16.328293102361943, Binary Loss: 4, Weights: [0.42872638 0.06067399 0.51059963]\n",
      "Epoch 743/1000, Cross-Entropy Loss: 16.328622813838074, Binary Loss: 4, Weights: [0.42875644 0.0606759  0.51056766]\n",
      "Epoch 744/1000, Cross-Entropy Loss: 16.32895204054751, Binary Loss: 4, Weights: [0.42878645 0.06067781 0.51053573]\n",
      "Epoch 745/1000, Cross-Entropy Loss: 16.32928078380171, Binary Loss: 4, Weights: [0.42881643 0.06067972 0.51050386]\n",
      "Epoch 746/1000, Cross-Entropy Loss: 16.32960904490693, Binary Loss: 4, Weights: [0.42884635 0.06068162 0.51047202]\n",
      "Epoch 747/1000, Cross-Entropy Loss: 16.329936825164253, Binary Loss: 4, Weights: [0.42887623 0.06068353 0.51044024]\n",
      "Epoch 748/1000, Cross-Entropy Loss: 16.330264125869594, Binary Loss: 4, Weights: [0.42890607 0.06068542 0.5104085 ]\n",
      "Epoch 749/1000, Cross-Entropy Loss: 16.330590948313766, Binary Loss: 4, Weights: [0.42893587 0.06068732 0.51037681]\n",
      "Epoch 750/1000, Cross-Entropy Loss: 16.330917293782466, Binary Loss: 4, Weights: [0.42896561 0.06068921 0.51034517]\n",
      "Epoch 751/1000, Cross-Entropy Loss: 16.33124316355633, Binary Loss: 4, Weights: [0.42899532 0.0606911  0.51031358]\n",
      "Epoch 752/1000, Cross-Entropy Loss: 16.33156855891095, Binary Loss: 4, Weights: [0.42902498 0.06069299 0.51028203]\n",
      "Epoch 753/1000, Cross-Entropy Loss: 16.331893481116882, Binary Loss: 4, Weights: [0.4290546  0.06069487 0.51025053]\n",
      "Epoch 754/1000, Cross-Entropy Loss: 16.332217931439704, Binary Loss: 4, Weights: [0.42908417 0.06069676 0.51021907]\n",
      "Epoch 755/1000, Cross-Entropy Loss: 16.332541911140023, Binary Loss: 4, Weights: [0.4291137  0.06069863 0.51018766]\n",
      "Epoch 756/1000, Cross-Entropy Loss: 16.3328654214735, Binary Loss: 4, Weights: [0.42914319 0.06070051 0.5101563 ]\n",
      "Epoch 757/1000, Cross-Entropy Loss: 16.333188463690885, Binary Loss: 4, Weights: [0.42917264 0.06070238 0.51012498]\n",
      "Epoch 758/1000, Cross-Entropy Loss: 16.333511039038015, Binary Loss: 4, Weights: [0.42920204 0.06070425 0.51009371]\n",
      "Epoch 759/1000, Cross-Entropy Loss: 16.333833148755893, Binary Loss: 4, Weights: [0.42923139 0.06070612 0.51006249]\n",
      "Epoch 760/1000, Cross-Entropy Loss: 16.334154794080657, Binary Loss: 4, Weights: [0.42926071 0.06070798 0.51003131]\n",
      "Epoch 761/1000, Cross-Entropy Loss: 16.334475976243628, Binary Loss: 4, Weights: [0.42928998 0.06070985 0.51000017]\n",
      "Epoch 762/1000, Cross-Entropy Loss: 16.334796696471347, Binary Loss: 4, Weights: [0.42931921 0.0607117  0.50996909]\n",
      "Epoch 763/1000, Cross-Entropy Loss: 16.335116955985562, Binary Loss: 4, Weights: [0.4293484  0.06071356 0.50993804]\n",
      "Epoch 764/1000, Cross-Entropy Loss: 16.335436756003308, Binary Loss: 4, Weights: [0.42937754 0.06071541 0.50990705]\n",
      "Epoch 765/1000, Cross-Entropy Loss: 16.335756097736873, Binary Loss: 4, Weights: [0.42940664 0.06071726 0.50987609]\n",
      "Epoch 766/1000, Cross-Entropy Loss: 16.33607498239386, Binary Loss: 4, Weights: [0.4294357  0.06071911 0.50984519]\n",
      "Epoch 767/1000, Cross-Entropy Loss: 16.336393411177202, Binary Loss: 4, Weights: [0.42946472 0.06072096 0.50981433]\n",
      "Epoch 768/1000, Cross-Entropy Loss: 16.336711385285174, Binary Loss: 4, Weights: [0.42949369 0.0607228  0.50978351]\n",
      "Epoch 769/1000, Cross-Entropy Loss: 16.337028905911428, Binary Loss: 4, Weights: [0.42952263 0.06072464 0.50975274]\n",
      "Epoch 770/1000, Cross-Entropy Loss: 16.337345974245018, Binary Loss: 4, Weights: [0.42955152 0.06072647 0.50972201]\n",
      "Epoch 771/1000, Cross-Entropy Loss: 16.337662591470416, Binary Loss: 4, Weights: [0.42958037 0.06072831 0.50969132]\n",
      "Epoch 772/1000, Cross-Entropy Loss: 16.337978758767537, Binary Loss: 4, Weights: [0.42960918 0.06073014 0.50966068]\n",
      "Epoch 773/1000, Cross-Entropy Loss: 16.338294477311756, Binary Loss: 4, Weights: [0.42963795 0.06073197 0.50963009]\n",
      "Epoch 774/1000, Cross-Entropy Loss: 16.33860974827395, Binary Loss: 4, Weights: [0.42966667 0.06073379 0.50959954]\n",
      "Epoch 775/1000, Cross-Entropy Loss: 16.338924572820503, Binary Loss: 4, Weights: [0.42969536 0.06073561 0.50956903]\n",
      "Epoch 776/1000, Cross-Entropy Loss: 16.33923895211333, Binary Loss: 4, Weights: [0.429724   0.06073743 0.50953857]\n",
      "Epoch 777/1000, Cross-Entropy Loss: 16.339552887309907, Binary Loss: 4, Weights: [0.4297526  0.06073925 0.50950815]\n",
      "Epoch 778/1000, Cross-Entropy Loss: 16.339866379563283, Binary Loss: 4, Weights: [0.42978116 0.06074107 0.50947777]\n",
      "Epoch 779/1000, Cross-Entropy Loss: 16.340179430022122, Binary Loss: 4, Weights: [0.42980968 0.06074288 0.50944744]\n",
      "Epoch 780/1000, Cross-Entropy Loss: 16.34049203983069, Binary Loss: 4, Weights: [0.42983816 0.06074469 0.50941715]\n",
      "Epoch 781/1000, Cross-Entropy Loss: 16.34080421012891, Binary Loss: 4, Weights: [0.4298666  0.06074649 0.5093869 ]\n",
      "Epoch 782/1000, Cross-Entropy Loss: 16.341115942052365, Binary Loss: 4, Weights: [0.429895  0.0607483 0.5093567]\n",
      "Epoch 783/1000, Cross-Entropy Loss: 16.34142723673235, Binary Loss: 4, Weights: [0.42992336 0.0607501  0.50932654]\n",
      "Epoch 784/1000, Cross-Entropy Loss: 16.34173809529583, Binary Loss: 4, Weights: [0.42995168 0.0607519  0.50929643]\n",
      "Epoch 785/1000, Cross-Entropy Loss: 16.342048518865536, Binary Loss: 4, Weights: [0.42997995 0.06075369 0.50926635]\n",
      "Epoch 786/1000, Cross-Entropy Loss: 16.34235850855992, Binary Loss: 4, Weights: [0.43000819 0.06075549 0.50923632]\n",
      "Epoch 787/1000, Cross-Entropy Loss: 16.342668065493225, Binary Loss: 4, Weights: [0.43003639 0.06075728 0.50920633]\n",
      "Epoch 788/1000, Cross-Entropy Loss: 16.342977190775493, Binary Loss: 4, Weights: [0.43006455 0.06075907 0.50917639]\n",
      "Epoch 789/1000, Cross-Entropy Loss: 16.343285885512564, Binary Loss: 4, Weights: [0.43009267 0.06076085 0.50914648]\n",
      "Epoch 790/1000, Cross-Entropy Loss: 16.343594150806123, Binary Loss: 4, Weights: [0.43012074 0.06076264 0.50911662]\n",
      "Epoch 791/1000, Cross-Entropy Loss: 16.343901987753707, Binary Loss: 4, Weights: [0.43014878 0.06076442 0.5090868 ]\n",
      "Epoch 792/1000, Cross-Entropy Loss: 16.34420939744872, Binary Loss: 4, Weights: [0.43017678 0.06076619 0.50905702]\n",
      "Epoch 793/1000, Cross-Entropy Loss: 16.344516380980483, Binary Loss: 4, Weights: [0.43020474 0.06076797 0.50902729]\n",
      "Epoch 794/1000, Cross-Entropy Loss: 16.34482293943421, Binary Loss: 4, Weights: [0.43023266 0.06076974 0.5089976 ]\n",
      "Epoch 795/1000, Cross-Entropy Loss: 16.345129073891076, Binary Loss: 4, Weights: [0.43026054 0.06077151 0.50896794]\n",
      "Epoch 796/1000, Cross-Entropy Loss: 16.345434785428182, Binary Loss: 4, Weights: [0.43028839 0.06077328 0.50893833]\n",
      "Epoch 797/1000, Cross-Entropy Loss: 16.345740075118627, Binary Loss: 4, Weights: [0.43031619 0.06077504 0.50890877]\n",
      "Epoch 798/1000, Cross-Entropy Loss: 16.346044944031497, Binary Loss: 4, Weights: [0.43034395 0.06077681 0.50887924]\n",
      "Epoch 799/1000, Cross-Entropy Loss: 16.346349393231893, Binary Loss: 4, Weights: [0.43037168 0.06077857 0.50884975]\n",
      "Epoch 800/1000, Cross-Entropy Loss: 16.346653423780957, Binary Loss: 4, Weights: [0.43039937 0.06078032 0.50882031]\n",
      "Epoch 801/1000, Cross-Entropy Loss: 16.34695703673587, Binary Loss: 4, Weights: [0.43042702 0.06078208 0.5087909 ]\n",
      "Epoch 802/1000, Cross-Entropy Loss: 16.3472602331499, Binary Loss: 4, Weights: [0.43045463 0.06078383 0.50876154]\n",
      "Epoch 803/1000, Cross-Entropy Loss: 16.3475630140724, Binary Loss: 4, Weights: [0.4304822  0.06078558 0.50873222]\n",
      "Epoch 804/1000, Cross-Entropy Loss: 16.34786538054883, Binary Loss: 4, Weights: [0.43050973 0.06078733 0.50870294]\n",
      "Epoch 805/1000, Cross-Entropy Loss: 16.348167333620786, Binary Loss: 4, Weights: [0.43053723 0.06078907 0.5086737 ]\n",
      "Epoch 806/1000, Cross-Entropy Loss: 16.348468874326013, Binary Loss: 4, Weights: [0.43056469 0.06079081 0.5086445 ]\n",
      "Epoch 807/1000, Cross-Entropy Loss: 16.34877000369842, Binary Loss: 4, Weights: [0.43059211 0.06079255 0.50861534]\n",
      "Epoch 808/1000, Cross-Entropy Loss: 16.349070722768094, Binary Loss: 4, Weights: [0.43061949 0.06079429 0.50858622]\n",
      "Epoch 809/1000, Cross-Entropy Loss: 16.34937103256134, Binary Loss: 4, Weights: [0.43064683 0.06079603 0.50855714]\n",
      "Epoch 810/1000, Cross-Entropy Loss: 16.349670934100672, Binary Loss: 4, Weights: [0.43067414 0.06079776 0.50852811]\n",
      "Epoch 811/1000, Cross-Entropy Loss: 16.349970428404855, Binary Loss: 4, Weights: [0.43070141 0.06079949 0.50849911]\n",
      "Epoch 812/1000, Cross-Entropy Loss: 16.3502695164889, Binary Loss: 4, Weights: [0.43072864 0.06080121 0.50847015]\n",
      "Epoch 813/1000, Cross-Entropy Loss: 16.350568199364098, Binary Loss: 4, Weights: [0.43075583 0.06080294 0.50844123]\n",
      "Epoch 814/1000, Cross-Entropy Loss: 16.350866478038046, Binary Loss: 4, Weights: [0.43078299 0.06080466 0.50841235]\n",
      "Epoch 815/1000, Cross-Entropy Loss: 16.35116435351464, Binary Loss: 4, Weights: [0.43081011 0.06080638 0.50838351]\n",
      "Epoch 816/1000, Cross-Entropy Loss: 16.351461826794097, Binary Loss: 4, Weights: [0.43083719 0.0608081  0.50835471]\n",
      "Epoch 817/1000, Cross-Entropy Loss: 16.35175889887301, Binary Loss: 4, Weights: [0.43086423 0.06080981 0.50832595]\n",
      "Epoch 818/1000, Cross-Entropy Loss: 16.352055570744294, Binary Loss: 4, Weights: [0.43089124 0.06081153 0.50829723]\n",
      "Epoch 819/1000, Cross-Entropy Loss: 16.35235184339729, Binary Loss: 4, Weights: [0.43091821 0.06081324 0.50826855]\n",
      "Epoch 820/1000, Cross-Entropy Loss: 16.35264771781771, Binary Loss: 4, Weights: [0.43094515 0.06081494 0.50823991]\n",
      "Epoch 821/1000, Cross-Entropy Loss: 16.352943194987688, Binary Loss: 4, Weights: [0.43097204 0.06081665 0.50821131]\n",
      "Epoch 822/1000, Cross-Entropy Loss: 16.353238275885793, Binary Loss: 4, Weights: [0.43099891 0.06081835 0.50818274]\n",
      "Epoch 823/1000, Cross-Entropy Loss: 16.353532961487034, Binary Loss: 4, Weights: [0.43102573 0.06082005 0.50815422]\n",
      "Epoch 824/1000, Cross-Entropy Loss: 16.35382725276291, Binary Loss: 4, Weights: [0.43105252 0.06082175 0.50812573]\n",
      "Epoch 825/1000, Cross-Entropy Loss: 16.354121150681372, Binary Loss: 4, Weights: [0.43107927 0.06082345 0.50809728]\n",
      "Epoch 826/1000, Cross-Entropy Loss: 16.354414656206902, Binary Loss: 4, Weights: [0.43110599 0.06082514 0.50806887]\n",
      "Epoch 827/1000, Cross-Entropy Loss: 16.354707770300468, Binary Loss: 4, Weights: [0.43113267 0.06082683 0.5080405 ]\n",
      "Epoch 828/1000, Cross-Entropy Loss: 16.3550004939196, Binary Loss: 4, Weights: [0.43115931 0.06082852 0.50801217]\n",
      "Epoch 829/1000, Cross-Entropy Loss: 16.355292828018356, Binary Loss: 4, Weights: [0.43118592 0.06083021 0.50798388]\n",
      "Epoch 830/1000, Cross-Entropy Loss: 16.355584773547367, Binary Loss: 4, Weights: [0.43121249 0.06083189 0.50795562]\n",
      "Epoch 831/1000, Cross-Entropy Loss: 16.355876331453846, Binary Loss: 4, Weights: [0.43123903 0.06083357 0.5079274 ]\n",
      "Epoch 832/1000, Cross-Entropy Loss: 16.35616750268161, Binary Loss: 4, Weights: [0.43126553 0.06083525 0.50789922]\n",
      "Epoch 833/1000, Cross-Entropy Loss: 16.356458288171066, Binary Loss: 4, Weights: [0.43129199 0.06083693 0.50787108]\n",
      "Epoch 834/1000, Cross-Entropy Loss: 16.35674868885928, Binary Loss: 4, Weights: [0.43131842 0.0608386  0.50784298]\n",
      "Epoch 835/1000, Cross-Entropy Loss: 16.357038705679948, Binary Loss: 4, Weights: [0.43134481 0.06084027 0.50781491]\n",
      "Epoch 836/1000, Cross-Entropy Loss: 16.35732833956343, Binary Loss: 4, Weights: [0.43137117 0.06084194 0.50778688]\n",
      "Epoch 837/1000, Cross-Entropy Loss: 16.35761759143676, Binary Loss: 4, Weights: [0.4313975  0.06084361 0.50775889]\n",
      "Epoch 838/1000, Cross-Entropy Loss: 16.35790646222366, Binary Loss: 4, Weights: [0.43142379 0.06084528 0.50773094]\n",
      "Epoch 839/1000, Cross-Entropy Loss: 16.358194952844574, Binary Loss: 4, Weights: [0.43145004 0.06084694 0.50770302]\n",
      "Epoch 840/1000, Cross-Entropy Loss: 16.358483064216653, Binary Loss: 4, Weights: [0.43147626 0.0608486  0.50767514]\n",
      "Epoch 841/1000, Cross-Entropy Loss: 16.358770797253797, Binary Loss: 4, Weights: [0.43150244 0.06085026 0.5076473 ]\n",
      "Epoch 842/1000, Cross-Entropy Loss: 16.35905815286665, Binary Loss: 4, Weights: [0.43152859 0.06085192 0.5076195 ]\n",
      "Epoch 843/1000, Cross-Entropy Loss: 16.359345131962623, Binary Loss: 4, Weights: [0.4315547  0.06085357 0.50759173]\n",
      "Epoch 844/1000, Cross-Entropy Loss: 16.359631735445937, Binary Loss: 4, Weights: [0.43158078 0.06085522 0.507564  ]\n",
      "Epoch 845/1000, Cross-Entropy Loss: 16.359917964217566, Binary Loss: 4, Weights: [0.43160683 0.06085687 0.5075363 ]\n",
      "Epoch 846/1000, Cross-Entropy Loss: 16.360203819175332, Binary Loss: 4, Weights: [0.43163284 0.06085852 0.50750865]\n",
      "Epoch 847/1000, Cross-Entropy Loss: 16.36048930121388, Binary Loss: 4, Weights: [0.43165881 0.06086016 0.50748103]\n",
      "Epoch 848/1000, Cross-Entropy Loss: 16.360774411224668, Binary Loss: 4, Weights: [0.43168475 0.0608618  0.50745344]\n",
      "Epoch 849/1000, Cross-Entropy Loss: 16.361059150096054, Binary Loss: 4, Weights: [0.43171066 0.06086344 0.50742589]\n",
      "Epoch 850/1000, Cross-Entropy Loss: 16.36134351871324, Binary Loss: 4, Weights: [0.43173653 0.06086508 0.50739838]\n",
      "Epoch 851/1000, Cross-Entropy Loss: 16.361627517958325, Binary Loss: 4, Weights: [0.43176237 0.06086672 0.50737091]\n",
      "Epoch 852/1000, Cross-Entropy Loss: 16.361911148710288, Binary Loss: 4, Weights: [0.43178818 0.06086835 0.50734347]\n",
      "Epoch 853/1000, Cross-Entropy Loss: 16.362194411845056, Binary Loss: 4, Weights: [0.43181395 0.06086998 0.50731607]\n",
      "Epoch 854/1000, Cross-Entropy Loss: 16.36247730823544, Binary Loss: 4, Weights: [0.43183969 0.06087161 0.5072887 ]\n",
      "Epoch 855/1000, Cross-Entropy Loss: 16.362759838751245, Binary Loss: 4, Weights: [0.43186539 0.06087324 0.50726137]\n",
      "Epoch 856/1000, Cross-Entropy Loss: 16.36304200425919, Binary Loss: 4, Weights: [0.43189106 0.06087486 0.50723408]\n",
      "Epoch 857/1000, Cross-Entropy Loss: 16.36332380562298, Binary Loss: 4, Weights: [0.4319167  0.06087648 0.50720682]\n",
      "Epoch 858/1000, Cross-Entropy Loss: 16.36360524370331, Binary Loss: 4, Weights: [0.4319423  0.06087811 0.50717959]\n",
      "Epoch 859/1000, Cross-Entropy Loss: 16.363886319357864, Binary Loss: 4, Weights: [0.43196787 0.06087972 0.50715241]\n",
      "Epoch 860/1000, Cross-Entropy Loss: 16.36416703344134, Binary Loss: 4, Weights: [0.43199341 0.06088134 0.50712525]\n",
      "Epoch 861/1000, Cross-Entropy Loss: 16.364447386805466, Binary Loss: 4, Weights: [0.43201891 0.06088295 0.50709814]\n",
      "Epoch 862/1000, Cross-Entropy Loss: 16.364727380298998, Binary Loss: 4, Weights: [0.43204438 0.06088456 0.50707106]\n",
      "Epoch 863/1000, Cross-Entropy Loss: 16.36500701476774, Binary Loss: 4, Weights: [0.43206982 0.06088617 0.50704401]\n",
      "Epoch 864/1000, Cross-Entropy Loss: 16.36528629105459, Binary Loss: 4, Weights: [0.43209522 0.06088778 0.507017  ]\n",
      "Epoch 865/1000, Cross-Entropy Loss: 16.3655652099995, Binary Loss: 4, Weights: [0.43212059 0.06088939 0.50699002]\n",
      "Epoch 866/1000, Cross-Entropy Loss: 16.365843772439515, Binary Loss: 4, Weights: [0.43214593 0.06089099 0.50696308]\n",
      "Epoch 867/1000, Cross-Entropy Loss: 16.366121979208792, Binary Loss: 4, Weights: [0.43217123 0.06089259 0.50693618]\n",
      "Epoch 868/1000, Cross-Entropy Loss: 16.366399831138605, Binary Loss: 4, Weights: [0.43219651 0.06089419 0.50690931]\n",
      "Epoch 869/1000, Cross-Entropy Loss: 16.366677329057353, Binary Loss: 4, Weights: [0.43222175 0.06089578 0.50688247]\n",
      "Epoch 870/1000, Cross-Entropy Loss: 16.366954473790596, Binary Loss: 4, Weights: [0.43224695 0.06089738 0.50685567]\n",
      "Epoch 871/1000, Cross-Entropy Loss: 16.36723126616102, Binary Loss: 4, Weights: [0.43227213 0.06089897 0.5068289 ]\n",
      "Epoch 872/1000, Cross-Entropy Loss: 16.367507706988523, Binary Loss: 4, Weights: [0.43229727 0.06090056 0.50680217]\n",
      "Epoch 873/1000, Cross-Entropy Loss: 16.367783797090144, Binary Loss: 4, Weights: [0.43232238 0.06090215 0.50677547]\n",
      "Epoch 874/1000, Cross-Entropy Loss: 16.36805953728014, Binary Loss: 4, Weights: [0.43234746 0.06090373 0.50674881]\n",
      "Epoch 875/1000, Cross-Entropy Loss: 16.368334928369972, Binary Loss: 4, Weights: [0.4323725  0.06090532 0.50672218]\n",
      "Epoch 876/1000, Cross-Entropy Loss: 16.36860997116832, Binary Loss: 4, Weights: [0.43239752 0.0609069  0.50669558]\n",
      "Epoch 877/1000, Cross-Entropy Loss: 16.368884666481094, Binary Loss: 4, Weights: [0.4324225  0.06090848 0.50666902]\n",
      "Epoch 878/1000, Cross-Entropy Loss: 16.369159015111453, Binary Loss: 4, Weights: [0.43244745 0.06091005 0.5066425 ]\n",
      "Epoch 879/1000, Cross-Entropy Loss: 16.36943301785981, Binary Loss: 4, Weights: [0.43247237 0.06091163 0.506616  ]\n",
      "Epoch 880/1000, Cross-Entropy Loss: 16.36970667552385, Binary Loss: 4, Weights: [0.43249725 0.0609132  0.50658954]\n",
      "Epoch 881/1000, Cross-Entropy Loss: 16.36997998889854, Binary Loss: 4, Weights: [0.43252211 0.06091477 0.50656312]\n",
      "Epoch 882/1000, Cross-Entropy Loss: 16.37025295877613, Binary Loss: 4, Weights: [0.43254693 0.06091634 0.50653673]\n",
      "Epoch 883/1000, Cross-Entropy Loss: 16.370525585946194, Binary Loss: 4, Weights: [0.43257172 0.06091791 0.50651037]\n",
      "Epoch 884/1000, Cross-Entropy Loss: 16.370797871195617, Binary Loss: 4, Weights: [0.43259648 0.06091947 0.50648405]\n",
      "Epoch 885/1000, Cross-Entropy Loss: 16.371069815308605, Binary Loss: 4, Weights: [0.43262121 0.06092104 0.50645775]\n",
      "Epoch 886/1000, Cross-Entropy Loss: 16.371341419066713, Binary Loss: 4, Weights: [0.43264591 0.0609226  0.5064315 ]\n",
      "Epoch 887/1000, Cross-Entropy Loss: 16.37161268324885, Binary Loss: 4, Weights: [0.43267057 0.06092416 0.50640527]\n",
      "Epoch 888/1000, Cross-Entropy Loss: 16.37188360863128, Binary Loss: 4, Weights: [0.43269521 0.06092571 0.50637908]\n",
      "Epoch 889/1000, Cross-Entropy Loss: 16.372154195987655, Binary Loss: 4, Weights: [0.43271981 0.06092727 0.50635292]\n",
      "Epoch 890/1000, Cross-Entropy Loss: 16.372424446089013, Binary Loss: 4, Weights: [0.43274438 0.06092882 0.5063268 ]\n",
      "Epoch 891/1000, Cross-Entropy Loss: 16.37269435970379, Binary Loss: 4, Weights: [0.43276892 0.06093037 0.50630071]\n",
      "Epoch 892/1000, Cross-Entropy Loss: 16.372963937597827, Binary Loss: 4, Weights: [0.43279343 0.06093192 0.50627465]\n",
      "Epoch 893/1000, Cross-Entropy Loss: 16.373233180534402, Binary Loss: 4, Weights: [0.43281791 0.06093346 0.50624862]\n",
      "Epoch 894/1000, Cross-Entropy Loss: 16.373502089274215, Binary Loss: 4, Weights: [0.43284236 0.06093501 0.50622263]\n",
      "Epoch 895/1000, Cross-Entropy Loss: 16.37377066457541, Binary Loss: 4, Weights: [0.43286678 0.06093655 0.50619667]\n",
      "Epoch 896/1000, Cross-Entropy Loss: 16.37403890719359, Binary Loss: 4, Weights: [0.43289117 0.06093809 0.50617074]\n",
      "Epoch 897/1000, Cross-Entropy Loss: 16.374306817881838, Binary Loss: 4, Weights: [0.43291552 0.06093963 0.50614485]\n",
      "Epoch 898/1000, Cross-Entropy Loss: 16.374574397390692, Binary Loss: 4, Weights: [0.43293985 0.06094117 0.50611899]\n",
      "Epoch 899/1000, Cross-Entropy Loss: 16.374841646468195, Binary Loss: 4, Weights: [0.43296414 0.0609427  0.50609316]\n",
      "Epoch 900/1000, Cross-Entropy Loss: 16.375108565859893, Binary Loss: 4, Weights: [0.43298841 0.06094423 0.50606736]\n",
      "Epoch 901/1000, Cross-Entropy Loss: 16.375375156308834, Binary Loss: 4, Weights: [0.43301265 0.06094576 0.50604159]\n",
      "Epoch 902/1000, Cross-Entropy Loss: 16.37564141855558, Binary Loss: 4, Weights: [0.43303685 0.06094729 0.50601586]\n",
      "Epoch 903/1000, Cross-Entropy Loss: 16.375907353338256, Binary Loss: 4, Weights: [0.43306103 0.06094882 0.50599016]\n",
      "Epoch 904/1000, Cross-Entropy Loss: 16.376172961392506, Binary Loss: 4, Weights: [0.43308517 0.06095034 0.50596449]\n",
      "Epoch 905/1000, Cross-Entropy Loss: 16.376438243451528, Binary Loss: 4, Weights: [0.43310928 0.06095186 0.50593885]\n",
      "Epoch 906/1000, Cross-Entropy Loss: 16.3767032002461, Binary Loss: 4, Weights: [0.43313337 0.06095338 0.50591325]\n",
      "Epoch 907/1000, Cross-Entropy Loss: 16.376967832504565, Binary Loss: 4, Weights: [0.43315742 0.0609549  0.50588767]\n",
      "Epoch 908/1000, Cross-Entropy Loss: 16.377232140952852, Binary Loss: 4, Weights: [0.43318145 0.06095642 0.50586213]\n",
      "Epoch 909/1000, Cross-Entropy Loss: 16.377496126314483, Binary Loss: 4, Weights: [0.43320544 0.06095793 0.50583662]\n",
      "Epoch 910/1000, Cross-Entropy Loss: 16.37775978931061, Binary Loss: 4, Weights: [0.43322941 0.06095945 0.50581114]\n",
      "Epoch 911/1000, Cross-Entropy Loss: 16.378023130659976, Binary Loss: 4, Weights: [0.43325335 0.06096096 0.5057857 ]\n",
      "Epoch 912/1000, Cross-Entropy Loss: 16.37828615107896, Binary Loss: 4, Weights: [0.43327725 0.06096246 0.50576028]\n",
      "Epoch 913/1000, Cross-Entropy Loss: 16.378548851281575, Binary Loss: 4, Weights: [0.43330113 0.06096397 0.5057349 ]\n",
      "Epoch 914/1000, Cross-Entropy Loss: 16.37881123197949, Binary Loss: 4, Weights: [0.43332498 0.06096548 0.50570955]\n",
      "Epoch 915/1000, Cross-Entropy Loss: 16.37907329388203, Binary Loss: 4, Weights: [0.4333488  0.06096698 0.50568423]\n",
      "Epoch 916/1000, Cross-Entropy Loss: 16.379335037696187, Binary Loss: 4, Weights: [0.43337258 0.06096848 0.50565894]\n",
      "Epoch 917/1000, Cross-Entropy Loss: 16.37959646412663, Binary Loss: 4, Weights: [0.43339634 0.06096998 0.50563368]\n",
      "Epoch 918/1000, Cross-Entropy Loss: 16.379857573875714, Binary Loss: 4, Weights: [0.43342008 0.06097148 0.50560845]\n",
      "Epoch 919/1000, Cross-Entropy Loss: 16.380118367643494, Binary Loss: 4, Weights: [0.43344378 0.06097297 0.50558325]\n",
      "Epoch 920/1000, Cross-Entropy Loss: 16.380378846127737, Binary Loss: 4, Weights: [0.43346745 0.06097446 0.50555809]\n",
      "Epoch 921/1000, Cross-Entropy Loss: 16.380639010023923, Binary Loss: 4, Weights: [0.43349109 0.06097596 0.50553295]\n",
      "Epoch 922/1000, Cross-Entropy Loss: 16.380898860025255, Binary Loss: 4, Weights: [0.43351471 0.06097745 0.50550785]\n",
      "Epoch 923/1000, Cross-Entropy Loss: 16.381158396822688, Binary Loss: 4, Weights: [0.43353829 0.06097893 0.50548277]\n",
      "Epoch 924/1000, Cross-Entropy Loss: 16.38141762110491, Binary Loss: 4, Weights: [0.43356185 0.06098042 0.50545773]\n",
      "Epoch 925/1000, Cross-Entropy Loss: 16.381676533558366, Binary Loss: 4, Weights: [0.43358538 0.0609819  0.50543272]\n",
      "Epoch 926/1000, Cross-Entropy Loss: 16.381935134867273, Binary Loss: 4, Weights: [0.43360888 0.06098338 0.50540774]\n",
      "Epoch 927/1000, Cross-Entropy Loss: 16.382193425713623, Binary Loss: 4, Weights: [0.43363235 0.06098486 0.50538279]\n",
      "Epoch 928/1000, Cross-Entropy Loss: 16.38245140677719, Binary Loss: 4, Weights: [0.43365579 0.06098634 0.50535786]\n",
      "Epoch 929/1000, Cross-Entropy Loss: 16.382709078735537, Binary Loss: 4, Weights: [0.43367921 0.06098782 0.50533297]\n",
      "Epoch 930/1000, Cross-Entropy Loss: 16.38296644226404, Binary Loss: 4, Weights: [0.43370259 0.06098929 0.50530811]\n",
      "Epoch 931/1000, Cross-Entropy Loss: 16.383223498035882, Binary Loss: 4, Weights: [0.43372595 0.06099077 0.50528328]\n",
      "Epoch 932/1000, Cross-Entropy Loss: 16.383480246722076, Binary Loss: 4, Weights: [0.43374928 0.06099224 0.50525848]\n",
      "Epoch 933/1000, Cross-Entropy Loss: 16.38373668899145, Binary Loss: 4, Weights: [0.43377258 0.06099371 0.50523371]\n",
      "Epoch 934/1000, Cross-Entropy Loss: 16.383992825510695, Binary Loss: 4, Weights: [0.43379585 0.06099517 0.50520897]\n",
      "Epoch 935/1000, Cross-Entropy Loss: 16.384248656944333, Binary Loss: 4, Weights: [0.4338191  0.06099664 0.50518426]\n",
      "Epoch 936/1000, Cross-Entropy Loss: 16.384504183954753, Binary Loss: 4, Weights: [0.43384232 0.0609981  0.50515958]\n",
      "Epoch 937/1000, Cross-Entropy Loss: 16.384759407202203, Binary Loss: 4, Weights: [0.43386551 0.06099956 0.50513493]\n",
      "Epoch 938/1000, Cross-Entropy Loss: 16.385014327344823, Binary Loss: 4, Weights: [0.43388867 0.06100102 0.50511031]\n",
      "Epoch 939/1000, Cross-Entropy Loss: 16.385268945038625, Binary Loss: 4, Weights: [0.4339118  0.06100248 0.50508572]\n",
      "Epoch 940/1000, Cross-Entropy Loss: 16.38552326093752, Binary Loss: 4, Weights: [0.43393491 0.06100394 0.50506116]\n",
      "Epoch 941/1000, Cross-Entropy Loss: 16.385777275693325, Binary Loss: 4, Weights: [0.43395798 0.06100539 0.50503663]\n",
      "Epoch 942/1000, Cross-Entropy Loss: 16.38603098995576, Binary Loss: 4, Weights: [0.43398103 0.06100684 0.50501212]\n",
      "Epoch 943/1000, Cross-Entropy Loss: 16.386284404372482, Binary Loss: 4, Weights: [0.43400406 0.06100829 0.50498765]\n",
      "Epoch 944/1000, Cross-Entropy Loss: 16.386537519589055, Binary Loss: 4, Weights: [0.43402705 0.06100974 0.50496321]\n",
      "Epoch 945/1000, Cross-Entropy Loss: 16.386790336248996, Binary Loss: 4, Weights: [0.43405002 0.06101119 0.50493879]\n",
      "Epoch 946/1000, Cross-Entropy Loss: 16.387042854993776, Binary Loss: 4, Weights: [0.43407296 0.06101263 0.50491441]\n",
      "Epoch 947/1000, Cross-Entropy Loss: 16.387295076462802, Binary Loss: 4, Weights: [0.43409587 0.06101408 0.50489005]\n",
      "Epoch 948/1000, Cross-Entropy Loss: 16.387547001293456, Binary Loss: 4, Weights: [0.43411876 0.06101552 0.50486572]\n",
      "Epoch 949/1000, Cross-Entropy Loss: 16.38779863012109, Binary Loss: 4, Weights: [0.43414162 0.06101696 0.50484143]\n",
      "Epoch 950/1000, Cross-Entropy Loss: 16.38804996357904, Binary Loss: 4, Weights: [0.43416445 0.0610184  0.50481716]\n",
      "Epoch 951/1000, Cross-Entropy Loss: 16.38830100229862, Binary Loss: 4, Weights: [0.43418725 0.06101983 0.50479292]\n",
      "Epoch 952/1000, Cross-Entropy Loss: 16.388551746909148, Binary Loss: 4, Weights: [0.43421003 0.06102127 0.5047687 ]\n",
      "Epoch 953/1000, Cross-Entropy Loss: 16.388802198037958, Binary Loss: 4, Weights: [0.43423278 0.0610227  0.50474452]\n",
      "Epoch 954/1000, Cross-Entropy Loss: 16.38905235631038, Binary Loss: 4, Weights: [0.4342555  0.06102413 0.50472037]\n",
      "Epoch 955/1000, Cross-Entropy Loss: 16.389302222349784, Binary Loss: 4, Weights: [0.4342782  0.06102556 0.50469624]\n",
      "Epoch 956/1000, Cross-Entropy Loss: 16.389551796777557, Binary Loss: 4, Weights: [0.43430087 0.06102699 0.50467215]\n",
      "Epoch 957/1000, Cross-Entropy Loss: 16.389801080213118, Binary Loss: 4, Weights: [0.43432351 0.06102841 0.50464808]\n",
      "Epoch 958/1000, Cross-Entropy Loss: 16.39005007327395, Binary Loss: 4, Weights: [0.43434613 0.06102984 0.50462404]\n",
      "Epoch 959/1000, Cross-Entropy Loss: 16.39029877657559, Binary Loss: 4, Weights: [0.43436872 0.06103126 0.50460003]\n",
      "Epoch 960/1000, Cross-Entropy Loss: 16.390547190731617, Binary Loss: 4, Weights: [0.43439128 0.06103268 0.50457604]\n",
      "Epoch 961/1000, Cross-Entropy Loss: 16.390795316353707, Binary Loss: 4, Weights: [0.43441382 0.0610341  0.50455209]\n",
      "Epoch 962/1000, Cross-Entropy Loss: 16.391043154051594, Binary Loss: 4, Weights: [0.43443633 0.06103551 0.50452816]\n",
      "Epoch 963/1000, Cross-Entropy Loss: 16.39129070443311, Binary Loss: 4, Weights: [0.43445881 0.06103693 0.50450426]\n",
      "Epoch 964/1000, Cross-Entropy Loss: 16.39153796810417, Binary Loss: 4, Weights: [0.43448127 0.06103834 0.50448039]\n",
      "Epoch 965/1000, Cross-Entropy Loss: 16.3917849456688, Binary Loss: 4, Weights: [0.4345037  0.06103975 0.50445655]\n",
      "Epoch 966/1000, Cross-Entropy Loss: 16.39203163772914, Binary Loss: 4, Weights: [0.4345261  0.06104116 0.50443273]\n",
      "Epoch 967/1000, Cross-Entropy Loss: 16.392278044885433, Binary Loss: 4, Weights: [0.43454848 0.06104257 0.50440895]\n",
      "Epoch 968/1000, Cross-Entropy Loss: 16.39252416773606, Binary Loss: 4, Weights: [0.43457083 0.06104398 0.50438519]\n",
      "Epoch 969/1000, Cross-Entropy Loss: 16.39277000687753, Binary Loss: 4, Weights: [0.43459316 0.06104538 0.50436146]\n",
      "Epoch 970/1000, Cross-Entropy Loss: 16.393015562904488, Binary Loss: 4, Weights: [0.43461546 0.06104679 0.50433776]\n",
      "Epoch 971/1000, Cross-Entropy Loss: 16.39326083640973, Binary Loss: 4, Weights: [0.43463773 0.06104819 0.50431408]\n",
      "Epoch 972/1000, Cross-Entropy Loss: 16.39350582798422, Binary Loss: 4, Weights: [0.43465998 0.06104959 0.50429043]\n",
      "Epoch 973/1000, Cross-Entropy Loss: 16.393750538217063, Binary Loss: 4, Weights: [0.4346822  0.06105099 0.50426681]\n",
      "Epoch 974/1000, Cross-Entropy Loss: 16.393994967695544, Binary Loss: 4, Weights: [0.4347044  0.06105238 0.50424322]\n",
      "Epoch 975/1000, Cross-Entropy Loss: 16.394239117005135, Binary Loss: 4, Weights: [0.43472657 0.06105378 0.50421965]\n",
      "Epoch 976/1000, Cross-Entropy Loss: 16.39448298672948, Binary Loss: 4, Weights: [0.43474871 0.06105517 0.50419611]\n",
      "Epoch 977/1000, Cross-Entropy Loss: 16.39472657745042, Binary Loss: 4, Weights: [0.43477083 0.06105656 0.5041726 ]\n",
      "Epoch 978/1000, Cross-Entropy Loss: 16.394969889748, Binary Loss: 4, Weights: [0.43479293 0.06105795 0.50414912]\n",
      "Epoch 979/1000, Cross-Entropy Loss: 16.395212924200468, Binary Loss: 4, Weights: [0.434815   0.06105934 0.50412566]\n",
      "Epoch 980/1000, Cross-Entropy Loss: 16.395455681384288, Binary Loss: 4, Weights: [0.43483704 0.06106072 0.50410224]\n",
      "Epoch 981/1000, Cross-Entropy Loss: 16.39569816187414, Binary Loss: 4, Weights: [0.43485906 0.06106211 0.50407883]\n",
      "Epoch 982/1000, Cross-Entropy Loss: 16.395940366242943, Binary Loss: 4, Weights: [0.43488105 0.06106349 0.50405546]\n",
      "Epoch 983/1000, Cross-Entropy Loss: 16.396182295061845, Binary Loss: 4, Weights: [0.43490302 0.06106487 0.50403211]\n",
      "Epoch 984/1000, Cross-Entropy Loss: 16.39642394890024, Binary Loss: 4, Weights: [0.43492496 0.06106625 0.50400879]\n",
      "Epoch 985/1000, Cross-Entropy Loss: 16.39666532832576, Binary Loss: 4, Weights: [0.43494687 0.06106763 0.5039855 ]\n",
      "Epoch 986/1000, Cross-Entropy Loss: 16.396906433904324, Binary Loss: 4, Weights: [0.43496876 0.06106901 0.50396223]\n",
      "Epoch 987/1000, Cross-Entropy Loss: 16.39714726620008, Binary Loss: 4, Weights: [0.43499063 0.06107038 0.50393899]\n",
      "Epoch 988/1000, Cross-Entropy Loss: 16.39738782577547, Binary Loss: 4, Weights: [0.43501247 0.06107175 0.50391577]\n",
      "Epoch 989/1000, Cross-Entropy Loss: 16.397628113191203, Binary Loss: 4, Weights: [0.43503429 0.06107313 0.50389259]\n",
      "Epoch 990/1000, Cross-Entropy Loss: 16.397868129006284, Binary Loss: 4, Weights: [0.43505608 0.0610745  0.50386943]\n",
      "Epoch 991/1000, Cross-Entropy Loss: 16.398107873777995, Binary Loss: 4, Weights: [0.43507784 0.06107586 0.50384629]\n",
      "Epoch 992/1000, Cross-Entropy Loss: 16.39834734806192, Binary Loss: 4, Weights: [0.43509959 0.06107723 0.50382318]\n",
      "Epoch 993/1000, Cross-Entropy Loss: 16.398586552411963, Binary Loss: 4, Weights: [0.4351213  0.06107859 0.5038001 ]\n",
      "Epoch 994/1000, Cross-Entropy Loss: 16.39882548738032, Binary Loss: 4, Weights: [0.43514299 0.06107996 0.50377705]\n",
      "Epoch 995/1000, Cross-Entropy Loss: 16.399064153517525, Binary Loss: 4, Weights: [0.43516466 0.06108132 0.50375402]\n",
      "Epoch 996/1000, Cross-Entropy Loss: 16.399302551372415, Binary Loss: 4, Weights: [0.4351863  0.06108268 0.50373102]\n",
      "Epoch 997/1000, Cross-Entropy Loss: 16.399540681492184, Binary Loss: 4, Weights: [0.43520792 0.06108404 0.50370804]\n",
      "Epoch 998/1000, Cross-Entropy Loss: 16.399778544422347, Binary Loss: 4, Weights: [0.43522951 0.06108539 0.50368509]\n",
      "Epoch 999/1000, Cross-Entropy Loss: 16.400016140706768, Binary Loss: 4, Weights: [0.43525108 0.06108675 0.50366217]\n",
      "Epoch 1000/1000, Cross-Entropy Loss: 16.400253470887666, Binary Loss: 4, Weights: [0.43527263 0.0610881  0.50363927]\n",
      "Best Weights based on Binary Loss: [0.46106853 0.0381899  0.50074157]\n",
      "Test Data - Final Brand: Disney, Confidence Score: 0.35608383650979397\n",
      "Test Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Carvana, Predicted: Carvana, Confidence: 0.6418508814322089, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Febreze, Predicted: Febreze, Confidence: 0.6555720240087504, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Unilever, Predicted: Dove, Confidence: 0.6688436420383966, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Discover Newport, Predicted: Discover Newport, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 0.9635559593225195, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: SmartLife, Predicted: Nectar, Confidence: 0.16864603682761806, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.9271119186450391, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Missouri, Predicted: Missouri, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: HelloFresh, Predicted: HelloFresh, Confidence: 0.7254391956431262, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: CREATION MUSEUM, Predicted: CREATION MUSEUM, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Ruggable, Predicted: Ruggable, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: WELLS FARGO, Predicted: WELLS FARGO, Confidence: 0.9271119186450391, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Ziploc, Predicted: Ziploc, Confidence: 0.3426532918905433, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Xfinity, Predicted: Xfinity, Confidence: 0.6584643677780102, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Comcast, Predicted: Comcast, Confidence: 0.9234153550888203, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: COMCAST BUSINESS, Predicted: COMCAST BUSINESS, Confidence: 0.5009328732058502, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Xfinity Mobile, Predicted: Xfinity Mobile, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.9612058945228683, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Mastercard, Predicted: Mastercard, Confidence: 0.9635559593225195, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.6924733367933394, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: ORIS, Predicted: ORIS, Confidence: 0.913759372871016, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Safelite, Predicted: Safelite, Confidence: 0.9635559593225195, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Baker's, Predicted: Baker's, Confidence: 0.3183250593541434, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Freeform, Predicted: Freeform, Confidence: 0.4898435481948089, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Booking.com, Predicted: Booking.com, Confidence: 0.6463404790145483, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.6683511826623192, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: SHERWIN-WILLIAMS, Predicted: SHERWIN-WILLIAMS, Confidence: 0.36591265852397437, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Hulu, Predicted: Disney, Confidence: 0.24686295175160206, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: MADE IN COOKWARE, Predicted: MADE IN COOKWARE, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Pandora, Predicted: Pandora, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Lincoln, Predicted: Lincoln, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: U.S. Bank, Predicted: U.S. Bank, Confidence: 0.9234153550888203, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: INK MASTER, Predicted: Paramount+, Confidence: 0.13302981134244474, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: MAIN EVENT, Predicted: MAIN EVENT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Amazon Prime, Predicted: Amazon Prime, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: ARIAT, Predicted: ARIAT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.9621775566588738, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.5059620839003313, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Total Loss on Training Data: 4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Example dataset\n",
    "data = data_set\n",
    "\n",
    "# Combining confidence scores with weights\n",
    "def combine_confidences(inputs, weights):\n",
    "    combined_scores = defaultdict(float)\n",
    "    for source, weight in zip(['gemini_results', 'vi_results', 'ocr_text'], weights):\n",
    "        for brand, score in inputs[source]:\n",
    "            combined_scores[brand] += weight * score\n",
    "    return combined_scores\n",
    "\n",
    "# Normalizing the scores\n",
    "def normalize_scores(combined_scores):\n",
    "    total_score = sum(combined_scores.values())\n",
    "    if total_score == 0:\n",
    "        return combined_scores\n",
    "    for brand in combined_scores:\n",
    "        combined_scores[brand] /= total_score\n",
    "    return combined_scores\n",
    "\n",
    "# Computing the final prediction\n",
    "def get_final_brand(inputs, weights):\n",
    "    combined_scores = combine_confidences(inputs, weights)\n",
    "    normalized_scores = normalize_scores(combined_scores)\n",
    "    final_brand = max(normalized_scores, key=normalized_scores.get)\n",
    "    final_confidence = normalized_scores[final_brand]\n",
    "    return final_brand, final_confidence, normalized_scores\n",
    "\n",
    "# Calculating cross-entropy loss\n",
    "def cross_entropy_loss(predicted_probs, actual_label, all_brands):\n",
    "    epsilon = 1e-15\n",
    "    predicted_probs = np.array([predicted_probs.get(brand, epsilon) for brand in all_brands])\n",
    "    predicted_probs = np.clip(predicted_probs, epsilon, 1 - epsilon)\n",
    "    actual_vector = np.array([1 if brand == actual_label else 0 for brand in all_brands])\n",
    "    return -np.sum(actual_vector * np.log(predicted_probs))\n",
    "\n",
    "# Simple binary loss for evaluation\n",
    "def calculate_loss(predicted, actual):\n",
    "    return 1 if predicted != actual else 0\n",
    "\n",
    "# Optimizing the weights using Adam optimizer with hybrid loss\n",
    "def optimize_weights(data_set, learning_rate=0.01, epochs=1000):\n",
    "    weights = np.ones(3) / 3  # Initial weights for gemini_results, vi_results, ocr_text\n",
    "    m = np.zeros(3)\n",
    "    v = np.zeros(3)\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    t = 0\n",
    "    \n",
    "    best_weights = weights.copy()\n",
    "    min_binary_loss = float('inf')\n",
    "    \n",
    "    all_brands = set()\n",
    "    for entry in data_set:\n",
    "        for source in ['gemini_results', 'vi_results', 'ocr_text']:\n",
    "            all_brands.update([brand for brand, _ in entry['inputs'][source]])\n",
    "    all_brands = list(all_brands)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_cross_entropy_loss = 0\n",
    "        total_binary_loss = 0\n",
    "        weight_gradients = np.zeros(3)\n",
    "        for entry in data_set:\n",
    "            actual_final = entry['output']['final'][0]\n",
    "            final_brand, _, predicted_probs = get_final_brand(entry['inputs'], weights)\n",
    "            cross_entropy = cross_entropy_loss(predicted_probs, actual_final, all_brands)\n",
    "            binary_loss = calculate_loss(final_brand, actual_final)\n",
    "            total_cross_entropy_loss += cross_entropy\n",
    "            total_binary_loss += binary_loss\n",
    "            for i, source in enumerate(['gemini_results', 'vi_results', 'ocr_text']):\n",
    "                for brand, score in entry['inputs'][source]:\n",
    "                    gradient = (predicted_probs.get(brand, 0) - (1 if brand == actual_final else 0)) * score\n",
    "                    weight_gradients[i] += gradient\n",
    "        \n",
    "        # Update best weights based on binary loss\n",
    "        if total_binary_loss < min_binary_loss:\n",
    "            min_binary_loss = total_binary_loss\n",
    "            best_weights = weights.copy()\n",
    "        \n",
    "        t += 1\n",
    "        m = beta1 * m + (1 - beta1) * weight_gradients\n",
    "        v = beta2 * v + (1 - beta2) * (weight_gradients ** 2)\n",
    "        m_hat = m / (1 - beta1 ** t)\n",
    "        v_hat = v / (1 - beta2 ** t)\n",
    "        weights -= learning_rate * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        \n",
    "        weights = np.clip(weights, 0, 1)\n",
    "        weights /= np.sum(weights)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Cross-Entropy Loss: {total_cross_entropy_loss}, Binary Loss: {total_binary_loss}, Weights: {weights}\")\n",
    "    \n",
    "    print(f\"Best Weights based on Binary Loss: {best_weights}\")\n",
    "    return best_weights\n",
    "\n",
    "# Optimizing weights and testing\n",
    "final_weights = optimize_weights(data)\n",
    "\n",
    "# Test case\n",
    "test_data = [\n",
    "    {'inputs': {'gemini_results': [('Disney', 1.00), ('Ziploc', 1.00), ('SC Johnson', 1.00)], 'vi_results': [('Ziploc', 0.99), ('The Walt Disney Company', 0.99)], 'ocr_text': [('Disney', 0.90), ('Ziploc', 0.80), ('MIPS', 0.50)]}},\n",
    "]\n",
    "\n",
    "# Get predictions for test data\n",
    "for entry in test_data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    print(f\"Test Data - Final Brand: {final_brand}, Confidence Score: {final_confidence}\")\n",
    "    print(f\"Test Data - Weights: {final_weights}\")\n",
    "\n",
    "# Print results for training data and calculate total loss\n",
    "total_loss = 0\n",
    "for entry in data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    actual_final = entry['output']['final'][0]\n",
    "    loss = calculate_loss(final_brand, actual_final)\n",
    "    total_loss += loss\n",
    "    print(f\"Expected: {actual_final}, Predicted: {final_brand}, Confidence: {final_confidence}, Loss: {loss}\")\n",
    "    print(f\"Training Data - Weights: {final_weights}\")\n",
    "\n",
    "print(f\"Total Loss on Training Data: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected: Carvana, Predicted: Carvana, Confidence: 0.6418508814322089, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Febreze, Predicted: Febreze, Confidence: 0.6555720240087504, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Unilever, Predicted: Dove, Confidence: 0.6688436420383966, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Discover Newport, Predicted: Discover Newport, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 0.9635559593225195, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: SmartLife, Predicted: Nectar, Confidence: 0.16864603682761806, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.9271119186450391, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Missouri, Predicted: Missouri, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Raymour & Flanigan, Predicted: Raymour & Flanigan, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: HelloFresh, Predicted: HelloFresh, Confidence: 0.7254391956431262, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: CREATION MUSEUM, Predicted: CREATION MUSEUM, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Ruggable, Predicted: Ruggable, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: WELLS FARGO, Predicted: WELLS FARGO, Confidence: 0.9271119186450391, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Ziploc, Predicted: Ziploc, Confidence: 0.3426532918905433, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Xfinity, Predicted: Xfinity, Confidence: 0.6584643677780102, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Comcast, Predicted: Comcast, Confidence: 0.9234153550888203, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: COMCAST BUSINESS, Predicted: COMCAST BUSINESS, Confidence: 0.5009328732058502, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Xfinity Mobile, Predicted: Xfinity Mobile, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Jacoby & Meyers, Predicted: Jacoby & Meyers, Confidence: 0.9612058945228683, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Mastercard, Predicted: Mastercard, Confidence: 0.9635559593225195, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: BETMGM, Predicted: BETMGM, Confidence: 0.6924733367933394, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: ORIS, Predicted: ORIS, Confidence: 0.913759372871016, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Safelite, Predicted: Safelite, Confidence: 0.9635559593225195, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Baker's, Predicted: Baker's, Confidence: 0.3183250593541434, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Freeform, Predicted: Freeform, Confidence: 0.4898435481948089, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Booking.com, Predicted: Booking.com, Confidence: 0.6463404790145483, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Paramount+, Predicted: Paramount+, Confidence: 0.6683511826623192, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: SHERWIN-WILLIAMS, Predicted: SHERWIN-WILLIAMS, Confidence: 0.36591265852397437, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Hulu, Predicted: Disney, Confidence: 0.24686295175160206, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: MADE IN COOKWARE, Predicted: MADE IN COOKWARE, Confidence: 0.9601829918282818, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Pandora, Predicted: Pandora, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Lincoln, Predicted: Lincoln, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: U.S. Bank, Predicted: U.S. Bank, Confidence: 0.9234153550888203, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: INK MASTER, Predicted: Paramount+, Confidence: 0.13302981134244474, Loss: 1\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: MAIN EVENT, Predicted: MAIN EVENT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Amazon Prime, Predicted: Amazon Prime, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: ARIAT, Predicted: ARIAT, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.9621775566588738, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Bassett, Predicted: Bassett, Confidence: 0.5059620839003313, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Expected: Taco Bell, Predicted: Taco Bell, Confidence: 1.0, Loss: 0\n",
      "Training Data - Weights: [0.46106853 0.0381899  0.50074157]\n",
      "Total Loss on Training Data: 4\n"
     ]
    }
   ],
   "source": [
    "# Print results for training data and calculate total loss\n",
    "total_loss = 0\n",
    "wrong_predictios = []\n",
    "for entry in data:\n",
    "    final_brand, final_confidence, _ = get_final_brand(entry['inputs'], final_weights)\n",
    "    actual_final = entry['output']['final'][0]\n",
    "    loss = calculate_loss(final_brand, actual_final)\n",
    "    if(loss):\n",
    "        wrong_predictios.append(entry)\n",
    "    total_loss += loss\n",
    "    print(f\"Expected: {actual_final}, Predicted: {final_brand}, Confidence: {final_confidence}, Loss: {loss}\")\n",
    "    print(f\"Training Data - Weights: {final_weights}\")\n",
    "\n",
    "print(f\"Total Loss on Training Data: {total_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'inputs': {'gemini_results': [('Dove', 1)],\n",
       "   'vi_results': [('Unilever', 0.99), ('Nike', 0.99)],\n",
       "   'ocr_text': [('Dove', 1), ('Unilever', 0.8)]},\n",
       "  'output': {'final': ['Unilever', 'Dove']}},\n",
       " {'inputs': {'gemini_results': [('Nectar', 1),\n",
       "    ('Sealy', 1),\n",
       "    ('Casper', 1),\n",
       "    ('Serta', 1),\n",
       "    ('Stearns & Foster', 1),\n",
       "    ('SmartLife', 1),\n",
       "    ('King Koil', 1)],\n",
       "   'vi_results': [('Raymour & Flanigan', 0.99), ('Sealy Corporation', 0.99)],\n",
       "   'ocr_text': [('Nectar', 0.9),\n",
       "    ('Sealy', 0.8),\n",
       "    ('Casper', 0.7),\n",
       "    ('Serta', 0.6),\n",
       "    ('Stearns & Foster', 0.5),\n",
       "    ('SmartLife', 0.4),\n",
       "    ('King Koil', 0.3)]},\n",
       "  'output': {'final': ['SmartLife',\n",
       "    'Casper',\n",
       "    'Serta',\n",
       "    'King Koil',\n",
       "    'Stearns & Foster',\n",
       "    'Nectar',\n",
       "    'Sealy']}},\n",
       " {'inputs': {'gemini_results': [('Disney', 1),\n",
       "    ('Hulu', 1),\n",
       "    ('Google', 1),\n",
       "    ('FX', 1),\n",
       "    ('20th Century Studios', 1),\n",
       "    ('Searchlight Pictures', 1)],\n",
       "   'vi_results': [('The Walt Disney Company', 0.99), ('Hulu', 0.99)],\n",
       "   'ocr_text': [('Disney', 0.9), ('Hulu', 0.8)]},\n",
       "  'output': {'final': ['Hulu', 'Disney']}},\n",
       " {'inputs': {'gemini_results': [('Paramount+', 1),\n",
       "    (\"RUPAUL'S DRAG RACE ALL STARS\", 1),\n",
       "    ('BIG BROTHER', 1),\n",
       "    ('THE CHALLENGE', 1),\n",
       "    ('SURVIVOR', 1),\n",
       "    ('ARE YOU THE ONE?', 1),\n",
       "    ('INK MASTER', 1),\n",
       "    ('QUEEN OF THE UNIVERSE', 1),\n",
       "    ('BAR RESCUE', 1)],\n",
       "   'vi_results': [],\n",
       "   'ocr_text': [('Paramount+', 0.9),\n",
       "    (\"RUPAUL'S DRAG RACE ALL STARS\", 0.8),\n",
       "    ('BIG BROTHER', 0.8),\n",
       "    ('THE CHALLENGE', 0.7),\n",
       "    ('SURVIVOR', 0.6),\n",
       "    ('ARE YOU THE ONE?', 0.5),\n",
       "    ('INK MASTER', 0.4),\n",
       "    ('QUEEN OF THE UNIVERSE', 0.4),\n",
       "    ('BAR RESCUE', 0.3)]},\n",
       "  'output': {'final': ['INK MASTER',\n",
       "    \"RUPAUL'S DRAG RACE ALL STARS\",\n",
       "    'ARE YOU THE ONE?',\n",
       "    'Paramount+',\n",
       "    'QUEEN OF THE UNIVERSE',\n",
       "    'BIG BROTHER',\n",
       "    'SURVIVOR',\n",
       "    'THE CHALLENGE',\n",
       "    'BAR RESCUE']}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrong_predictios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
